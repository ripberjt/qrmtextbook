<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>13 Multiple Regression and Model Building | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R</title>
  <meta name="description" content="13 Multiple Regression and Model Building | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="13 Multiple Regression and Model Building | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13 Multiple Regression and Model Building | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R" />
  
  
  

<meta name="author" content="Hank Jenkins-Smith">
<meta name="author" content="Joseph Ripberger">
<meta name="author" content="Gary Copeland">
<meta name="author" content="Matthew Nowlin">
<meta name="author" content="Tyler Hughes">
<meta name="author" content="Aaron Fister">
<meta name="author" content="Wesley Wehde">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="the-logic-of-multiple-regression.html">
<link rel="next" href="topics-in-multiple-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface and Acknowledgments</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#copyright"><i class="fa fa-check"></i>Copyright</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html"><i class="fa fa-check"></i><b>1</b> Theories and Social Science</a><ul>
<li class="chapter" data-level="1.1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#the-scientific-method"><i class="fa fa-check"></i><b>1.1</b> The Scientific Method</a></li>
<li class="chapter" data-level="1.2" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-and-empirical-research"><i class="fa fa-check"></i><b>1.2</b> Theory and Empirical Research</a><ul>
<li class="chapter" data-level="1.2.1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#coherent-and-internally-consistent"><i class="fa fa-check"></i><b>1.2.1</b> Coherent and Internally Consistent</a></li>
<li class="chapter" data-level="1.2.2" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theories-and-causality"><i class="fa fa-check"></i><b>1.2.2</b> Theories and Causality</a></li>
<li class="chapter" data-level="1.2.3" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#generation-of-testable-hypothesis"><i class="fa fa-check"></i><b>1.2.3</b> Generation of Testable Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-and-functions"><i class="fa fa-check"></i><b>1.3</b> Theory and Functions</a></li>
<li class="chapter" data-level="1.4" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-in-social-science"><i class="fa fa-check"></i><b>1.4</b> Theory in Social Science</a></li>
<li class="chapter" data-level="1.5" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#outline-of-the-book"><i class="fa fa-check"></i><b>1.5</b> Outline of the Book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="research-design.html"><a href="research-design.html"><i class="fa fa-check"></i><b>2</b> Research Design</a><ul>
<li class="chapter" data-level="2.1" data-path="research-design.html"><a href="research-design.html#overview-of-the-research-process"><i class="fa fa-check"></i><b>2.1</b> Overview of the Research Process</a></li>
<li class="chapter" data-level="2.2" data-path="research-design.html"><a href="research-design.html#internal-and-external-validity"><i class="fa fa-check"></i><b>2.2</b> Internal and External Validity</a></li>
<li class="chapter" data-level="2.3" data-path="research-design.html"><a href="research-design.html#major-classes-of-designs"><i class="fa fa-check"></i><b>2.3</b> Major Classes of Designs</a></li>
<li class="chapter" data-level="2.4" data-path="research-design.html"><a href="research-design.html#threats-to-validity"><i class="fa fa-check"></i><b>2.4</b> Threats to Validity</a></li>
<li class="chapter" data-level="2.5" data-path="research-design.html"><a href="research-design.html#some-common-designs"><i class="fa fa-check"></i><b>2.5</b> Some Common Designs</a></li>
<li class="chapter" data-level="2.6" data-path="research-design.html"><a href="research-design.html#plan-meets-reality"><i class="fa fa-check"></i><b>2.6</b> Plan Meets Reality</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html"><i class="fa fa-check"></i><b>3</b> Exploring and Visualizing Data</a><ul>
<li class="chapter" data-level="3.1" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#characterizing-data"><i class="fa fa-check"></i><b>3.1</b> Characterizing Data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#central-tendency"><i class="fa fa-check"></i><b>3.1.1</b> Central Tendency</a></li>
<li class="chapter" data-level="3.1.2" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#level-of-measurement-and-central-tendency"><i class="fa fa-check"></i><b>3.1.2</b> Level of Measurement and Central Tendency</a></li>
<li class="chapter" data-level="3.1.3" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#moments"><i class="fa fa-check"></i><b>3.1.3</b> Moments</a></li>
<li class="chapter" data-level="3.1.4" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#first-moment-expected-value"><i class="fa fa-check"></i><b>3.1.4</b> First Moment – Expected Value</a></li>
<li class="chapter" data-level="3.1.5" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#the-second-moment-variance-and-standard-deviation"><i class="fa fa-check"></i><b>3.1.5</b> The Second Moment – Variance and Standard Deviation</a></li>
<li class="chapter" data-level="3.1.6" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#the-third-moment-skewness"><i class="fa fa-check"></i><b>3.1.6</b> The Third Moment – Skewness</a></li>
<li class="chapter" data-level="3.1.7" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#the-fourth-moment-kurtosis"><i class="fa fa-check"></i><b>3.1.7</b> The Fourth Moment – Kurtosis</a></li>
<li class="chapter" data-level="3.1.8" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#order-statistics"><i class="fa fa-check"></i><b>3.1.8</b> Order Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#summary"><i class="fa fa-check"></i><b>3.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#finding-probabilities"><i class="fa fa-check"></i><b>4.1</b> Finding Probabilities</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#finding-probabilities-with-the-normal-curve"><i class="fa fa-check"></i><b>4.2</b> Finding Probabilities with the Normal Curve</a></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#summary-1"><i class="fa fa-check"></i><b>4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>5</b> Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="inference.html"><a href="inference.html#inference-populations-and-samples"><i class="fa fa-check"></i><b>5.1</b> Inference: Populations and Samples</a><ul>
<li class="chapter" data-level="5.1.1" data-path="inference.html"><a href="inference.html#populations-and-samples"><i class="fa fa-check"></i><b>5.1.1</b> Populations and Samples</a></li>
<li class="chapter" data-level="5.1.2" data-path="inference.html"><a href="inference.html#sampling-and-knowing"><i class="fa fa-check"></i><b>5.1.2</b> Sampling and Knowing</a></li>
<li class="chapter" data-level="5.1.3" data-path="inference.html"><a href="inference.html#sampling-strategies"><i class="fa fa-check"></i><b>5.1.3</b> Sampling Strategies</a></li>
<li class="chapter" data-level="5.1.4" data-path="inference.html"><a href="inference.html#sampling-techniques"><i class="fa fa-check"></i><b>5.1.4</b> Sampling Techniques</a></li>
<li class="chapter" data-level="5.1.5" data-path="inference.html"><a href="inference.html#so-how-is-it-that-we-know"><i class="fa fa-check"></i><b>5.1.5</b> So How is it That We Know?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inference.html"><a href="inference.html#the-normal-distribution"><i class="fa fa-check"></i><b>5.2</b> The Normal Distribution</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inference.html"><a href="inference.html#standardizing-a-normal-distribution-and-z-scores"><i class="fa fa-check"></i><b>5.2.1</b> Standardizing a Normal Distribution and Z-scores</a></li>
<li class="chapter" data-level="5.2.2" data-path="inference.html"><a href="inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.3" data-path="inference.html"><a href="inference.html#populations-samples-and-symbols"><i class="fa fa-check"></i><b>5.2.3</b> Populations, Samples and Symbols</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference.html"><a href="inference.html#inferences-to-the-population-from-the-sample"><i class="fa fa-check"></i><b>5.3</b> Inferences to the Population from the Sample</a><ul>
<li class="chapter" data-level="5.3.1" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference.html"><a href="inference.html#the-logic-of-hypothesis-testing"><i class="fa fa-check"></i><b>5.3.2</b> The Logic of Hypothesis Testing</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference.html"><a href="inference.html#some-miscellaneous-notes-about-hypothesis-testing"><i class="fa fa-check"></i><b>5.3.3</b> Some Miscellaneous Notes about Hypothesis Testing</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="inference.html"><a href="inference.html#differences-between-groups"><i class="fa fa-check"></i><b>5.4</b> Differences Between Groups</a><ul>
<li class="chapter" data-level="5.4.1" data-path="inference.html"><a href="inference.html#t-tests"><i class="fa fa-check"></i><b>5.4.1</b> <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="inference.html"><a href="inference.html#summary-2"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="association-of-variables.html"><a href="association-of-variables.html"><i class="fa fa-check"></i><b>6</b> Association of Variables</a><ul>
<li class="chapter" data-level="6.1" data-path="association-of-variables.html"><a href="association-of-variables.html#cross-tabulation"><i class="fa fa-check"></i><b>6.1</b> Cross-Tabulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="association-of-variables.html"><a href="association-of-variables.html#crosstabulation-and-control"><i class="fa fa-check"></i><b>6.1.1</b> Crosstabulation and Control</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="association-of-variables.html"><a href="association-of-variables.html#covariance"><i class="fa fa-check"></i><b>6.2</b> Covariance</a></li>
<li class="chapter" data-level="6.3" data-path="association-of-variables.html"><a href="association-of-variables.html#correlation"><i class="fa fa-check"></i><b>6.3</b> Correlation</a></li>
<li class="chapter" data-level="6.4" data-path="association-of-variables.html"><a href="association-of-variables.html#scatterplots"><i class="fa fa-check"></i><b>6.4</b> Scatterplots</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html"><i class="fa fa-check"></i><b>7</b> The Logic of Ordinary Least Squares Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#theoretical-models"><i class="fa fa-check"></i><b>7.1</b> Theoretical Models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#deterministic-linear-model"><i class="fa fa-check"></i><b>7.1.1</b> Deterministic Linear Model</a></li>
<li class="chapter" data-level="7.1.2" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#stochastic-linear-model"><i class="fa fa-check"></i><b>7.1.2</b> Stochastic Linear Model</a></li>
<li class="chapter" data-level="7.1.3" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#assumptions-about-the-error-term"><i class="fa fa-check"></i><b>7.1.3</b> Assumptions about the Error Term</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#estimating-linear-models"><i class="fa fa-check"></i><b>7.2</b> Estimating Linear Models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#residuals"><i class="fa fa-check"></i><b>7.2.1</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#an-example-of-simple-regression"><i class="fa fa-check"></i><b>7.3</b> An Example of Simple Regression</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html"><i class="fa fa-check"></i><b>8</b> Linear Estimation and Minimizing Error</a><ul>
<li class="chapter" data-level="8.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#minimizing-error-using-derivatives"><i class="fa fa-check"></i><b>8.1</b> Minimizing Error using Derivatives</a><ul>
<li class="chapter" data-level="8.1.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#rules-of-derivation"><i class="fa fa-check"></i><b>8.1.1</b> Rules of Derivation</a></li>
<li class="chapter" data-level="8.1.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#critical-points"><i class="fa fa-check"></i><b>8.1.2</b> Critical Points</a></li>
<li class="chapter" data-level="8.1.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#partial-derivation"><i class="fa fa-check"></i><b>8.1.3</b> Partial Derivation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#deriving-ols-estimators"><i class="fa fa-check"></i><b>8.2</b> Deriving OLS Estimators</a><ul>
<li class="chapter" data-level="8.2.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#ols-derivation-of-hatalpha"><i class="fa fa-check"></i><b>8.2.1</b> OLS Derivation of <span class="math inline">\(\hat{\alpha}\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#ols-derivation-of-hatbeta"><i class="fa fa-check"></i><b>8.2.2</b> OLS Derivation of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li class="chapter" data-level="8.2.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#interpreting-hatbeta-and-hatalpha"><i class="fa fa-check"></i><b>8.2.3</b> Interpreting <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\alpha}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#summary-3"><i class="fa fa-check"></i><b>8.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html"><i class="fa fa-check"></i><b>9</b> Bi-Variate Hypothesis Testing and Model Fit</a><ul>
<li class="chapter" data-level="9.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#hypothesis-tests-for-regression-coefficients"><i class="fa fa-check"></i><b>9.1</b> Hypothesis Tests for Regression Coefficients</a><ul>
<li class="chapter" data-level="9.1.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#residual-standard-error"><i class="fa fa-check"></i><b>9.1.1</b> Residual Standard Error</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#measuring-goodness-of-fit"><i class="fa fa-check"></i><b>9.2</b> Measuring Goodness of Fit</a><ul>
<li class="chapter" data-level="9.2.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#sample-covariance-and-correlations"><i class="fa fa-check"></i><b>9.2.1</b> Sample Covariance and Correlations</a></li>
<li class="chapter" data-level="9.2.2" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>9.2.2</b> Coefficient of Determination: <span class="math inline">\(R^{2}\)</span></a></li>
<li class="chapter" data-level="9.2.3" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#visualizing-bivariate-regression"><i class="fa fa-check"></i><b>9.2.3</b> Visualizing Bivariate Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#summary-4"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html"><i class="fa fa-check"></i><b>10</b> OLS Assumptions and Simple Regression Diagnostics</a><ul>
<li class="chapter" data-level="10.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#a-recap-of-modeling-assumptions"><i class="fa fa-check"></i><b>10.1</b> A Recap of Modeling Assumptions</a></li>
<li class="chapter" data-level="10.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#when-things-go-bad-with-residuals"><i class="fa fa-check"></i><b>10.2</b> When Things Go Bad with Residuals</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#outlier-data"><i class="fa fa-check"></i><b>10.2.1</b> “Outlier” Data</a></li>
<li class="chapter" data-level="10.2.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#non-constant-variance"><i class="fa fa-check"></i><b>10.2.2</b> Non-Constant Variance</a></li>
<li class="chapter" data-level="10.2.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#non-linearity-in-the-parameters"><i class="fa fa-check"></i><b>10.2.3</b> Non-Linearity in the Parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#application-of-residual-diagnostics"><i class="fa fa-check"></i><b>10.3</b> Application of Residual Diagnostics</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-non-linearity"><i class="fa fa-check"></i><b>10.3.1</b> Testing for Non-Linearity</a></li>
<li class="chapter" data-level="10.3.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-normality-in-model-residuals"><i class="fa fa-check"></i><b>10.3.2</b> Testing for Normality in Model Residuals</a></li>
<li class="chapter" data-level="10.3.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-non-constant-variance-in-the-residuals"><i class="fa fa-check"></i><b>10.3.3</b> Testing for Non-Constant Variance in the Residuals</a></li>
<li class="chapter" data-level="10.3.4" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#examining-outlier-data"><i class="fa fa-check"></i><b>10.3.4</b> Examining Outlier Data</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#so-now-what-implications-of-residual-analysis"><i class="fa fa-check"></i><b>10.4</b> So Now What? Implications of Residual Analysis</a></li>
<li class="chapter" data-level="10.5" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#summary-5"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html"><i class="fa fa-check"></i><b>11</b> Introduction to Multiple Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-algebra-and-multiple-regression"><i class="fa fa-check"></i><b>11.1</b> Matrix Algebra and Multiple Regression</a></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#the-basics-of-matrix-algebra"><i class="fa fa-check"></i><b>11.2</b> The Basics of Matrix Algebra</a><ul>
<li class="chapter" data-level="11.2.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-basics"><i class="fa fa-check"></i><b>11.2.1</b> Matrix Basics</a></li>
<li class="chapter" data-level="11.2.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#vectors"><i class="fa fa-check"></i><b>11.2.2</b> Vectors</a></li>
<li class="chapter" data-level="11.2.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-operations"><i class="fa fa-check"></i><b>11.2.3</b> Matrix Operations</a></li>
<li class="chapter" data-level="11.2.4" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#transpose"><i class="fa fa-check"></i><b>11.2.4</b> Transpose</a></li>
<li class="chapter" data-level="11.2.5" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#adding-matrices"><i class="fa fa-check"></i><b>11.2.5</b> Adding Matrices</a></li>
<li class="chapter" data-level="11.2.6" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#multiplication-of-matrices"><i class="fa fa-check"></i><b>11.2.6</b> Multiplication of Matrices</a></li>
<li class="chapter" data-level="11.2.7" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#identity-matrices"><i class="fa fa-check"></i><b>11.2.7</b> Identity Matrices</a></li>
<li class="chapter" data-level="11.2.8" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-inversion"><i class="fa fa-check"></i><b>11.2.8</b> Matrix Inversion</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#ols-regression-in-matrix-form"><i class="fa fa-check"></i><b>11.3</b> OLS Regression in Matrix Form</a></li>
<li class="chapter" data-level="11.4" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#summary-6"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html"><i class="fa fa-check"></i><b>12</b> The Logic of Multiple Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#theoretical-specification"><i class="fa fa-check"></i><b>12.1</b> Theoretical Specification</a><ul>
<li class="chapter" data-level="12.1.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#assumptions-of-ols-regression"><i class="fa fa-check"></i><b>12.1.1</b> Assumptions of OLS Regression</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#partial-effects"><i class="fa fa-check"></i><b>12.2</b> Partial Effects</a></li>
<li class="chapter" data-level="12.3" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#multiple-regression-example"><i class="fa fa-check"></i><b>12.3</b> Multiple Regression Example</a><ul>
<li class="chapter" data-level="12.3.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#hypothesis-testing-and-t-tests"><i class="fa fa-check"></i><b>12.3.1</b> Hypothesis Testing and <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#summary-7"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html"><i class="fa fa-check"></i><b>13</b> Multiple Regression and Model Building</a><ul>
<li class="chapter" data-level="13.1" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#model-building"><i class="fa fa-check"></i><b>13.1</b> Model Building</a><ul>
<li class="chapter" data-level="13.1.1" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#theory-and-hypotheses"><i class="fa fa-check"></i><b>13.1.1</b> Theory and Hypotheses</a></li>
<li class="chapter" data-level="13.1.2" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#empirical-indicators"><i class="fa fa-check"></i><b>13.1.2</b> Empirical Indicators</a></li>
<li class="chapter" data-level="13.1.3" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#risks-in-model-building"><i class="fa fa-check"></i><b>13.1.3</b> Risks in Model Building</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#evils-of-stepwise-regression"><i class="fa fa-check"></i><b>13.2</b> Evils of Stepwise Regression</a></li>
<li class="chapter" data-level="13.3" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#summary-8"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html"><i class="fa fa-check"></i><b>14</b> Topics in Multiple Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#dummy-variables"><i class="fa fa-check"></i><b>14.1</b> Dummy Variables</a></li>
<li class="chapter" data-level="14.2" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#interaction-effects"><i class="fa fa-check"></i><b>14.2</b> Interaction Effects</a></li>
<li class="chapter" data-level="14.3" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#standardized-regression-coefficients"><i class="fa fa-check"></i><b>14.3</b> Standardized Regression Coefficients</a></li>
<li class="chapter" data-level="14.4" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#summary-9"><i class="fa fa-check"></i><b>14.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html"><i class="fa fa-check"></i><b>15</b> The Art of Regression Diagnostics</a><ul>
<li class="chapter" data-level="15.1" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#ols-error-assumptions-revisited"><i class="fa fa-check"></i><b>15.1</b> OLS Error Assumptions Revisited</a></li>
<li class="chapter" data-level="15.2" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#ols-diagnostic-techniques"><i class="fa fa-check"></i><b>15.2</b> OLS Diagnostic Techniques</a><ul>
<li class="chapter" data-level="15.2.1" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#non-linearity"><i class="fa fa-check"></i><b>15.2.1</b> Non-Linearity</a></li>
<li class="chapter" data-level="15.2.2" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#non-constant-variance-or-heteroscedasticity"><i class="fa fa-check"></i><b>15.2.2</b> Non-Constant Variance, or Heteroscedasticity</a></li>
<li class="chapter" data-level="15.2.3" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#independence-of-e"><i class="fa fa-check"></i><b>15.2.3</b> Independence of <span class="math inline">\(E\)</span></a></li>
<li class="chapter" data-level="15.2.4" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#normality-of-the-residuals"><i class="fa fa-check"></i><b>15.2.4</b> Normality of the Residuals</a></li>
<li class="chapter" data-level="15.2.5" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#outliers-leverage-and-influence"><i class="fa fa-check"></i><b>15.2.5</b> Outliers, Leverage, and Influence</a></li>
<li class="chapter" data-level="15.2.6" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#outliers"><i class="fa fa-check"></i><b>15.2.6</b> Outliers</a></li>
<li class="chapter" data-level="15.2.7" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#multicollinearity"><i class="fa fa-check"></i><b>15.2.7</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#summary-10"><i class="fa fa-check"></i><b>15.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="logit-regression.html"><a href="logit-regression.html"><i class="fa fa-check"></i><b>16</b> Logit Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="logit-regression.html"><a href="logit-regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>16.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="16.2" data-path="logit-regression.html"><a href="logit-regression.html#logit-estimation"><i class="fa fa-check"></i><b>16.2</b> Logit Estimation</a><ul>
<li class="chapter" data-level="16.2.1" data-path="logit-regression.html"><a href="logit-regression.html#logit-hypothesis-tests"><i class="fa fa-check"></i><b>16.2.1</b> Logit Hypothesis Tests</a></li>
<li class="chapter" data-level="16.2.2" data-path="logit-regression.html"><a href="logit-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>16.2.2</b> Goodness of Fit</a></li>
<li class="chapter" data-level="16.2.3" data-path="logit-regression.html"><a href="logit-regression.html#interpreting-logits"><i class="fa fa-check"></i><b>16.2.3</b> Interpreting Logits</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="logit-regression.html"><a href="logit-regression.html#summary-11"><i class="fa fa-check"></i><b>16.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html"><i class="fa fa-check"></i><b>17</b> Appendix: Basic R</a><ul>
<li class="chapter" data-level="17.1" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#introduction-to-r"><i class="fa fa-check"></i><b>17.1</b> Introduction to R</a></li>
<li class="chapter" data-level="17.2" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#downloading-r-and-rstudio"><i class="fa fa-check"></i><b>17.2</b> Downloading R and RStudio</a></li>
<li class="chapter" data-level="17.3" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#introduction-to-programming"><i class="fa fa-check"></i><b>17.3</b> Introduction to Programming</a></li>
<li class="chapter" data-level="17.4" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#uploadingreading-data"><i class="fa fa-check"></i><b>17.4</b> Uploading/Reading Data</a></li>
<li class="chapter" data-level="17.5" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#data-manipulation-in-r"><i class="fa fa-check"></i><b>17.5</b> Data Manipulation in R</a></li>
<li class="chapter" data-level="17.6" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#savingwriting-data"><i class="fa fa-check"></i><b>17.6</b> Saving/Writing Data</a></li>
<li class="chapter" data-level="17.7" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#the-tidyverse"><i class="fa fa-check"></i><b>17.7</b> The Tidyverse</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-regression-and-model-building" class="section level1">
<h1><span class="header-section-number">13</span> Multiple Regression and Model Building</h1>
<p>This book focuses on the use of systematic quantitative analysis for purposes of building, refining and testing theoretical propositions in the policy and social sciences. All of the tools discussed so far – including univariate, bi-variate, and simple regression analysis – provide means to evaluate distributions and test hypotheses concerning simple relationships. Most policy and social theories, however, include multiple explanatory variables. <strong>Multiple regression</strong> extends the utility of simple regression by permitting the inclusion of two or more explanatory variables. This chapter discusses strategies for determining what variables to include (or exclude) in the model.</p>
<div id="model-building" class="section level2">
<h2><span class="header-section-number">13.1</span> Model Building</h2>
<p>Model building is the process of deciding which independent variables to include in the model.<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a> For our purposes, when deciding which variables to include, theory and findings from the extant literature should be the most prominent guides. Apart from theory, however, this chapter examines empirical strategies that can help determine if the addition of new variables improves overall model fit. In general, when adding a variable, check for: a) improved prediction based on empirical indicators, b) statistically and substantively significant estimated coefficients, and c) stability of model coefficients—do other coefficients change when adding the new one – particularly look for sign changes.</p>
<div id="theory-and-hypotheses" class="section level3">
<h3><span class="header-section-number">13.1.1</span> Theory and Hypotheses</h3>
<p>The most important guidance for deciding whether a variable (or variables) should be included in your model is provided by theory and prior research. Simply put, knowing the literature on your topic is vital to knowing what variables are important. You should be able to articulate a clear theoretical reason for including each variable in your model. In those cases where you don’t have much theoretical guidance, however, you should use model <em>parsimony</em>, which is a function of simplicity and model fit, as your guide. You can focus on whether the inclusion of a variable improves model fit. In the next section, we will explore several empirical indicators that can be used to evaluate the appropriateness of variable inclusion.</p>
</div>
<div id="empirical-indicators" class="section level3">
<h3><span class="header-section-number">13.1.2</span> Empirical Indicators</h3>
<p>When building a model, it is best to start with a few IV’s and then begin adding other variables. However, when adding a variable, check for:</p>
<ul>
<li>Improved prediction (increase in adjusted <span class="math inline">\(R^2\)</span>)</li>
<li>Statistically and substantively significant estimated coefficients</li>
<li>Stability of model coefficients</li>
<li>Do other coefficients change when adding the new one?</li>
<li>Particularly look for sign changes for estimated coefficients.</li>
</ul>
<div id="coefficient-of-determination-r2-1" class="section level4 unnumbered">
<h4>Coefficient of Determination: <span class="math inline">\(R^2\)</span></h4>
<p><span class="math inline">\(R^2\)</span> was previously discussed within the context of simple regression. The extension to multiple regression is straightforward, except that multiple regression leads us to place greater weight on the use of the <strong>adjusted <span class="math inline">\(R^2\)</span></strong>. Recall that the adjusted <span class="math inline">\(R^2\)</span> corrects for the inclusion of multiple independent variables; <span class="math inline">\(R^{2}\)</span> is the ratio of the explained sum of squares to the total sum of squares (<em>ESS/TSS</em>).</p>
<p><span class="math inline">\(R^2\)</span> is expressed as:</p>
<span class="math display" id="eq:13-1">\[\begin{equation}
 R^{2} = 1-\frac{RSS}{TSS}  
 \tag{13.1}
\end{equation}\]</span>
<p>However, this formulation of <span class="math inline">\(R^2\)</span> is insensitive to the complexity of the model and the degrees of freedom provided by your data. This means that an increase in the number of <span class="math inline">\(k\)</span> independent variables, can increase the <span class="math inline">\(R^2\)</span>. Adjusted <span class="math inline">\(R^2\)</span> penalizes the <span class="math inline">\(R^2\)</span> by correcting for the degrees of freedom. It is defined as:</p>
<span class="math display" id="eq:13-2">\[\begin{equation}
 \text{adjusted} R^2 = 1-\frac{\frac{RSS}{n-k-1}}{\frac{TSS}{n-k-1}}
 \tag{13.2}
\end{equation}\]</span>
<p>The <span class="math inline">\(R^2\)</span> of two models can be compared, as illustrated by the following example. The first (simpler) model consists of basic demographics (age, education, and income) as predictors of climate change risk. The second (more complex) model adds the variable measuring political ideology to the explanation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ds.temp &lt;-<span class="st"> </span><span class="kw">filter</span>(ds) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(glbcc_risk, age, education, income, ideol) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">na.omit</span>()

ols1 &lt;-<span class="st"> </span><span class="kw">lm</span>(glbcc_risk <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>income, <span class="dt">data =</span> ds.temp)
<span class="kw">summary</span>(ols1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glbcc_risk ~ age + education + income, data = ds.temp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9189 -2.0546  0.0828  2.5823  5.1908 
## 
## Coefficients:
##                 Estimate   Std. Error t value             Pr(&gt;|t|)    
## (Intercept)  6.160506689  0.342491831  17.987 &lt; 0.0000000000000002 ***
## age         -0.015571138  0.004519107  -3.446              0.00058 ***
## education    0.225285858  0.036572082   6.160       0.000000000858 ***
## income      -0.000005576  0.000001110  -5.022       0.000000551452 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.008 on 2268 degrees of freedom
## Multiple R-squared:  0.02565,    Adjusted R-squared:  0.02437 
## F-statistic: 19.91 on 3 and 2268 DF,  p-value: 0.0000000000009815</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ols2 &lt;-<span class="st"> </span><span class="kw">lm</span>(glbcc_risk <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>ideol, <span class="dt">data =</span> ds.temp)
<span class="kw">summary</span>(ols2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glbcc_risk ~ age + education + income + ideol, data = ds.temp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.7991 -1.6654  0.2246  1.4437  6.5968 
## 
## Coefficients:
##                  Estimate    Std. Error t value             Pr(&gt;|t|)    
## (Intercept) 10.9232861851  0.3092149750  35.326 &lt; 0.0000000000000002 ***
## age         -0.0044231931  0.0036688855  -1.206              0.22810    
## education    0.0632823391  0.0299443094   2.113              0.03468 *  
## income      -0.0000026033  0.0000009021  -2.886              0.00394 ** 
## ideol       -1.0366154295  0.0299166747 -34.650 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.433 on 2267 degrees of freedom
## Multiple R-squared:  0.363,  Adjusted R-squared:  0.3619 
## F-statistic:   323 on 4 and 2267 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>As can be seen by comparing the model results, the more complex model that includes political ideology has a higher <span class="math inline">\(R^2\)</span> than does the simpler model. This indicates that the more complex model explains a greater fraction of the variance in perceived risks of climate change. However, we don’t know if this improvement is statistically significant. In order to determine whether the more complex model adds significantly to the explanation of perceive risks, we can utilize the <span class="math inline">\(F\)</span>-test.</p>
</div>
<div id="f-test" class="section level4 unnumbered">
<h4><span class="math inline">\(F\)</span>-test</h4>
The <span class="math inline">\(F\)</span>-test is a test statistic based on the <span class="math inline">\(F\)</span> distribution, in the same way the the <span class="math inline">\(t\)</span>-test is based on the <span class="math inline">\(t\)</span> distribution. The <span class="math inline">\(F\)</span> distribution skews right and ranges between <span class="math inline">\(0\)</span> and <span class="math inline">\(\infty\)</span>. Just like the <span class="math inline">\(t\)</span> distribution, the <span class="math inline">\(F\)</span> distribution approaches normal as the degrees of freedom increase.^[Note that the <span class="math inline">\(F\)</span> distribution is the square of a <span class="math inline">\(t\)</span>-distributed variable with <span class="math inline">\(m\)</span> degrees of freedom. The <span class="math inline">\(F\)</span> distribution has <span class="math inline">\(1\)</span> degree of freedom in the numerator and <span class="math inline">\(m\)</span> degrees of in the denominator:
<span class="math display">\[\begin{equation*}
  t^2_m = F_{1,m} 
\end{equation*}\]</span>
<p>]</p>
<p><span class="math inline">\(F\)</span>-tests are used to test for the statistical significance of the overall model fit. The null hypothesis for an <span class="math inline">\(F\)</span>-test is that the model offers no improvement for predicting <span class="math inline">\(Y_i\)</span> over the mean of <span class="math inline">\(Y\)</span>, <span class="math inline">\(\bar{Y}\)</span>.</p>
<p>The formula for the <span class="math inline">\(F\)</span>-test is:</p>
<span class="math display" id="eq:13-3">\[\begin{equation}
  F = \frac{\frac{ESS}{k}}{\frac{RSS}{n-k-1}}
  \tag{13.3}
\end{equation}\]</span>
<p>where <span class="math inline">\(k\)</span> is the number of parameters and <span class="math inline">\(n-k-1\)</span> are the degrees of freedom. Therefore, <span class="math inline">\(F\)</span> is a ratio of the explained variance to the residual variance, correcting for the number of observations and parameters. The <span class="math inline">\(F\)</span>-value is compared to the <span class="math inline">\(F\)</span>-distribution, just like a <span class="math inline">\(t\)</span>-distribution, to obtain a <span class="math inline">\(p\)</span>-value. Note that the <code>R</code> output includes the <span class="math inline">\(F\)</span> statistic and <span class="math inline">\(p\)</span> value.</p>
</div>
<div id="nested-f-test" class="section level4 unnumbered">
<h4>Nested <span class="math inline">\(F\)</span>-test</h4>
<p>For model building we turn to the nested <span class="math inline">\(F\)</span>-test, which tests whether a more complex model (with more IVs) adds to the explanatory power over a simpler model (with fewer IVs). To find out, we calculate an F-statistic for the model improvement:</p>
<span class="math display" id="eq:13-4">\[\begin{equation}
  F = \frac{\frac{ESS_1-ESS_0}{q}}{\frac{RSS_1}{n-k-1}}
  \tag{13.4}
\end{equation}\]</span>
<p>where <span class="math inline">\(q\)</span> is the difference in the number of IVs between the simpler and the more complex models. The complex model has <span class="math inline">\(k\)</span> IVs (and estimates <span class="math inline">\(k\)</span> parameters), and the simpler model has <span class="math inline">\(k-q\)</span> IVs (and estimates only <span class="math inline">\(k-q\)</span> parameters). <span class="math inline">\(ESS_1\)</span> is the explained sum of squares for the complex model. <span class="math inline">\(RSS_1\)</span> is the residual sum of squares for the complex model. <span class="math inline">\(ESS_0\)</span> is the explained sum of squares for the simpler model. So the nested-F represents the ratio of the additional explanation per added IV, over the residual sum of squares divided by the model degrees of freedom.</p>
<p>We can use <code>R</code>, to calculate the <span class="math inline">\(F\)</span> statistic based on our previous example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TSS &lt;-<span class="st"> </span><span class="kw">sum</span>((ds.temp<span class="op">$</span>glbcc_risk<span class="op">-</span><span class="kw">mean</span>(ds.temp<span class="op">$</span>glbcc_risk))<span class="op">^</span><span class="dv">2</span>)
TSS</code></pre></div>
<pre><code>## [1] 21059.86</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">RSS.mod1 &lt;-<span class="st"> </span><span class="kw">sum</span>(ols1<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)
RSS.mod1</code></pre></div>
<pre><code>## [1] 20519.57</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ESS.mod1 &lt;-<span class="st"> </span>TSS<span class="op">-</span>RSS.mod1
ESS.mod1</code></pre></div>
<pre><code>## [1] 540.2891</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">RSS.mod2 &lt;-<span class="st"> </span><span class="kw">sum</span>(ols2<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)
RSS.mod2</code></pre></div>
<pre><code>## [1] 13414.89</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ESS.mod2 &lt;-<span class="st"> </span>TSS<span class="op">-</span>RSS.mod2
ESS.mod2</code></pre></div>
<pre><code>## [1] 7644.965</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">F &lt;-<span class="st"> </span>((ESS.mod2 <span class="op">-</span><span class="st"> </span>ESS.mod1)<span class="op">/</span><span class="dv">1</span>)<span class="op">/</span>(RSS.mod2<span class="op">/</span>(<span class="kw">length</span>(ds.temp<span class="op">$</span>glbcc_risk)<span class="op">-</span><span class="dv">4</span><span class="op">-</span><span class="dv">1</span>))
F</code></pre></div>
<pre><code>## [1] 1200.629</code></pre>
<p>Or, you can simply use the <code>anova</code> function in <span class="math inline">\(R\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(ols1,ols2) </code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: glbcc_risk ~ age + education + income
## Model 2: glbcc_risk ~ age + education + income + ideol
##   Res.Df   RSS Df Sum of Sq      F                Pr(&gt;F)    
## 1   2268 20520                                              
## 2   2267 13415  1    7104.7 1200.6 &lt; 0.00000000000000022 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As shown using both approaches, the inclusion of ideology significantly improves model fit.</p>
</div>
</div>
<div id="risks-in-model-building" class="section level3">
<h3><span class="header-section-number">13.1.3</span> Risks in Model Building</h3>
<p>As is true of most things in life, there are risks to consider when building statistical models. First, are you including irrelevant <span class="math inline">\(X\)</span>’s? These can increase model complexity, reduce adjusted <span class="math inline">\(R^2\)</span>, and increase model variability across samples. Remember that you should have a theoretical basis for inclusion of all of the variables in your model.</p>
<p>Second, are you omitting relevant <span class="math inline">\(X\)</span>’s? Not including important variables can fail to capture fit and can bias other estimated coefficients, particularly when the omitted <span class="math inline">\(X\)</span> is related to both other <span class="math inline">\(X\)</span>’s and to the dependent variable <span class="math inline">\(Y\)</span>.</p>
<p>Finally, remember that we are using sample data. Therefore, about 5% of the time, our sample will include random observations of <span class="math inline">\(X\)</span>’s that result in <span class="math inline">\(B\)</span>’s that meet classical hypothesis tests – resulting in a Type I error. Conversely, the <span class="math inline">\(B\)</span>’s may be important, but the sample data will randomly include observations of <span class="math inline">\(X\)</span> that result in estimated parameters that do not meet the classical statistical tests – resulting in a Type II error. That’s why we rely on theory, prior hypotheses, and replication.</p>
</div>
</div>
<div id="evils-of-stepwise-regression" class="section level2">
<h2><span class="header-section-number">13.2</span> Evils of Stepwise Regression</h2>
<p>Almost all statistical software packages (including <span class="math inline">\(R\)</span>) permit a number of mechanical <code>search strategies&quot; for finding IVs that make a statistically significant contribution to the prediction of the model dependent variable. The most common of these is called __stepwise regression__, which may also be referred to as forward, backward (or maybe even upside down!) stepwise regression. Stepwise procedures do not require that the analyst think -- you just have to designate a pool of possible IVs and let the package go to work, sifting through the IVs to identify those that (on the basis of your sample data) appear to be related to the model dependent variable.  The stepwise procedures use sequential F-tests, sequentially adding variables that</code>improve the fit&quot; of the mindless model until there are no more IVs that meet some threshold (usually <span class="math inline">\(p&lt;0.05\)</span>) of statistical significance. These procedures are like mechanically wringing all of the explanation you can get for <span class="math inline">\(Y\)</span> out of some pool of <span class="math inline">\(X\)</span>.</p>
<p>You should already recognize that these kind of methods pose serious problems. First and foremost, this is an atheoretical approach to model building. But, what if you have no theory to start with – is a stepwise approach appropriate then? No, for several reasons. If any of the candidate <span class="math inline">\(X\)</span> variables are strongly correlated, the inclusion of the first one will ``use up&quot; some of the explanation of the second, because of the way OLS calculates partial regression coefficients. For that reason, once one of the variables is mechanically selected, the other will tend to be excluded because it will have less to contribute to <span class="math inline">\(Y\)</span>. Perhaps more damning, stepwise approaches are highly susceptible to inclusion of spuriously related variables. Recall that we are using samples, drawn from the larger population, and that samples are subject to random variation. If the step-wise process uses the classical 0.05 cut-off for inclusion of a variable, that means that one time in twenty (in the long run) we will include a variable that meets the criterion only by random chance.<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> Recall that the classical hypothesis test requires that we specify our hypothesis in advance; step-wise processes simply rummage around within a set of potential IVs to find those that fit.</p>
<p>There have been notable cases in which mechanical model building has resulted in seriously problematic “findings” that have very costly implications for society. One is recounted in the PBS Frontline episode called “Currents of Fear”.<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> The story concerns whether electromagnetic fields (EMFs) from technologies including high-voltage power lines cause cancer in people who are exposed. The problem was that ``cancer clusters&quot; could be identified that were proximate to the power lines, but no laboratory experiments could find a connection. However, concerned citizens and activists persisted in believing there was a causal relationship. In that context, the Swedish government sponsored a very ambitious study to settle the question. Here is the text of the discussion from the Frontline program:</p>
<blockquote>
<p>… in 1992, a landmark study appeared from Sweden. A huge investigation, it enrolled everyone living within 300 meters of Sweden’s high-voltage transmission line system over a 25-year period. They went far beyond all previous studies in their efforts to measure magnetic fields, calculating the fields that the children were exposed to at the time of their cancer diagnosis and before. This study reported an apparently clear association between magnetic field exposure and childhood leukemia, with a risk ratio for the most highly exposed of nearly 4.</p>
</blockquote>
<blockquote>
<p>The Swedish government announced it was investigating new policy options, including whether to move children away from schools near power lines. Surely, here was the proof that power lines were dangerous, the proof that even the physicists and biological naysayers would have to accept. But three years after the study was published, the Swedish research no longer looks so unassailable. This is a copy of the original contractor’s report, which reveals the remarkable thoroughness of the Swedish team. Unlike the published article, which just summarizes part of the data, the report shows everything they did in great detail, all the things they measured and all the comparisons they made.</p>
</blockquote>
<blockquote>
<p>When scientists saw how many things they had measured – nearly 800 risk ratios are in the report – they began accusing the Swedes of falling into one of the most fundamental errors in epidemiology, sometimes called the multiple comparisons fallacy.</p>
</blockquote>
<p>So, according to the Frontline report, the Swedish EMF study regressed the incidence of nearly 800 possible cancers onto the proximity of its citizens to high-voltage power lines. In some cases, there appeared to be a positive relationship. These they reported. In other cases, there was no relationship, and in some the relationship was negative - which would seem to imply (if you were so silly as to do so) that living near the high voltage lines actually protected people from cancer. But only the positive relationships were included in the reports, leading to a false impression that the study had confirmed that proximity to high-voltage lines causes cancer. Embarrassing to the study authors, to put it mildly.</p>
</div>
<div id="summary-8" class="section level2">
<h2><span class="header-section-number">13.3</span> Summary</h2>
<p>This chapter has focused on multiple regression model building. The keys to that process are understanding (a) the critical role of theory and prior research findings in model specification, and (b) the meaning of the partial regression coefficients produced by OLS. When theory is not well-developed, you can thoughtfully employ nested F-tests to evaluate whether the hypothesized inclusion of an <span class="math inline">\(X\)</span> variable meaningfully contributes to the explanation of <span class="math inline">\(Y\)</span>. But you should avoid reliance on mechanical model-building routines, like step-wise regression, because these can lead you down into statistical perdition. None of us want to see that happen!</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="22">
<li id="fn22"><p>Model building also concerns decisions about model functional form, which we address in the next chapter.<a href="multiple-regression-and-model-building.html#fnref22">↩</a></p></li>
<li id="fn23"><p>Add to that the propensity of journals to publish articles that have new and exciting findings, in the form of statistically significant modeled coefficients, and you can see that there would be a substantial risk: that of finding and promoting nonsense findings.<a href="multiple-regression-and-model-building.html#fnref23">↩</a></p></li>
<li id="fn24"><p>The program was written, produced and directed by Jon Palfreman, and it was first broadcast on June 13, 1995. The full transcript can be found here: <a href="http://www.pbs.org/wgbh/pages/frontline/programs/transcripts/1319.html" class="uri">http://www.pbs.org/wgbh/pages/frontline/programs/transcripts/1319.html</a><a href="multiple-regression-and-model-building.html#fnref24">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-logic-of-multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="topics-in-multiple-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
