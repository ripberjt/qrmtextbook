<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>8 Linear Estimation and Minimizing Error | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R</title>
  <meta name="description" content="8 Linear Estimation and Minimizing Error | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="8 Linear Estimation and Minimizing Error | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Linear Estimation and Minimizing Error | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R" />
  
  
  

<meta name="author" content="Hank Jenkins-Smith">
<meta name="author" content="Joseph Ripberger">
<meta name="author" content="Gary Copeland">
<meta name="author" content="Matthew Nowlin">
<meta name="author" content="Tyler Hughes">
<meta name="author" content="Aaron Fister">
<meta name="author" content="Wesley Wehde">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="the-logic-of-ordinary-least-squares-estimation.html">
<link rel="next" href="bi-variate-hypothesis-testing-and-model-fit.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface and Acknowledgments</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#copyright"><i class="fa fa-check"></i>Copyright</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html"><i class="fa fa-check"></i><b>1</b> Theories and Social Science</a><ul>
<li class="chapter" data-level="1.1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#the-scientific-method"><i class="fa fa-check"></i><b>1.1</b> The Scientific Method</a></li>
<li class="chapter" data-level="1.2" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-and-empirical-research"><i class="fa fa-check"></i><b>1.2</b> Theory and Empirical Research</a><ul>
<li class="chapter" data-level="1.2.1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#coherent-and-internally-consistent"><i class="fa fa-check"></i><b>1.2.1</b> Coherent and Internally Consistent</a></li>
<li class="chapter" data-level="1.2.2" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theories-and-causality"><i class="fa fa-check"></i><b>1.2.2</b> Theories and Causality</a></li>
<li class="chapter" data-level="1.2.3" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#generation-of-testable-hypothesis"><i class="fa fa-check"></i><b>1.2.3</b> Generation of Testable Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-and-functions"><i class="fa fa-check"></i><b>1.3</b> Theory and Functions</a></li>
<li class="chapter" data-level="1.4" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-in-social-science"><i class="fa fa-check"></i><b>1.4</b> Theory in Social Science</a></li>
<li class="chapter" data-level="1.5" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#outline-of-the-book"><i class="fa fa-check"></i><b>1.5</b> Outline of the Book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="research-design.html"><a href="research-design.html"><i class="fa fa-check"></i><b>2</b> Research Design</a><ul>
<li class="chapter" data-level="2.1" data-path="research-design.html"><a href="research-design.html#overview-of-the-research-process"><i class="fa fa-check"></i><b>2.1</b> Overview of the Research Process</a></li>
<li class="chapter" data-level="2.2" data-path="research-design.html"><a href="research-design.html#internal-and-external-validity"><i class="fa fa-check"></i><b>2.2</b> Internal and External Validity</a></li>
<li class="chapter" data-level="2.3" data-path="research-design.html"><a href="research-design.html#major-classes-of-designs"><i class="fa fa-check"></i><b>2.3</b> Major Classes of Designs</a></li>
<li class="chapter" data-level="2.4" data-path="research-design.html"><a href="research-design.html#threats-to-validity"><i class="fa fa-check"></i><b>2.4</b> Threats to Validity</a></li>
<li class="chapter" data-level="2.5" data-path="research-design.html"><a href="research-design.html#some-common-designs"><i class="fa fa-check"></i><b>2.5</b> Some Common Designs</a></li>
<li class="chapter" data-level="2.6" data-path="research-design.html"><a href="research-design.html#plan-meets-reality"><i class="fa fa-check"></i><b>2.6</b> Plan Meets Reality</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html"><i class="fa fa-check"></i><b>3</b> Exploring and Visualizing Data</a><ul>
<li class="chapter" data-level="3.1" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#characterizing-data"><i class="fa fa-check"></i><b>3.1</b> Characterizing Data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#central-tendency"><i class="fa fa-check"></i><b>3.1.1</b> Central Tendency</a></li>
<li class="chapter" data-level="3.1.2" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#level-of-measurement-and-central-tendency"><i class="fa fa-check"></i><b>3.1.2</b> Level of Measurement and Central Tendency</a></li>
<li class="chapter" data-level="3.1.3" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#moments"><i class="fa fa-check"></i><b>3.1.3</b> Moments</a></li>
<li class="chapter" data-level="3.1.4" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#first-moment-expected-value"><i class="fa fa-check"></i><b>3.1.4</b> First Moment – Expected Value</a></li>
<li class="chapter" data-level="3.1.5" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#the-second-moment-variance-and-standard-deviation"><i class="fa fa-check"></i><b>3.1.5</b> The Second Moment – Variance and Standard Deviation</a></li>
<li class="chapter" data-level="3.1.6" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#the-third-moment-skewness"><i class="fa fa-check"></i><b>3.1.6</b> The Third Moment – Skewness</a></li>
<li class="chapter" data-level="3.1.7" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#the-fourth-moment-kurtosis"><i class="fa fa-check"></i><b>3.1.7</b> The Fourth Moment – Kurtosis</a></li>
<li class="chapter" data-level="3.1.8" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#order-statistics"><i class="fa fa-check"></i><b>3.1.8</b> Order Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#summary"><i class="fa fa-check"></i><b>3.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#finding-probabilities"><i class="fa fa-check"></i><b>4.1</b> Finding Probabilities</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#finding-probabilities-with-the-normal-curve"><i class="fa fa-check"></i><b>4.2</b> Finding Probabilities with the Normal Curve</a></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#summary-1"><i class="fa fa-check"></i><b>4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>5</b> Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="inference.html"><a href="inference.html#inference-populations-and-samples"><i class="fa fa-check"></i><b>5.1</b> Inference: Populations and Samples</a><ul>
<li class="chapter" data-level="5.1.1" data-path="inference.html"><a href="inference.html#populations-and-samples"><i class="fa fa-check"></i><b>5.1.1</b> Populations and Samples</a></li>
<li class="chapter" data-level="5.1.2" data-path="inference.html"><a href="inference.html#sampling-and-knowing"><i class="fa fa-check"></i><b>5.1.2</b> Sampling and Knowing</a></li>
<li class="chapter" data-level="5.1.3" data-path="inference.html"><a href="inference.html#sampling-strategies"><i class="fa fa-check"></i><b>5.1.3</b> Sampling Strategies</a></li>
<li class="chapter" data-level="5.1.4" data-path="inference.html"><a href="inference.html#sampling-techniques"><i class="fa fa-check"></i><b>5.1.4</b> Sampling Techniques</a></li>
<li class="chapter" data-level="5.1.5" data-path="inference.html"><a href="inference.html#so-how-is-it-that-we-know"><i class="fa fa-check"></i><b>5.1.5</b> So How is it That We Know?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inference.html"><a href="inference.html#the-normal-distribution"><i class="fa fa-check"></i><b>5.2</b> The Normal Distribution</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inference.html"><a href="inference.html#standardizing-a-normal-distribution-and-z-scores"><i class="fa fa-check"></i><b>5.2.1</b> Standardizing a Normal Distribution and Z-scores</a></li>
<li class="chapter" data-level="5.2.2" data-path="inference.html"><a href="inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.3" data-path="inference.html"><a href="inference.html#populations-samples-and-symbols"><i class="fa fa-check"></i><b>5.2.3</b> Populations, Samples and Symbols</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference.html"><a href="inference.html#inferences-to-the-population-from-the-sample"><i class="fa fa-check"></i><b>5.3</b> Inferences to the Population from the Sample</a><ul>
<li class="chapter" data-level="5.3.1" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference.html"><a href="inference.html#the-logic-of-hypothesis-testing"><i class="fa fa-check"></i><b>5.3.2</b> The Logic of Hypothesis Testing</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference.html"><a href="inference.html#some-miscellaneous-notes-about-hypothesis-testing"><i class="fa fa-check"></i><b>5.3.3</b> Some Miscellaneous Notes about Hypothesis Testing</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="inference.html"><a href="inference.html#differences-between-groups"><i class="fa fa-check"></i><b>5.4</b> Differences Between Groups</a><ul>
<li class="chapter" data-level="5.4.1" data-path="inference.html"><a href="inference.html#t-tests"><i class="fa fa-check"></i><b>5.4.1</b> <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="inference.html"><a href="inference.html#summary-2"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="association-of-variables.html"><a href="association-of-variables.html"><i class="fa fa-check"></i><b>6</b> Association of Variables</a><ul>
<li class="chapter" data-level="6.1" data-path="association-of-variables.html"><a href="association-of-variables.html#cross-tabulation"><i class="fa fa-check"></i><b>6.1</b> Cross-Tabulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="association-of-variables.html"><a href="association-of-variables.html#crosstabulation-and-control"><i class="fa fa-check"></i><b>6.1.1</b> Crosstabulation and Control</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="association-of-variables.html"><a href="association-of-variables.html#covariance"><i class="fa fa-check"></i><b>6.2</b> Covariance</a></li>
<li class="chapter" data-level="6.3" data-path="association-of-variables.html"><a href="association-of-variables.html#correlation"><i class="fa fa-check"></i><b>6.3</b> Correlation</a></li>
<li class="chapter" data-level="6.4" data-path="association-of-variables.html"><a href="association-of-variables.html#scatterplots"><i class="fa fa-check"></i><b>6.4</b> Scatterplots</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html"><i class="fa fa-check"></i><b>7</b> The Logic of Ordinary Least Squares Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#theoretical-models"><i class="fa fa-check"></i><b>7.1</b> Theoretical Models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#deterministic-linear-model"><i class="fa fa-check"></i><b>7.1.1</b> Deterministic Linear Model</a></li>
<li class="chapter" data-level="7.1.2" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#stochastic-linear-model"><i class="fa fa-check"></i><b>7.1.2</b> Stochastic Linear Model</a></li>
<li class="chapter" data-level="7.1.3" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#assumptions-about-the-error-term"><i class="fa fa-check"></i><b>7.1.3</b> Assumptions about the Error Term</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#estimating-linear-models"><i class="fa fa-check"></i><b>7.2</b> Estimating Linear Models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#residuals"><i class="fa fa-check"></i><b>7.2.1</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#an-example-of-simple-regression"><i class="fa fa-check"></i><b>7.3</b> An Example of Simple Regression</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html"><i class="fa fa-check"></i><b>8</b> Linear Estimation and Minimizing Error</a><ul>
<li class="chapter" data-level="8.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#minimizing-error-using-derivatives"><i class="fa fa-check"></i><b>8.1</b> Minimizing Error using Derivatives</a><ul>
<li class="chapter" data-level="8.1.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#rules-of-derivation"><i class="fa fa-check"></i><b>8.1.1</b> Rules of Derivation</a></li>
<li class="chapter" data-level="8.1.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#critical-points"><i class="fa fa-check"></i><b>8.1.2</b> Critical Points</a></li>
<li class="chapter" data-level="8.1.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#partial-derivation"><i class="fa fa-check"></i><b>8.1.3</b> Partial Derivation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#deriving-ols-estimators"><i class="fa fa-check"></i><b>8.2</b> Deriving OLS Estimators</a><ul>
<li class="chapter" data-level="8.2.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#ols-derivation-of-hatalpha"><i class="fa fa-check"></i><b>8.2.1</b> OLS Derivation of <span class="math inline">\(\hat{\alpha}\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#ols-derivation-of-hatbeta"><i class="fa fa-check"></i><b>8.2.2</b> OLS Derivation of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li class="chapter" data-level="8.2.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#interpreting-hatbeta-and-hatalpha"><i class="fa fa-check"></i><b>8.2.3</b> Interpreting <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\alpha}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#summary-3"><i class="fa fa-check"></i><b>8.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html"><i class="fa fa-check"></i><b>9</b> Bi-Variate Hypothesis Testing and Model Fit</a><ul>
<li class="chapter" data-level="9.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#hypothesis-tests-for-regression-coefficients"><i class="fa fa-check"></i><b>9.1</b> Hypothesis Tests for Regression Coefficients</a><ul>
<li class="chapter" data-level="9.1.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#residual-standard-error"><i class="fa fa-check"></i><b>9.1.1</b> Residual Standard Error</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#measuring-goodness-of-fit"><i class="fa fa-check"></i><b>9.2</b> Measuring Goodness of Fit</a><ul>
<li class="chapter" data-level="9.2.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#sample-covariance-and-correlations"><i class="fa fa-check"></i><b>9.2.1</b> Sample Covariance and Correlations</a></li>
<li class="chapter" data-level="9.2.2" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>9.2.2</b> Coefficient of Determination: <span class="math inline">\(R^{2}\)</span></a></li>
<li class="chapter" data-level="9.2.3" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#visualizing-bivariate-regression"><i class="fa fa-check"></i><b>9.2.3</b> Visualizing Bivariate Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#summary-4"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html"><i class="fa fa-check"></i><b>10</b> OLS Assumptions and Simple Regression Diagnostics</a><ul>
<li class="chapter" data-level="10.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#a-recap-of-modeling-assumptions"><i class="fa fa-check"></i><b>10.1</b> A Recap of Modeling Assumptions</a></li>
<li class="chapter" data-level="10.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#when-things-go-bad-with-residuals"><i class="fa fa-check"></i><b>10.2</b> When Things Go Bad with Residuals</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#outlier-data"><i class="fa fa-check"></i><b>10.2.1</b> “Outlier” Data</a></li>
<li class="chapter" data-level="10.2.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#non-constant-variance"><i class="fa fa-check"></i><b>10.2.2</b> Non-Constant Variance</a></li>
<li class="chapter" data-level="10.2.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#non-linearity-in-the-parameters"><i class="fa fa-check"></i><b>10.2.3</b> Non-Linearity in the Parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#application-of-residual-diagnostics"><i class="fa fa-check"></i><b>10.3</b> Application of Residual Diagnostics</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-non-linearity"><i class="fa fa-check"></i><b>10.3.1</b> Testing for Non-Linearity</a></li>
<li class="chapter" data-level="10.3.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-normality-in-model-residuals"><i class="fa fa-check"></i><b>10.3.2</b> Testing for Normality in Model Residuals</a></li>
<li class="chapter" data-level="10.3.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-non-constant-variance-in-the-residuals"><i class="fa fa-check"></i><b>10.3.3</b> Testing for Non-Constant Variance in the Residuals</a></li>
<li class="chapter" data-level="10.3.4" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#examining-outlier-data"><i class="fa fa-check"></i><b>10.3.4</b> Examining Outlier Data</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#so-now-what-implications-of-residual-analysis"><i class="fa fa-check"></i><b>10.4</b> So Now What? Implications of Residual Analysis</a></li>
<li class="chapter" data-level="10.5" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#summary-5"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html"><i class="fa fa-check"></i><b>11</b> Introduction to Multiple Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-algebra-and-multiple-regression"><i class="fa fa-check"></i><b>11.1</b> Matrix Algebra and Multiple Regression</a></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#the-basics-of-matrix-algebra"><i class="fa fa-check"></i><b>11.2</b> The Basics of Matrix Algebra</a><ul>
<li class="chapter" data-level="11.2.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-basics"><i class="fa fa-check"></i><b>11.2.1</b> Matrix Basics</a></li>
<li class="chapter" data-level="11.2.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#vectors"><i class="fa fa-check"></i><b>11.2.2</b> Vectors</a></li>
<li class="chapter" data-level="11.2.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-operations"><i class="fa fa-check"></i><b>11.2.3</b> Matrix Operations</a></li>
<li class="chapter" data-level="11.2.4" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#transpose"><i class="fa fa-check"></i><b>11.2.4</b> Transpose</a></li>
<li class="chapter" data-level="11.2.5" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#adding-matrices"><i class="fa fa-check"></i><b>11.2.5</b> Adding Matrices</a></li>
<li class="chapter" data-level="11.2.6" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#multiplication-of-matrices"><i class="fa fa-check"></i><b>11.2.6</b> Multiplication of Matrices</a></li>
<li class="chapter" data-level="11.2.7" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#identity-matrices"><i class="fa fa-check"></i><b>11.2.7</b> Identity Matrices</a></li>
<li class="chapter" data-level="11.2.8" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-inversion"><i class="fa fa-check"></i><b>11.2.8</b> Matrix Inversion</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#ols-regression-in-matrix-form"><i class="fa fa-check"></i><b>11.3</b> OLS Regression in Matrix Form</a></li>
<li class="chapter" data-level="11.4" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#summary-6"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html"><i class="fa fa-check"></i><b>12</b> The Logic of Multiple Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#theoretical-specification"><i class="fa fa-check"></i><b>12.1</b> Theoretical Specification</a><ul>
<li class="chapter" data-level="12.1.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#assumptions-of-ols-regression"><i class="fa fa-check"></i><b>12.1.1</b> Assumptions of OLS Regression</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#partial-effects"><i class="fa fa-check"></i><b>12.2</b> Partial Effects</a></li>
<li class="chapter" data-level="12.3" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#multiple-regression-example"><i class="fa fa-check"></i><b>12.3</b> Multiple Regression Example</a><ul>
<li class="chapter" data-level="12.3.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#hypothesis-testing-and-t-tests"><i class="fa fa-check"></i><b>12.3.1</b> Hypothesis Testing and <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#summary-7"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html"><i class="fa fa-check"></i><b>13</b> Multiple Regression and Model Building</a><ul>
<li class="chapter" data-level="13.1" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#model-building"><i class="fa fa-check"></i><b>13.1</b> Model Building</a><ul>
<li class="chapter" data-level="13.1.1" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#theory-and-hypotheses"><i class="fa fa-check"></i><b>13.1.1</b> Theory and Hypotheses</a></li>
<li class="chapter" data-level="13.1.2" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#empirical-indicators"><i class="fa fa-check"></i><b>13.1.2</b> Empirical Indicators</a></li>
<li class="chapter" data-level="13.1.3" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#risks-in-model-building"><i class="fa fa-check"></i><b>13.1.3</b> Risks in Model Building</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#evils-of-stepwise-regression"><i class="fa fa-check"></i><b>13.2</b> Evils of Stepwise Regression</a></li>
<li class="chapter" data-level="13.3" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#summary-8"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html"><i class="fa fa-check"></i><b>14</b> Topics in Multiple Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#dummy-variables"><i class="fa fa-check"></i><b>14.1</b> Dummy Variables</a></li>
<li class="chapter" data-level="14.2" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#interaction-effects"><i class="fa fa-check"></i><b>14.2</b> Interaction Effects</a></li>
<li class="chapter" data-level="14.3" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#standardized-regression-coefficients"><i class="fa fa-check"></i><b>14.3</b> Standardized Regression Coefficients</a></li>
<li class="chapter" data-level="14.4" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#summary-9"><i class="fa fa-check"></i><b>14.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html"><i class="fa fa-check"></i><b>15</b> The Art of Regression Diagnostics</a><ul>
<li class="chapter" data-level="15.1" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#ols-error-assumptions-revisited"><i class="fa fa-check"></i><b>15.1</b> OLS Error Assumptions Revisited</a></li>
<li class="chapter" data-level="15.2" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#ols-diagnostic-techniques"><i class="fa fa-check"></i><b>15.2</b> OLS Diagnostic Techniques</a><ul>
<li class="chapter" data-level="15.2.1" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#non-linearity"><i class="fa fa-check"></i><b>15.2.1</b> Non-Linearity</a></li>
<li class="chapter" data-level="15.2.2" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#non-constant-variance-or-heteroscedasticity"><i class="fa fa-check"></i><b>15.2.2</b> Non-Constant Variance, or Heteroscedasticity</a></li>
<li class="chapter" data-level="15.2.3" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#independence-of-e"><i class="fa fa-check"></i><b>15.2.3</b> Independence of <span class="math inline">\(E\)</span></a></li>
<li class="chapter" data-level="15.2.4" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#normality-of-the-residuals"><i class="fa fa-check"></i><b>15.2.4</b> Normality of the Residuals</a></li>
<li class="chapter" data-level="15.2.5" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#outliers-leverage-and-influence"><i class="fa fa-check"></i><b>15.2.5</b> Outliers, Leverage, and Influence</a></li>
<li class="chapter" data-level="15.2.6" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#outliers"><i class="fa fa-check"></i><b>15.2.6</b> Outliers</a></li>
<li class="chapter" data-level="15.2.7" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#multicollinearity"><i class="fa fa-check"></i><b>15.2.7</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#summary-10"><i class="fa fa-check"></i><b>15.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="logit-regression.html"><a href="logit-regression.html"><i class="fa fa-check"></i><b>16</b> Logit Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="logit-regression.html"><a href="logit-regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>16.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="16.2" data-path="logit-regression.html"><a href="logit-regression.html#logit-estimation"><i class="fa fa-check"></i><b>16.2</b> Logit Estimation</a><ul>
<li class="chapter" data-level="16.2.1" data-path="logit-regression.html"><a href="logit-regression.html#logit-hypothesis-tests"><i class="fa fa-check"></i><b>16.2.1</b> Logit Hypothesis Tests</a></li>
<li class="chapter" data-level="16.2.2" data-path="logit-regression.html"><a href="logit-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>16.2.2</b> Goodness of Fit</a></li>
<li class="chapter" data-level="16.2.3" data-path="logit-regression.html"><a href="logit-regression.html#interpreting-logits"><i class="fa fa-check"></i><b>16.2.3</b> Interpreting Logits</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="logit-regression.html"><a href="logit-regression.html#summary-11"><i class="fa fa-check"></i><b>16.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html"><i class="fa fa-check"></i><b>17</b> Appendix: Basic R</a><ul>
<li class="chapter" data-level="17.1" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#introduction-to-r"><i class="fa fa-check"></i><b>17.1</b> Introduction to R</a></li>
<li class="chapter" data-level="17.2" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#downloading-r-and-rstudio"><i class="fa fa-check"></i><b>17.2</b> Downloading R and RStudio</a></li>
<li class="chapter" data-level="17.3" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#introduction-to-programming"><i class="fa fa-check"></i><b>17.3</b> Introduction to Programming</a></li>
<li class="chapter" data-level="17.4" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#uploadingreading-data"><i class="fa fa-check"></i><b>17.4</b> Uploading/Reading Data</a></li>
<li class="chapter" data-level="17.5" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#data-manipulation-in-r"><i class="fa fa-check"></i><b>17.5</b> Data Manipulation in R</a></li>
<li class="chapter" data-level="17.6" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#savingwriting-data"><i class="fa fa-check"></i><b>17.6</b> Saving/Writing Data</a></li>
<li class="chapter" data-level="17.7" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#the-tidyverse"><i class="fa fa-check"></i><b>17.7</b> The Tidyverse</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-estimation-and-minimizing-error" class="section level1">
<h1><span class="header-section-number">8</span> Linear Estimation and Minimizing Error</h1>
<p>As noted in the last chapter, the objective when estimating a linear model is to minimize the aggregate of the squared error. Specifically, when estimating a linear model, <span class="math inline">\(Y=A+BX+E\)</span>, we seek to find the values of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> that minimize the <span class="math inline">\(\sum \epsilon^{2}\)</span>. To accomplish this, we use calculus.</p>
<div id="minimizing-error-using-derivatives" class="section level2">
<h2><span class="header-section-number">8.1</span> Minimizing Error using Derivatives</h2>
<p>In calculus, the <em>derivative</em> is a measure the slope of any function of x, or <span class="math inline">\(f(x)\)</span>, at each given value of <span class="math inline">\(x\)</span>. For the function <span class="math inline">\(f(x)\)</span>, the derivative is denoted as <span class="math inline">\(f&#39;(x)\)</span> or, pronounced as <strong>“f prime x”</strong>. Because the formula for <span class="math inline">\(\sum \epsilon^{2}\)</span> is known, and can be treated as a function, the derivative of that function permits the calculation of the change in the sum of the squared error over each possible value of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>. For that reason we need to find the derivative for <span class="math inline">\(\sum \epsilon^{2}\)</span> with respect to changes in <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>. That, in turn, will permit us to “derive” the values of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> that result in the lowest possible <span class="math inline">\(\sum \epsilon^{2}\)</span>.</p>
<p>Look – we understand that this all sounds complicated. But it’s not all <em>that</em> complicated. In this chapter we will walk through all the steps so you’ll see that its really rather simple and, well, elegant. You will see that differential calculus (the kind of calculus that is concerned with rates of change) is built on a set of clearly defined rules for finding the derivative for any function <span class="math inline">\(f(x)\)</span>. It’s like solving a puzzle. The next section outlines these rules, so we can start solving puzzles.</p>
<div id="rules-of-derivation" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Rules of Derivation</h3>
<div id="derivative-rules" class="section level4 unnumbered">
<h4>Derivative Rules</h4>
<ol style="list-style-type: decimal">
<li>Power Rule
</li>
<li>Constant Rule
</li>
<li>A Constant Times a Function
</li>
<li>Differentiating a Sum
</li>
<li>Product Rule
</li>
<li>Quotient Rule
</li>
<li>Chain Rule
</li>
</ol>
<p>The following sections provide examples of the application of each rule.</p>
</div>
<div id="rule-1-the-power-rule" class="section level4 unnumbered">
<h4><em>Rule 1: The Power Rule</em></h4>

 Example:
<span class="math display">\[\begin{align*}  
 f(x) &amp;= x^{6} \\ 
 f&#39;(x) &amp;= 6*x^{6-1} \\
 &amp;=6x^{5} 
\end{align*}\]</span>
<p>A second example can be plotted in <code>R</code>. The function is <span class="math inline">\(f(x)=x^{2}\)</span> and therefore, using the power rule, the derivative is: <span class="math inline">\(f&#39;(x)=2x\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span><span class="op">:</span><span class="dv">5</span>)
x</code></pre></div>
<pre><code>##  [1] -5 -4 -3 -2 -1  0  1  2  3  4  5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>
y</code></pre></div>
<pre><code>##  [1] 25 16  9  4  1  0  1  4  9 16 25</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x,y, <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">pch=</span><span class="dv">19</span>)</code></pre></div>
<div class="figure"><span id="fig:powerfun"></span>
<img src="_main_files/figure-html/powerfun-1.png" alt="Calculating Slopes for $(x,y)$ Pairs" width="672" />
<p class="caption">
Figure 8.1: Calculating Slopes for <span class="math inline">\((x,y)\)</span> Pairs
</p>
</div>
</div>
<div id="rule-2-the-constant-rule" class="section level4 unnumbered">
<h4><em>Rule 2: The Constant Rule</em></h4>

Example:
<span class="math display">\[\begin{align*}
f(x) &amp;=346 \\ 
f&#39;(x)&amp;=0 \\
&amp;=10x
\end{align*}\]</span>
</div>
<div id="rule-3-a-constant-times-a-function" class="section level4 unnumbered">
<h4><em>Rule 3: A Constant Times a Function</em></h4>

Example:
<span class="math display">\[\begin{align*}
f(x) &amp;= 5x^{2} \\ 
f&#39;(x)&amp;=5*2x^{2-1} \\
&amp;=10x
\end{align*}\]</span>
</div>
<div id="rule-4-differentiating-a-sum" class="section level4 unnumbered">
<h4><em>Rule 4: Differentiating a Sum</em></h4>

Example:<br />

<span class="math display">\[\begin{align*}
f(x)&amp;= 4x^{2}+32x \\
f&#39;(x)&amp;=(4x^{2})&#39;+(32x)&#39; \\
&amp;=4*2x^{2-1}+32 \\
&amp;=8x+32 
\end{align*}\]</span>
</div>
<div id="rule-5-the-product-rule" class="section level4 unnumbered">
<h4><em>Rule 5: The Product Rule</em></h4>

Example:
<span class="math display">\[\begin{align*}
f(x) &amp;= x^{3}(x-5) \\ 
f&#39;(x)&amp;=(x^{3})&#39;(x-5)+(x^{3})(x-5)&#39; \\
&amp;=3x^{2}(x-5)+(x^{3})*1 \\
&amp;=3x^{3}-15x^{2}+x^{3}\\
&amp;=4x^{3}-15x^{2} 
\end{align*}\]</span>
<p>In a second example, the product rule is applied to the function <span class="math inline">\(y=f(x)=x^{2}-6x+5\)</span>. The derivative of this function is <span class="math inline">\(f&#39;(x)=2x-6\)</span>. This function can be plotted in <code>R</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span><span class="op">:</span><span class="dv">7</span>)
x</code></pre></div>
<pre><code>## [1] -1  0  1  2  3  4  5  6  7</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span>x<span class="op">^</span><span class="dv">2</span><span class="op">-</span><span class="dv">6</span><span class="op">*</span>x<span class="op">+</span><span class="dv">5</span>
y</code></pre></div>
<pre><code>## [1] 12  5  0 -3 -4 -3  0  5 12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x,y, <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">pch=</span><span class="dv">19</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">v=</span><span class="dv">0</span>)</code></pre></div>
<div class="figure"><span id="fig:prodfun"></span>
<img src="_main_files/figure-html/prodfun-1.png" alt="Plot of Function $y=f(x)=x^2-6x+5$" width="672" />
<p class="caption">
Figure 8.2: Plot of Function <span class="math inline">\(y=f(x)=x^2-6x+5\)</span>
</p>
</div>
<p>We can also use the derivative and <code>R</code> to calculate the slope for each value of <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">b &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span>x<span class="op">-</span><span class="dv">6</span>
b</code></pre></div>
<pre><code>## [1] -8 -6 -4 -2  0  2  4  6  8</code></pre>
<p>The values for <span class="math inline">\(X\)</span>, which are shown in Figure <a href="linear-estimation-and-minimizing-error.html#fig:prodfun">8.2</a>, range from -8 to +8 and return derivatives (slopes at a point) ranging from -25 to +25.</p>
</div>
<div id="rule-6-the-quotient-rule" class="section level4 unnumbered">
<h4><em>Rule 6: the Quotient Rule</em></h4>

Example:
<span class="math display">\[\begin{align*}
f(x) &amp;=\frac{x}{x^{2}+5} \\
f&#39;(x)&amp;=\frac{(x^{2}+5)(x)&#39;-(x^{2}+5)&#39;(x)}{(x^{2}+5)^{2}} \\
&amp;=\frac{(x^{2}+5)-(2x)(x)}{(x^{2}+5)^{2}} \\
&amp;= \frac{-x^{2}+5}{(x^{2}+5)^{2}}   
\end{align*}\]</span>
</div>
<div id="rule-7-the-chain-rule" class="section level4 unnumbered">
<h4><em>Rule 7: the Chain Rule</em></h4>

Example:
<span class="math display">\[\begin{align*}
f(x) &amp;= (7x^{2}-2x+13)^{5} \\ 
f&#39;(x)&amp;=5(7x^{2}-2x+13)^{4}*(7x^{2}-2x+13)&#39; \\
&amp;=5(7x^{2}-2x+13)^{4}*(14x-2)
\end{align*}\]</span>
</div>
</div>
<div id="critical-points" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Critical Points</h3>
<p>Our goal is to use derivatives to find the values of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> that minimize the sum of the squared error. To do this we need to find the <em>minima</em> of a function. The minima is the smallest value that a function takes, whereas the maxima is the largest value. To find the minima and maxima, the <em>critical points</em> are key. The critical point is where the derivative of the function is equal to <span class="math inline">\(0\)</span>, or <span class="math inline">\(f&#39;(x)=0\)</span>. Note that this is equivalent to the slope being equal to <span class="math inline">\(0\)</span>.</p>
<div id="example-finding-the-critical-points" class="section level4 unnumbered">
<h4>Example: Finding the Critical Points</h4>
<p>To find the critical point for the function</p>
<p><span class="math inline">\(y=f(x)=(x^{2}-4x+5)\)</span>;</p>
<ul>
<li><p>First find the derivative; <span class="math inline">\(f&#39;(x)=2x-4\)</span></p></li>
<li><p>Set the derivative equal to <span class="math inline">\(0\)</span>; <span class="math inline">\(f&#39;(x)=2x-4=0\)</span></p></li>
<li><p>Solve for <span class="math inline">\(x\)</span>; <span class="math inline">\(x=2\)</span></p></li>
<li><p>Substitute <span class="math inline">\(2\)</span> for <span class="math inline">\(x\)</span> into the function and solve for <span class="math inline">\(y\)</span></p>
</li>
<li><p>Thus, the critical point (there’s only one in this case) of the function is <span class="math inline">\((2,1)\)</span></p></li>
</ul>
<p>Once a critical point is identified, the next step is to determine whether that point is a minima or a maxima. The most straightforward way to do this is to identify the x,y coordinates and plot. This can be done in <code>R</code>, as we will show using the function <span class="math inline">\(y=f(x)=(x^{2}-4x+5)\)</span>. The plot is shown in Figure <a href="linear-estimation-and-minimizing-error.html#fig:crit">8.3</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">5</span><span class="op">:</span><span class="dv">5</span>)
x</code></pre></div>
<pre><code>##  [1] -5 -4 -3 -2 -1  0  1  2  3  4  5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span>x<span class="op">^</span><span class="dv">2</span><span class="op">-</span><span class="dv">4</span><span class="op">*</span>x<span class="op">+</span><span class="dv">5</span>
y</code></pre></div>
<pre><code>##  [1] 50 37 26 17 10  5  2  1  2  5 10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x,y, <span class="dt">type=</span><span class="st">&quot;o&quot;</span>, <span class="dt">pch=</span><span class="dv">19</span>)</code></pre></div>
<div class="figure"><span id="fig:crit"></span>
<img src="_main_files/figure-html/crit-1.png" alt="Identification of Critical Points" width="672" />
<p class="caption">
Figure 8.3: Identification of Critical Points
</p>
</div>
<p>As can be seen, the critical point <span class="math inline">\((2,1)\)</span> is a minima.</p>
</div>
</div>
<div id="partial-derivation" class="section level3">
<h3><span class="header-section-number">8.1.3</span> Partial Derivation</h3>
<p>When an equation includes two variables, one can take a <em>partial derivative</em> with respect to only one variable, while the other variable is simply treated as a constant. This is particularly useful in our case, because the function <span class="math inline">\(\sum \epsilon^{2}\)</span> has two variables – <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p>Let’s take an example. For the function <span class="math inline">\(y=f(x,z)=x^{3}+4xz-5z^{2}\)</span>, we first take the derivative of <span class="math inline">\(x\)</span> holding <span class="math inline">\(z\)</span> constant.</p>
<span class="math display">\[\begin{align*}
 \frac{\partial y}{\partial x} &amp;= \frac{\partial f(x,z)}{\partial x} \\
 &amp;= 3x^{2}+4z
\end{align*}\]</span>
<p>Next we take the derivative of <span class="math inline">\(z\)</span> holding <span class="math inline">\(x\)</span> constant.</p>
<span class="math display">\[\begin{align*}
 \frac{\partial y}{\partial z} &amp;= \frac{\partial f(x,z)}{\partial z} \\
 &amp;= 4x-10z
\end{align*}\]</span>
</div>
</div>
<div id="deriving-ols-estimators" class="section level2">
<h2><span class="header-section-number">8.2</span> Deriving OLS Estimators</h2>
<p>Now that we have developed some of the rules for differential calculus, we can see how OLS finds values of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> that minimize the sum of the squared error. In formal terms, let’s define the set, <span class="math inline">\(S(\hat{\alpha},\hat{\beta})\)</span>, as a pair of regression estimators that jointly determine the residual sum of squares given that: <span class="math inline">\(Y_{i}=\hat {Y}_{i}+\epsilon_{i}=\hat{\alpha}+\hat{\beta}X_{i}+\epsilon_{i}\)</span>. This function can be expressed:</p>
<span class="math display">\[\begin{equation*}
 S(\hat{\alpha},\hat{\beta})=\sum_{i=1}^{n} \epsilon^{2}_{i}=\sum (Y_{i}-\hat{Y_{i}})^{2}=\sum (Y_{i}-\hat{\alpha}-\hat{\beta} X_{i})^{2}  
\end{equation*}\]</span>
<p>First, we will derive <span class="math inline">\(\hat{\alpha}\)</span>.</p>
<div id="ols-derivation-of-hatalpha" class="section level3">
<h3><span class="header-section-number">8.2.1</span> OLS Derivation of <span class="math inline">\(\hat{\alpha}\)</span></h3>
<p>Take the partial derivatives of <span class="math inline">\(S(\hat{\alpha},\hat{\beta})\)</span> with-respect-to (w.r.t) <span class="math inline">\(\hat{\alpha}\)</span> in order to determine the formulation of <span class="math inline">\(\hat{\alpha}\)</span> that minimizes <span class="math inline">\(S(\hat{\alpha},\hat{\beta})\)</span>. Using the chain rule,</p>
<span class="math display">\[\begin{align*}
\frac{\partial S(\hat{\alpha},\hat{\beta})}{\partial \hat{\alpha}} &amp;= \sum
2(Y_{i}-\hat{\alpha}-\hat{\beta}X_{i})^{2-1}*(Y_{i}-\hat{\alpha}-\hat{\beta}X_{i})&#39; \\
&amp;= \sum 2(Y_{i}-\hat{\alpha}-\hat{\beta}X_{i})^{1}*(-1) \\
&amp;= -2 \sum (Y_{i}-\hat{\alpha}-\hat{\beta}X_{i}) \\ 
&amp;= -2 \sum Y_{i}+2n\hat{\alpha}+2\hat{\beta} \sum X_{i} 
\end{align*}\]</span>
<p>Next, set the derivative equal to <span class="math inline">\(0\)</span>.</p>
<span class="math display">\[\begin{equation*}
\frac{\partial S(\hat{\alpha},\hat{\beta})}{\partial \hat{\alpha}} =  -2 \sum Y_{i}+2n\hat{\alpha}+2\hat{\beta} \sum X_{i} = 0 
\end{equation*}\]</span>
<p>Then, shift non-<span class="math inline">\(\hat{\alpha}\)</span> terms to the other side of the equal sign:</p>
<span class="math display">\[\begin{equation*}
2n\hat{\alpha} = 2 \sum Y_{i}-2\hat{\beta} \sum X_{i} 
\end{equation*}\]</span>
Finally, divide through by <span class="math inline">\(2n\)</span>:
<span class="math display">\[\begin{align*}
\frac{2n\hat{\alpha}}{2n} &amp;= \frac{2 \sum Y_{i}-2\hat{\beta} \sum X_{i}}{2n} \\
A &amp;= \frac{\sum Y_{i}}{n}-\hat{\beta}*\frac{\sum X_{i}}{n} \\
&amp;= \bar {Y}-\hat{\beta} \bar{X} \\
\end{align*}\]</span>
<span class="math display" id="eq:08-1">\[\begin{equation}
\therefore \hat{\alpha} = \bar {Y}-\hat{\beta} \bar{X} 
\tag{8.1}
\end{equation}\]</span>
</div>
<div id="ols-derivation-of-hatbeta" class="section level3">
<h3><span class="header-section-number">8.2.2</span> OLS Derivation of <span class="math inline">\(\hat{\beta}\)</span></h3>
<p>Having found <span class="math inline">\(\hat{\alpha}\)</span>, the next step is to derive <span class="math inline">\(\hat{\beta}\)</span>. This time we will take the partial derivative w.r.t <span class="math inline">\(\hat{\beta}\)</span>. As you will see, the steps are a little more involved for <span class="math inline">\(\hat{\beta}\)</span> than they were for <span class="math inline">\(\hat{\alpha}\)</span>.</p>
<span class="math display">\[\begin{align*}
\frac{\partial S(\hat{\alpha},\hat{\beta})}{\partial \hat{\beta}} &amp;= \sum
2(Y_{i}-\hat{\alpha}-\hat{\beta}X_{i})^{2-1}*(Y_{i}-\hat{\alpha}-\hat{\beta}X_{i})&#39; \\
&amp;= \sum 2(Y_{i}-\hat{\alpha}-\hat{\beta}X_{i})^{1}*(-X_{i}) \\
&amp;= 2 \sum (-X_{i}Y_{i}+\hat{\alpha}X_{i}+\hat{\beta}X^{2}_{i}) \\ 
&amp;= -2 \sum X_{i}Y_{i}+2\hat{\alpha} \sum X_{i} + 2\hat{\beta} \sum X^{2}_{i}  
\end{align*}\]</span>
<p>Since we know that <span class="math inline">\(\hat{\alpha} = \bar {Y}-\hat{\beta} \bar{X}\)</span>, we can substitute <span class="math inline">\(\bar {Y}-\hat{\beta} \bar{X}\)</span> for <span class="math inline">\(\hat{\alpha}\)</span>.</p>
<span class="math display">\[\begin{align*}
\frac{\partial S(\hat{\alpha},\hat{\beta})}{\partial \hat{\beta}} &amp;= -2 \sum X_{i}Y_{i}+2(\bar {Y}-\hat{\beta}
\bar{X})\sum X_{i} + 2\hat{\beta} \sum X^{2}_{i} \\
&amp;= -2 \sum X_{i}Y_{i}+2 \bar{Y} \sum X_{i}-2\hat{\beta} \bar{X} \sum X_{i} + 2\hat{\beta}
\sum X^{2}_{i} 
\end{align*}\]</span>
<p>Next, we can substitute <span class="math inline">\(\frac{\sum Y_{i}}{n}\)</span> for <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(\frac{\sum X_{i}}{n}\)</span> for <span class="math inline">\(\bar{X}\)</span> and set it equal to <span class="math inline">\(0\)</span>.</p>
<span class="math display">\[\begin{equation*}
\frac{\partial S(\hat{\alpha},\hat{\beta})}{\partial \hat{\beta}} = -2 \sum X_{i}Y_{i}+\frac{2\sum Y_{i}
  \sum X_{i}}{n}-\frac{2\hat{\beta}\sum X_{i} \sum X_{i}}{n}+ 2\hat{\beta}
\sum X^{2}_{i} = 0
\end{equation*}\]</span>
<p>Then, multiply through by <span class="math inline">\(\frac{n}{2}\)</span> and put all the <span class="math inline">\(\hat{\beta}\)</span> terms on the same side.</p>
<span class="math display">\[\begin{align*}
n\hat{\beta} \sum X^{2}_{i}-\hat{\beta}(\sum X_{i})^{2} &amp;= n \sum X_{i}Y_{i}-\sum X_{i}
\sum Y_{i} \\
\hat{\beta}(n \sum X^{2}_{i}-(\sum X_{i})^{2}) &amp;= n \sum X_{i}Y_{i}-\sum X_{i}
\sum Y_{i} \\
\therefore \hat{\beta} = \frac{n \sum X_{i}Y_{i}-\sum X_{i}
\sum Y_{i}}{n\sum X^{2}_{i}-(\sum X_{i})^{2}}
\end{align*}\]</span>
<p>The <span class="math inline">\(\hat{\beta}\)</span> term can be rearranged such that:</p>
<span class="math display" id="eq:08-2">\[\begin{equation} 
\hat{\beta}=\frac{\Sigma(X_{i}-\bar X)(Y_{i}-\bar Y)}{\Sigma(X_{i}-\bar X)^2}  
\tag{8.2}
\end{equation}\]</span>
<p>Now remember what we are doing here: we used the partial derivatives for <span class="math inline">\(\sum \epsilon^{2}\)</span> with respect to <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> to find the values for <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> that will give us the smallest value for <span class="math inline">\(\sum \epsilon^{2}\)</span>. Put differently, the formulas for <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\alpha}\)</span> allow the calculation of the error-minimizing slope (change in <span class="math inline">\(Y\)</span> given a one unit change in <span class="math inline">\(X\)</span>) and intercept (value for <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> is zero) for any data set representing a bivariate, linear relationship. No other formulas will give us a line, using the same data, that will result in as small a squared-error. Therefore, OLS is referred to as the Best Linear Unbiased Estimator (BLUE).</p>
</div>
<div id="interpreting-hatbeta-and-hatalpha" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Interpreting <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\alpha}\)</span></h3>
<p>In a regression equation, <span class="math inline">\(Y=\hat{\alpha}+\hat{\beta}X\)</span>, where <span class="math inline">\(\hat{\alpha}\)</span> is shown in Equation <a href="linear-estimation-and-minimizing-error.html#eq:08-1">(8.1)</a> and <span class="math inline">\(\hat{\beta}\)</span> is shown in Equation <a href="linear-estimation-and-minimizing-error.html#eq:08-2">(8.2)</a>. Equation <a href="linear-estimation-and-minimizing-error.html#eq:08-2">(8.2)</a> shows that for each 1-unit increase in <span class="math inline">\(X\)</span> you get <span class="math inline">\(\hat{\beta}\)</span> units change in <span class="math inline">\(Y\)</span>. Equation <a href="linear-estimation-and-minimizing-error.html#eq:08-1">(8.1)</a> shows that when <span class="math inline">\(X\)</span> is <span class="math inline">\(0\)</span>, <span class="math inline">\(Y\)</span> is equal to <span class="math inline">\(\hat{\alpha}\)</span>. Note that in a regression model with no independent variables, <span class="math inline">\(\hat{\alpha}\)</span> is simply the expected value (i.e., mean) of <span class="math inline">\(Y\)</span>.</p>
<p>The intuition behind these formulas can be shown by using <code>R</code> to calculate “by hand” the slope (<span class="math inline">\(\hat{\beta}\)</span>) and intercept (<span class="math inline">\(\hat{\alpha}\)</span>) coefficients. A theoretical simple regression model is structured as follows:</p>
<span class="math display">\[\begin{equation*}
Y_{i} = \alpha + \beta X_{i} + \epsilon_{i} 
\end{equation*}\]</span>
<ul>
<li><span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are constant terms</li>
<li><span class="math inline">\(\alpha\)</span> is the intercept</li>
<li><span class="math inline">\(\beta\)</span> is the slope</li>
<li><span class="math inline">\(X_{i}\)</span> is a predictor of <span class="math inline">\(Y_{i}\)</span></li>
<li><span class="math inline">\(\epsilon\)</span> is the error term</li>
</ul>
<p>The model to be estimated is expressed as <span class="math inline">\(Y=\hat{\beta}+\hat{\beta}X+/epsilon\)</span>.</p>
<p>As noted, the goal is to calculate the intercept coefficient:</p>
<span class="math display">\[\begin{equation*}
\hat{\alpha}=\bar Y-\hat{\beta}\bar X 
\end{equation*}\]</span>
and the slope coefficient:
<span class="math display">\[\begin{equation*}
\hat{\beta}=\frac{\Sigma(X_{i}-\bar X)(Y_{i}-\bar Y)}{\Sigma(X_{i}-\bar X)^2}
\end{equation*}\]</span>
<p>Using <code>R</code>, this can be accomplished in a few steps. First create a vector of values for <code>x</code> and <code>y</code> (note that we chose these values arbitrarily for the purpose of this example).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">4</span>,<span class="dv">9</span>)
x</code></pre></div>
<pre><code>## [1] 4 2 4 3 5 7 4 9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">7</span>)
y</code></pre></div>
<pre><code>## [1] 2 1 5 3 6 4 2 7</code></pre>
<p>Then, create objects for <span class="math inline">\(\bar {X}\)</span> and <span class="math inline">\(\bar {Y}\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xbar &lt;-<span class="st"> </span><span class="kw">mean</span>(x)
xbar</code></pre></div>
<pre><code>## [1] 4.75</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ybar &lt;-<span class="st"> </span><span class="kw">mean</span>(y)
ybar</code></pre></div>
<pre><code>## [1] 3.75</code></pre>
<p>Next, create objects for <span class="math inline">\((X-\bar X)\)</span> and <span class="math inline">\((Y-\bar Y)\)</span>, the deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> around their means:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x.m.xbar &lt;-<span class="st"> </span>x<span class="op">-</span>xbar
x.m.xbar</code></pre></div>
<pre><code>## [1] -0.75 -2.75 -0.75 -1.75  0.25  2.25 -0.75  4.25</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y.m.ybar &lt;-<span class="st"> </span>y<span class="op">-</span>ybar
y.m.ybar</code></pre></div>
<pre><code>## [1] -1.75 -2.75  1.25 -0.75  2.25  0.25 -1.75  3.25</code></pre>
<p>Then, calculate <span class="math inline">\(\hat{\beta}\)</span>:</p>
<span class="math display">\[\begin{equation*}
\hat{\beta}=\frac{\Sigma(X_{i}-\bar X)(Y_{i}-\bar Y)}{\Sigma(X_{i}-\bar X)^2}
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="kw">sum</span>((x.m.xbar)<span class="op">*</span>(y.m.ybar))<span class="op">/</span><span class="kw">sum</span>((x.m.xbar)<span class="op">^</span><span class="dv">2</span>)
B</code></pre></div>
<pre><code>## [1] 0.7183099</code></pre>
<p>Finally, calculate <span class="math inline">\(\hat{\alpha}\)</span></p>
<span class="math display">\[\begin{equation*}
\hat{\alpha}=\bar Y-\hat{\beta}\bar X 
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">A &lt;-<span class="st"> </span>ybar<span class="op">-</span>B<span class="op">*</span>xbar
A</code></pre></div>
<pre><code>## [1] 0.3380282</code></pre>
<p>To see the relationship, we can produce a scatterplot of <code>x</code> and <code>y</code> and add our regression line, as shown in Figure <a href="linear-estimation-and-minimizing-error.html#fig:regex">8.4</a>. So, for each unit increase in <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> increases by 0.7183099 and when <span class="math inline">\(x\)</span> is <span class="math inline">\(0\)</span>, <span class="math inline">\(y\)</span> is equal to 0.3380282.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(x,y)
<span class="kw">lines</span>(x,A<span class="op">+</span>B<span class="op">*</span>x)</code></pre></div>
<div class="figure"><span id="fig:regex"></span>
<img src="_main_files/figure-html/regex-1.png" alt="Simple Regression of $x$ and $y$" width="672" />
<p class="caption">
Figure 8.4: Simple Regression of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dev.off</span>()</code></pre></div>
<pre><code>## RStudioGD 
##         2</code></pre>
</div>
</div>
<div id="summary-3" class="section level2">
<h2><span class="header-section-number">8.3</span> Summary</h2>
<p>Whoa! Think of what you’ve accomplished here: You learned enough calculus to find a minima for an equation with two variables, then applied that to the equation for the <span class="math inline">\(\sum \epsilon^{2}\)</span>. You derived the error minimizing values for <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span>, then used those formulae in <code>R</code> to calculate ``by hand&quot; the OLS regression for a small dataset.</p>
<p>Congratulate yourself – you deserve it!</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-logic-of-ordinary-least-squares-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bi-variate-hypothesis-testing-and-model-fit.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
