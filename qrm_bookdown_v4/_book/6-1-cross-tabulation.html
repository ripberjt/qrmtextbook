<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="6.1 Cross-Tabulation | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R" />
<meta property="og:type" content="book" />





<meta name="author" content="Hank Jenkins-Smith" />
<meta name="author" content="Joseph Ripberger" />
<meta name="author" content="Gary Copeland" />
<meta name="author" content="Matthew Nowlin" />
<meta name="author" content="Tyler Hughes" />
<meta name="author" content="Aaron Fister" />
<meta name="author" content="Wesley Wehde" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="6.1 Cross-Tabulation | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R">

<title>6.1 Cross-Tabulation | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#preface-and-acknowledgments">Preface and Acknowledgments</a><ul>
<li><a href="copyright.html#copyright">Copyright</a></li>
</ul></li>
<li class="has-sub"><a href="1-theories-and-social-science.html#theories-and-social-science"><span class="toc-section-number">1</span> Theories and Social Science</a><ul>
<li><a href="1-1-the-scientific-method.html#the-scientific-method"><span class="toc-section-number">1.1</span> The Scientific Method</a></li>
<li class="has-sub"><a href="1-2-theory-and-empirical-research.html#theory-and-empirical-research"><span class="toc-section-number">1.2</span> Theory and Empirical Research</a><ul>
<li><a href="1-2-theory-and-empirical-research.html#coherent-and-internally-consistent"><span class="toc-section-number">1.2.1</span> Coherent and Internally Consistent</a></li>
<li><a href="1-2-theory-and-empirical-research.html#theories-and-causality"><span class="toc-section-number">1.2.2</span> Theories and Causality</a></li>
<li><a href="1-2-theory-and-empirical-research.html#generation-of-testable-hypothesis"><span class="toc-section-number">1.2.3</span> Generation of Testable Hypothesis</a></li>
</ul></li>
<li><a href="1-3-theory-and-functions.html#theory-and-functions"><span class="toc-section-number">1.3</span> Theory and Functions</a></li>
<li><a href="1-4-theory-in-social-science.html#theory-in-social-science"><span class="toc-section-number">1.4</span> Theory in Social Science</a></li>
<li><a href="1-5-outline-of-the-book.html#outline-of-the-book"><span class="toc-section-number">1.5</span> Outline of the Book</a></li>
</ul></li>
<li class="has-sub"><a href="2-research-design.html#research-design"><span class="toc-section-number">2</span> Research Design</a><ul>
<li><a href="2-1-overview-of-the-research-process.html#overview-of-the-research-process"><span class="toc-section-number">2.1</span> Overview of the Research Process</a></li>
<li><a href="2-2-internal-and-external-validity.html#internal-and-external-validity"><span class="toc-section-number">2.2</span> Internal and External Validity</a></li>
<li><a href="2-3-major-classes-of-designs.html#major-classes-of-designs"><span class="toc-section-number">2.3</span> Major Classes of Designs</a></li>
<li><a href="2-4-threats-to-validity.html#threats-to-validity"><span class="toc-section-number">2.4</span> Threats to Validity</a></li>
<li><a href="2-5-some-common-designs.html#some-common-designs"><span class="toc-section-number">2.5</span> Some Common Designs</a></li>
<li><a href="2-6-plan-meets-reality.html#plan-meets-reality"><span class="toc-section-number">2.6</span> Plan Meets Reality</a></li>
</ul></li>
<li class="has-sub"><a href="3-exploring-and-visualizing-data.html#exploring-and-visualizing-data"><span class="toc-section-number">3</span> Exploring and Visualizing Data</a><ul>
<li class="has-sub"><a href="3-1-characterizing-data.html#characterizing-data"><span class="toc-section-number">3.1</span> Characterizing Data</a><ul>
<li><a href="3-1-characterizing-data.html#central-tendency"><span class="toc-section-number">3.1.1</span> Central Tendency</a></li>
<li><a href="3-1-characterizing-data.html#level-of-measurement-and-central-tendency"><span class="toc-section-number">3.1.2</span> Level of Measurement and Central Tendency</a></li>
<li><a href="3-1-characterizing-data.html#moments"><span class="toc-section-number">3.1.3</span> Moments</a></li>
<li><a href="3-1-characterizing-data.html#first-moment-expected-value"><span class="toc-section-number">3.1.4</span> First Moment – Expected Value</a></li>
<li><a href="3-1-characterizing-data.html#the-second-moment-variance-and-standard-deviation"><span class="toc-section-number">3.1.5</span> The Second Moment – Variance and Standard Deviation</a></li>
<li><a href="3-1-characterizing-data.html#the-third-moment-skewness"><span class="toc-section-number">3.1.6</span> The Third Moment – Skewness</a></li>
<li><a href="3-1-characterizing-data.html#the-fourth-moment-kurtosis"><span class="toc-section-number">3.1.7</span> The Fourth Moment – Kurtosis</a></li>
<li><a href="3-1-characterizing-data.html#order-statistics"><span class="toc-section-number">3.1.8</span> Order Statistics</a></li>
</ul></li>
<li><a href="3-2-summary.html#summary"><span class="toc-section-number">3.2</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="4-probability.html#probability"><span class="toc-section-number">4</span> Probability</a><ul>
<li><a href="4-1-finding-probabilities.html#finding-probabilities"><span class="toc-section-number">4.1</span> Finding Probabilities</a></li>
<li><a href="4-2-finding-probabilities-with-the-normal-curve.html#finding-probabilities-with-the-normal-curve"><span class="toc-section-number">4.2</span> Finding Probabilities with the Normal Curve</a></li>
<li><a href="4-3-summary-1.html#summary-1"><span class="toc-section-number">4.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a><ul>
<li class="has-sub"><a href="5-1-inference-populations-and-samples.html#inference-populations-and-samples"><span class="toc-section-number">5.1</span> Inference: Populations and Samples</a><ul>
<li><a href="5-1-inference-populations-and-samples.html#populations-and-samples"><span class="toc-section-number">5.1.1</span> Populations and Samples</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-and-knowing"><span class="toc-section-number">5.1.2</span> Sampling and Knowing</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-strategies"><span class="toc-section-number">5.1.3</span> Sampling Strategies</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-techniques"><span class="toc-section-number">5.1.4</span> Sampling Techniques</a></li>
<li><a href="5-1-inference-populations-and-samples.html#so-how-is-it-that-we-know"><span class="toc-section-number">5.1.5</span> So How is it That We Know?</a></li>
</ul></li>
<li class="has-sub"><a href="5-2-the-normal-distribution.html#the-normal-distribution"><span class="toc-section-number">5.2</span> The Normal Distribution</a><ul>
<li><a href="5-2-the-normal-distribution.html#standardizing-a-normal-distribution-and-z-scores"><span class="toc-section-number">5.2.1</span> Standardizing a Normal Distribution and Z-scores</a></li>
<li><a href="5-2-the-normal-distribution.html#the-central-limit-theorem"><span class="toc-section-number">5.2.2</span> The Central Limit Theorem</a></li>
<li><a href="5-2-the-normal-distribution.html#populations-samples-and-symbols"><span class="toc-section-number">5.2.3</span> Populations, Samples and Symbols</a></li>
</ul></li>
<li class="has-sub"><a href="5-3-inferences-to-the-population-from-the-sample.html#inferences-to-the-population-from-the-sample"><span class="toc-section-number">5.3</span> Inferences to the Population from the Sample</a><ul>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#confidence-intervals"><span class="toc-section-number">5.3.1</span> Confidence Intervals</a></li>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#the-logic-of-hypothesis-testing"><span class="toc-section-number">5.3.2</span> The Logic of Hypothesis Testing</a></li>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#some-miscellaneous-notes-about-hypothesis-testing"><span class="toc-section-number">5.3.3</span> Some Miscellaneous Notes about Hypothesis Testing</a></li>
</ul></li>
<li class="has-sub"><a href="5-4-differences-between-groups.html#differences-between-groups"><span class="toc-section-number">5.4</span> Differences Between Groups</a><ul>
<li><a href="5-4-differences-between-groups.html#t-tests"><span class="toc-section-number">5.4.1</span> <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li><a href="5-5-summary-2.html#summary-2"><span class="toc-section-number">5.5</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="6-association-of-variables.html#association-of-variables"><span class="toc-section-number">6</span> Association of Variables</a><ul>
<li class="has-sub"><a href="6-1-cross-tabulation.html#cross-tabulation"><span class="toc-section-number">6.1</span> Cross-Tabulation</a><ul>
<li><a href="6-1-cross-tabulation.html#crosstabulation-and-control"><span class="toc-section-number">6.1.1</span> Crosstabulation and Control</a></li>
</ul></li>
<li><a href="6-2-covariance.html#covariance"><span class="toc-section-number">6.2</span> Covariance</a></li>
<li><a href="6-3-correlation.html#correlation"><span class="toc-section-number">6.3</span> Correlation</a></li>
<li><a href="6-4-scatterplots.html#scatterplots"><span class="toc-section-number">6.4</span> Scatterplots</a></li>
</ul></li>
<li class="has-sub"><a href="7-the-logic-of-ordinary-least-squares-estimation.html#the-logic-of-ordinary-least-squares-estimation"><span class="toc-section-number">7</span> The Logic of Ordinary Least Squares Estimation</a><ul>
<li class="has-sub"><a href="7-1-theoretical-models.html#theoretical-models"><span class="toc-section-number">7.1</span> Theoretical Models</a><ul>
<li><a href="7-1-theoretical-models.html#deterministic-linear-model"><span class="toc-section-number">7.1.1</span> Deterministic Linear Model</a></li>
<li><a href="7-1-theoretical-models.html#stochastic-linear-model"><span class="toc-section-number">7.1.2</span> Stochastic Linear Model</a></li>
<li><a href="7-1-theoretical-models.html#assumptions-about-the-error-term"><span class="toc-section-number">7.1.3</span> Assumptions about the Error Term</a></li>
</ul></li>
<li class="has-sub"><a href="7-2-estimating-linear-models.html#estimating-linear-models"><span class="toc-section-number">7.2</span> Estimating Linear Models</a><ul>
<li><a href="7-2-estimating-linear-models.html#residuals"><span class="toc-section-number">7.2.1</span> Residuals</a></li>
</ul></li>
<li><a href="7-3-an-example-of-simple-regression.html#an-example-of-simple-regression"><span class="toc-section-number">7.3</span> An Example of Simple Regression</a></li>
</ul></li>
<li class="has-sub"><a href="8-linear-estimation-and-minimizing-error.html#linear-estimation-and-minimizing-error"><span class="toc-section-number">8</span> Linear Estimation and Minimizing Error</a><ul>
<li class="has-sub"><a href="8-1-minimizing-error-using-derivatives.html#minimizing-error-using-derivatives"><span class="toc-section-number">8.1</span> Minimizing Error using Derivatives</a><ul>
<li><a href="8-1-minimizing-error-using-derivatives.html#rules-of-derivation"><span class="toc-section-number">8.1.1</span> Rules of Derivation</a></li>
<li><a href="8-1-minimizing-error-using-derivatives.html#critical-points"><span class="toc-section-number">8.1.2</span> Critical Points</a></li>
<li><a href="8-1-minimizing-error-using-derivatives.html#partial-derivation"><span class="toc-section-number">8.1.3</span> Partial Derivation</a></li>
</ul></li>
<li class="has-sub"><a href="8-2-deriving-ols-estimators.html#deriving-ols-estimators"><span class="toc-section-number">8.2</span> Deriving OLS Estimators</a><ul>
<li><a href="8-2-deriving-ols-estimators.html#ols-derivation-of-hatalpha"><span class="toc-section-number">8.2.1</span> OLS Derivation of <span class="math inline">\(\hat{\alpha}\)</span></a></li>
<li><a href="8-2-deriving-ols-estimators.html#ols-derivation-of-hatbeta"><span class="toc-section-number">8.2.2</span> OLS Derivation of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li><a href="8-2-deriving-ols-estimators.html#interpreting-hatbeta-and-hatalpha"><span class="toc-section-number">8.2.3</span> Interpreting <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\alpha}\)</span></a></li>
</ul></li>
<li><a href="8-3-summary-3.html#summary-3"><span class="toc-section-number">8.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="9-bi-variate-hypothesis-testing-and-model-fit.html#bi-variate-hypothesis-testing-and-model-fit"><span class="toc-section-number">9</span> Bi-Variate Hypothesis Testing and Model Fit</a><ul>
<li class="has-sub"><a href="9-1-hypothesis-tests-for-regression-coefficients.html#hypothesis-tests-for-regression-coefficients"><span class="toc-section-number">9.1</span> Hypothesis Tests for Regression Coefficients</a><ul>
<li><a href="9-1-hypothesis-tests-for-regression-coefficients.html#residual-standard-error"><span class="toc-section-number">9.1.1</span> Residual Standard Error</a></li>
</ul></li>
<li class="has-sub"><a href="9-2-measuring-goodness-of-fit.html#measuring-goodness-of-fit"><span class="toc-section-number">9.2</span> Measuring Goodness of Fit</a><ul>
<li><a href="9-2-measuring-goodness-of-fit.html#sample-covariance-and-correlations"><span class="toc-section-number">9.2.1</span> Sample Covariance and Correlations</a></li>
<li><a href="9-2-measuring-goodness-of-fit.html#coefficient-of-determination-r2"><span class="toc-section-number">9.2.2</span> Coefficient of Determination: <span class="math inline">\(R^{2}\)</span></a></li>
<li><a href="9-2-measuring-goodness-of-fit.html#visualizing-bivariate-regression"><span class="toc-section-number">9.2.3</span> Visualizing Bivariate Regression</a></li>
</ul></li>
<li><a href="9-3-summary-4.html#summary-4"><span class="toc-section-number">9.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="10-ols-assumptions-and-simple-regression-diagnostics.html#ols-assumptions-and-simple-regression-diagnostics"><span class="toc-section-number">10</span> OLS Assumptions and Simple Regression Diagnostics</a><ul>
<li><a href="10-1-a-recap-of-modeling-assumptions.html#a-recap-of-modeling-assumptions"><span class="toc-section-number">10.1</span> A Recap of Modeling Assumptions</a></li>
<li class="has-sub"><a href="10-2-when-things-go-bad-with-residuals.html#when-things-go-bad-with-residuals"><span class="toc-section-number">10.2</span> When Things Go Bad with Residuals</a><ul>
<li><a href="10-2-when-things-go-bad-with-residuals.html#outlier-data"><span class="toc-section-number">10.2.1</span> “Outlier” Data</a></li>
<li><a href="10-2-when-things-go-bad-with-residuals.html#non-constant-variance"><span class="toc-section-number">10.2.2</span> Non-Constant Variance</a></li>
<li><a href="10-2-when-things-go-bad-with-residuals.html#non-linearity-in-the-parameters"><span class="toc-section-number">10.2.3</span> Non-Linearity in the Parameters</a></li>
</ul></li>
<li class="has-sub"><a href="10-3-application-of-residual-diagnostics.html#application-of-residual-diagnostics"><span class="toc-section-number">10.3</span> Application of Residual Diagnostics</a><ul>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-non-linearity"><span class="toc-section-number">10.3.1</span> Testing for Non-Linearity</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-normality-in-model-residuals"><span class="toc-section-number">10.3.2</span> Testing for Normality in Model Residuals</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-non-constant-variance-in-the-residuals"><span class="toc-section-number">10.3.3</span> Testing for Non-Constant Variance in the Residuals</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#examining-outlier-data"><span class="toc-section-number">10.3.4</span> Examining Outlier Data</a></li>
</ul></li>
<li><a href="10-4-so-now-what-implications-of-residual-analysis.html#so-now-what-implications-of-residual-analysis"><span class="toc-section-number">10.4</span> So Now What? Implications of Residual Analysis</a></li>
<li><a href="10-5-summary-5.html#summary-5"><span class="toc-section-number">10.5</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="11-introduction-to-multiple-regression.html#introduction-to-multiple-regression"><span class="toc-section-number">11</span> Introduction to Multiple Regression</a><ul>
<li><a href="11-1-matrix-algebra-and-multiple-regression.html#matrix-algebra-and-multiple-regression"><span class="toc-section-number">11.1</span> Matrix Algebra and Multiple Regression</a></li>
<li class="has-sub"><a href="11-2-the-basics-of-matrix-algebra.html#the-basics-of-matrix-algebra"><span class="toc-section-number">11.2</span> The Basics of Matrix Algebra</a><ul>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-basics"><span class="toc-section-number">11.2.1</span> Matrix Basics</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#vectors"><span class="toc-section-number">11.2.2</span> Vectors</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-operations"><span class="toc-section-number">11.2.3</span> Matrix Operations</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#transpose"><span class="toc-section-number">11.2.4</span> Transpose</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#adding-matrices"><span class="toc-section-number">11.2.5</span> Adding Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#multiplication-of-matrices"><span class="toc-section-number">11.2.6</span> Multiplication of Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#identity-matrices"><span class="toc-section-number">11.2.7</span> Identity Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-inversion"><span class="toc-section-number">11.2.8</span> Matrix Inversion</a></li>
</ul></li>
<li><a href="11-3-ols-regression-in-matrix-form.html#ols-regression-in-matrix-form"><span class="toc-section-number">11.3</span> OLS Regression in Matrix Form</a></li>
<li><a href="11-4-summary-6.html#summary-6"><span class="toc-section-number">11.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="12-the-logic-of-multiple-regression.html#the-logic-of-multiple-regression"><span class="toc-section-number">12</span> The Logic of Multiple Regression</a><ul>
<li class="has-sub"><a href="12-1-theoretical-specification.html#theoretical-specification"><span class="toc-section-number">12.1</span> Theoretical Specification</a><ul>
<li><a href="12-1-theoretical-specification.html#assumptions-of-ols-regression"><span class="toc-section-number">12.1.1</span> Assumptions of OLS Regression</a></li>
</ul></li>
<li><a href="12-2-partial-effects.html#partial-effects"><span class="toc-section-number">12.2</span> Partial Effects</a></li>
<li class="has-sub"><a href="12-3-multiple-regression-example.html#multiple-regression-example"><span class="toc-section-number">12.3</span> Multiple Regression Example</a><ul>
<li><a href="12-3-multiple-regression-example.html#hypothesis-testing-and-t-tests"><span class="toc-section-number">12.3.1</span> Hypothesis Testing and <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li><a href="12-4-summary-7.html#summary-7"><span class="toc-section-number">12.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="13-multiple-regression-and-model-building.html#multiple-regression-and-model-building"><span class="toc-section-number">13</span> Multiple Regression and Model Building</a><ul>
<li class="has-sub"><a href="13-1-model-building.html#model-building"><span class="toc-section-number">13.1</span> Model Building</a><ul>
<li><a href="13-1-model-building.html#theory-and-hypotheses"><span class="toc-section-number">13.1.1</span> Theory and Hypotheses</a></li>
<li><a href="13-1-model-building.html#empirical-indicators"><span class="toc-section-number">13.1.2</span> Empirical Indicators</a></li>
<li><a href="13-1-model-building.html#risks-in-model-building"><span class="toc-section-number">13.1.3</span> Risks in Model Building</a></li>
</ul></li>
<li><a href="13-2-evils-of-stepwise-regression.html#evils-of-stepwise-regression"><span class="toc-section-number">13.2</span> Evils of Stepwise Regression</a></li>
<li><a href="13-3-summary-8.html#summary-8"><span class="toc-section-number">13.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="14-topics-in-multiple-regression.html#topics-in-multiple-regression"><span class="toc-section-number">14</span> Topics in Multiple Regression</a><ul>
<li><a href="14-1-dummy-variables.html#dummy-variables"><span class="toc-section-number">14.1</span> Dummy Variables</a></li>
<li><a href="14-2-interaction-effects.html#interaction-effects"><span class="toc-section-number">14.2</span> Interaction Effects</a></li>
<li><a href="14-3-standardized-regression-coefficients.html#standardized-regression-coefficients"><span class="toc-section-number">14.3</span> Standardized Regression Coefficients</a></li>
<li><a href="14-4-summary-9.html#summary-9"><span class="toc-section-number">14.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="15-the-art-of-regression-diagnostics.html#the-art-of-regression-diagnostics"><span class="toc-section-number">15</span> The Art of Regression Diagnostics</a><ul>
<li><a href="15-1-ols-error-assumptions-revisited.html#ols-error-assumptions-revisited"><span class="toc-section-number">15.1</span> OLS Error Assumptions Revisited</a></li>
<li class="has-sub"><a href="15-2-ols-diagnostic-techniques.html#ols-diagnostic-techniques"><span class="toc-section-number">15.2</span> OLS Diagnostic Techniques</a><ul>
<li><a href="15-2-ols-diagnostic-techniques.html#non-linearity"><span class="toc-section-number">15.2.1</span> Non-Linearity</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#non-constant-variance-or-heteroscedasticity"><span class="toc-section-number">15.2.2</span> Non-Constant Variance, or Heteroscedasticity</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#independence-of-e"><span class="toc-section-number">15.2.3</span> Independence of <span class="math inline">\(E\)</span></a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#normality-of-the-residuals"><span class="toc-section-number">15.2.4</span> Normality of the Residuals</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#outliers-leverage-and-influence"><span class="toc-section-number">15.2.5</span> Outliers, Leverage, and Influence</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#outliers"><span class="toc-section-number">15.2.6</span> Outliers</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#multicollinearity"><span class="toc-section-number">15.2.7</span> Multicollinearity</a></li>
</ul></li>
<li><a href="15-3-summary-10.html#summary-10"><span class="toc-section-number">15.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="16-logit-regression.html#logit-regression"><span class="toc-section-number">16</span> Logit Regression</a><ul>
<li><a href="16-1-generalized-linear-models.html#generalized-linear-models"><span class="toc-section-number">16.1</span> Generalized Linear Models</a></li>
<li class="has-sub"><a href="16-2-logit-estimation.html#logit-estimation"><span class="toc-section-number">16.2</span> Logit Estimation</a><ul>
<li><a href="16-2-logit-estimation.html#logit-hypothesis-tests"><span class="toc-section-number">16.2.1</span> Logit Hypothesis Tests</a></li>
<li><a href="16-2-logit-estimation.html#goodness-of-fit"><span class="toc-section-number">16.2.2</span> Goodness of Fit</a></li>
<li><a href="16-2-logit-estimation.html#interpreting-logits"><span class="toc-section-number">16.2.3</span> Interpreting Logits</a></li>
</ul></li>
<li><a href="16-3-summary-11.html#summary-11"><span class="toc-section-number">16.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="17-appendix-basic-r.html#appendix-basic-r"><span class="toc-section-number">17</span> Appendix: Basic R</a><ul>
<li><a href="17-1-introduction-to-r.html#introduction-to-r"><span class="toc-section-number">17.1</span> Introduction to R</a></li>
<li><a href="17-2-downloading-r-and-rstudio.html#downloading-r-and-rstudio"><span class="toc-section-number">17.2</span> Downloading R and RStudio</a></li>
<li><a href="17-3-introduction-to-programming.html#introduction-to-programming"><span class="toc-section-number">17.3</span> Introduction to Programming</a></li>
<li><a href="17-4-uploadingreading-data.html#uploadingreading-data"><span class="toc-section-number">17.4</span> Uploading/Reading Data</a></li>
<li><a href="17-5-data-manipulation-in-r.html#data-manipulation-in-r"><span class="toc-section-number">17.5</span> Data Manipulation in R</a></li>
<li><a href="17-6-savingwriting-data.html#savingwriting-data"><span class="toc-section-number">17.6</span> Saving/Writing Data</a></li>
<li><a href="17-7-the-tidyverse.html#the-tidyverse"><span class="toc-section-number">17.7</span> The Tidyverse</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="cross-tabulation" class="section level2">
<h2><span class="header-section-number">6.1</span> Cross-Tabulation</h2>
<p>To determine if there is an association between two variables measured at the nominal or ordinal levels, we use cross-tabulation and a set of supporting statistics. A cross-tabulation (or just crosstab) is a table that looks at the distribution of two variables simultaneously. Table <a href="6-1-cross-tabulation.html#fig:iv-dv">6.1</a> provides a sample layout of a 2 X 2 table.</p>
<div class="figure"><span id="fig:iv-dv"></span>
<img src="iv-dv.png" alt="Sample Table Layout"  />
<p class="caption">
Figure 6.1: Sample Table Layout
</p>
</div>
<p>As Table <a href="6-1-cross-tabulation.html#fig:iv-dv">6.1</a> illustrates, a crosstab is set up so that the independent variable is on the top, forming columns, and the dependent variable is on the side, forming rows. Toward the upper left hand corner of the table are the low, or negative, variable categories. Generally, a table will be displayed in percentage format. The marginals for a table are the column totals and the row totals and are the same as a frequency distribution would be for that variable. Each cross-classification reports how many observations have that shared characteristic. The cross-classification groups are referred to as <strong>cells</strong>, so Table <a href="6-1-cross-tabulation.html#fig:iv-dv">6.1</a> is a four-celled table.</p>
<p>A table like Table <a href="6-1-cross-tabulation.html#fig:iv-dv">6.1</a> provides a basis to begin to answer the question of whether our independent and dependent variables are related. Remember that our null hypothesis says there is no relationship between our IV and our DV. Looking at Table <a href="6-1-cross-tabulation.html#fig:iv-dv">6.1</a>, we can say of those low on the IV, 60% of them will also be low on the DV; and that those high on the IV will be low on the DV 40% of the time. Our null hypothesis says there should be no difference, but in this case, there is a 20% difference so it appears that our null hypothesis is incorrect. What we learned in our inferential statistics chapter, though, tells us that it is still possible that the null hypothesis is true. The question is how likely is it that we could have a 20% difference in our sample even if the null hypothesis is true?<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
<p>We use the <strong>chi square statistic</strong> to test our null hypothesis when using crosstabs. To find chi square (<span class="math inline">\(\chi^2\)</span>), we begin by assuming the null hypothesis to be true and find the expected frequencies for each cell in our table. We do so using a posterior methodology based on the marginals for our dependent variable. We see that 53% of our total sample is low on the dependent variable. If our null hypothesis is correct, then where one is located on the independent variable should not matter: 53% of those who are low on the IV should be low on the DV and 53% of those who are high on the IV should be low on the DV. Table <a href="6-1-cross-tabulation.html#fig:iv-dv2">6.2</a> &amp; <a href="6-1-cross-tabulation.html#fig:iv-dv3">6.3</a> illustrate this pattern. To find the expected frequency for each cell, we simply multiply the expected cell percentage times the number of people in each category of the IV: the expected frequency for the low-low cell is <span class="math inline">\(.53 * 200 = 106\)</span>; for the low-high cell, it is <span class="math inline">\(.47 * 200 = 94\)</span>; for the low-high cell it is <span class="math inline">\(.53 * 100 = 53\)</span>; and for the high-high cell, the expected frequency is <span class="math inline">\(.47 * 100 = 47\)</span>. (See Table <a href="6-1-cross-tabulation.html#fig:iv-dv2">6.2</a> &amp; <a href="6-1-cross-tabulation.html#fig:iv-dv3">6.3</a>).</p>
<p>The formula for the chi square takes the expected frequency for each of the cells and subtracts the observed frequency from it, squares those differences, divides by the expected frequency, and sums those values:</p>
<span class="math display" id="eq:06-1">\[\begin{equation}
\chi^2  = \sum \frac{(O-E)^2}{E}
\tag{6.1}
  \end{equation}\]</span>
<p>where: <span class="math inline">\(\chi^2\)</span> = The Test Statistic <span class="math inline">\(\sum\)</span> = The Summation Operator <span class="math inline">\(O\)</span> = Observed Frequencies <span class="math inline">\(E\)</span> = Expected Frequencies</p>
<div class="figure"><span id="fig:iv-dv2"></span>
<img src="iv-dv2.PNG" alt="Sample Null-Hypothesized Table Layout as Percentages" width="217" />
<p class="caption">
Figure 6.2: Sample Null-Hypothesized Table Layout as Percentages
</p>
</div>
<div class="figure"><span id="fig:iv-dv3"></span>
<img src="iv-dv3.PNG" alt="Sample Null-Hypothesized Table Layout as Counts" width="206" />
<p class="caption">
Figure 6.3: Sample Null-Hypothesized Table Layout as Counts
</p>
</div>
<p>Table <a href="6-1-cross-tabulation.html#fig:chiscalc">6.4</a> provides those calculations. It shows a final chi square of 10.73. With that chi square, we can go to a chi square table to determine whether to accept or reject the null hypothesis. Before going to that chi square table, we need to figure out two things. First, we need to determine the level of significance we want, presumably .05. Second, we need to determine our degrees of freedom. We will provide more on that concept as we go on, but for now, know that it is the number of rows minus one times the number of columns minus one. In this case we have <span class="math inline">\((2-1)(2-1) = 1\)</span> degree of freedom.</p>
<div class="figure"><span id="fig:chiscalc"></span>
<img src="iv-dv4.PNG" alt="Chi Square Calculation" width="253" />
<p class="caption">
Figure 6.4: Chi Square Calculation
</p>
</div>
<p>Table <a href="#tab:chisquaretable"><strong>??</strong></a> (at the end of this chapter) is a chi square table that shows the critical values for various levels of significance and degrees of freedom. The critical value for one degree of freedom with a .05 level of significance is 3.84. Since our chi square is larger than that we can reject our null hypothesis - there is less than a .05 probability that we could have found the results in our sample if there is no relationship in the population. In fact, if we follow the row for one degree of freedom across, we see we can reject our null hypothesis even at the .005 level of significance and, almost but not quite, at the .001 level of significance.</p>
<p>Having rejected the null hypothesis, we believe there is a relationship between the two variables, but we still want to know how strong that relationship is. Measures of association are used to determine the strength of a relationship. One type of measure of association relies on a co-variation model as elaborated upon in Sections 6.2 and 6.3. Co-variation models are directional models and require ordinal or interval level measures; otherwise, the variables have no direction. Here we consider alternative models.</p>
<p>If one or both of our variables is nominal, we cannot specify directional change. Still, we might see a recognizable pattern of change in one variable as the other variable varies. Women might be more concerned about climate change than are men, for example. For that type of case, we may use a reduction in error or a <strong>proportional reduction in error (PRE) model</strong>. We consider how well we predict using a naive model (assuming no relationship) and compare it to how much better we predict when we use our independent variable to make that prediction. These measures of association only range from <span class="math inline">\(0 - 1.0\)</span>, since the sign otherwise indicates direction. Generally, we use this type of measure when at least one our variables is nominal, but we will also use a PRE model measure, <span class="math inline">\(r^2\)</span>, in regression analysis. <strong>Lambda</strong> is a commonly used PRE-based measure of association for nominal level data, but it can underestimate the relationship in some circumstances.</p>
<p>Another set of measures of association suitable for nominal level data is based on chi square. <strong>Cramer’s V</strong> is a simple chi square based indicator, but like chi square itself, its value is affected by the sample size and the dimensions of the table. <strong>Phi</strong> corrects for sample size, but is appropriate only for a 2 X 2 table. The <strong>contingency coefficient</strong>, C, also corrects for sample size and can be applied to larger tables, but requires a square table, i.e., the same number of rows and columns.</p>
<p>If we have ordinal level data, we can use a co-variation model, but the specific model developed below in Section 6.3 looks at how observations are distributed around their means. Since we cannot find a mean for ordinal level data, we need an alternative. <strong>Gamma</strong> is commonly used with ordinal level data and provides a summary comparing how many observations fall around the diagonal in the table that supports a positive relationship (e.g. observations in the low-low cell and the high-high cells) as opposed to observations following the negative diagonal (e.g. the low-high cell and the high-low cells). Gamma ranges from <span class="math inline">\(-1.0\)</span> to <span class="math inline">\(+1.0\)</span>.\</p>
<p>Crosstabulations and their associated statistics can be calculated using R. In this example we continue to use the Global Climate Change dataset (ds). The dataset includes measures of survey respondents: gender (female = 0, male = 1); perceived risk posed by climate change, or glbcc_risk (0 = Not Risk; 10 = extreme risk), and political ideology (1 = strong liberal, 7 = strong conservative). Here we look at whether there is a relationship between gender and the glbcc_risk variable. The glbcc_risk variable has eleven categories; to make the table more manageable, we recode it to five categories.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Factor the gender variable</span>
ds<span class="op">$</span>f.gend &lt;-<span class="st"> </span><span class="kw">factor</span>(ds<span class="op">$</span>gender, <span class="dt">levels=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Women&quot;</span>, <span class="st">&quot;Men&quot;</span>))

<span class="co">#  recode glbcc_risk to five categories</span>
<span class="kw">library</span>(car)
ds<span class="op">$</span>r.glbcc_risk &lt;-<span class="st"> </span>car<span class="op">::</span><span class="kw">recode</span>(ds<span class="op">$</span>glbcc_risk, <span class="st">&quot;0:1=1; 2:3=2; 4:6=3; 7:8:=4;</span>
<span class="st">                          9:10=5; NA=NA&quot;</span>)</code></pre></div>
<p>Using the <code>table</code> function, we produce a frequency table reflecting the relationship between gender and the recoded glbccrisk variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  create the table</span>
<span class="kw">table</span>(ds<span class="op">$</span>r.glbcc_risk, ds<span class="op">$</span>f.gend)</code></pre></div>
<pre><code>##    
##     Women Men
##   1   134 134
##   2   175 155
##   3   480 281
##   4   330 208
##   5   393 245</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  create the table as an R Object</span>
glbcc.table &lt;-<span class="st"> </span><span class="kw">table</span>(ds<span class="op">$</span>r.glbcc_risk, ds<span class="op">$</span>f.gend)</code></pre></div>
<p>This table is difficult to interpret because of the numbers of men and women are different. To make the table easier to interpret, we convert it to percentages using the <code>prop.table</code> function. Looking at the new table, we can see that there are more men at the lower end of the perceived risk scale and more women at the upper end.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Multiply by 100</span>
<span class="kw">prop.table</span>(glbcc.table, <span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></code></pre></div>
<pre><code>##    
##         Women       Men
##   1  8.862434 13.098729
##   2 11.574074 15.151515
##   3 31.746032 27.468231
##   4 21.825397 20.332356
##   5 25.992063 23.949169</code></pre>
<p>The percentaged table suggests that there is a relationship between the two variables, but also illustrates the challenge of relying on percentage differences to determine the significance of that relationship. So, to test our null hypothesis, we calculate our chi square using the chisq.test function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Chi Square Test</span>
<span class="kw">chisq.test</span>(glbcc.table)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  glbcc.table
## X-squared = 21.729, df = 4, p-value = 0.0002269</code></pre>
<p>R reports our chi square to equal 21.73. It also tells us that we have 4 degrees of freedom and a p value of .0002269. Since that p value is substantially less than .05, we can reject our null hypothesis with great confidence. There is, evidently, a relationship between gender and percieved risk of climate change.</p>
<p>Finally, we want to know how strong the relationship is. We use the <code>assocstats</code> function to get several measures of association. Since the table is not a 2 X 2 table nor square, neither phi not the contingency coefficient is appropriate, but we can report Cramer’s V. Cramer’s V is .093, indicating a relatively weak relationship between gender and the perceived global climate change risk variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(vcd)
<span class="kw">assocstats</span>(glbcc.table)</code></pre></div>
<pre><code>##                     X^2 df   P(&gt; X^2)
## Likelihood Ratio 21.494  4 0.00025270
## Pearson          21.729  4 0.00022695
## 
## Phi-Coefficient   : NA 
## Contingency Coeff.: 0.092 
## Cramer&#39;s V        : 0.093</code></pre>
<div id="crosstabulation-and-control" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Crosstabulation and Control</h3>
<p>In Chapter 2 we talked about the importance of experimental control if we want to make causal statements. In experimental designs we rely on physical control and randomization to provide that control to give us confidence in the causal nature of any relationship we find. With quasi-experimental designs, however, we do not have that type of control and have to wonder whether any relationship that we find might be spurious. At that point, we promised that the situation is not hopeless with quasi-experimental designs and that there are statistical substitutes for the control naturally afforded to us in experimental designs. In this section, we will describe that process when using crosstabulation. We will first look at some hypothetical data to get some clean examples of what might happen when you control for an alternative explanatory variable before looking at a real example using R.</p>
<p>The process used to control for an alternative explanatory variable, commonly referred to as a third variable, is straightforward. To control for a third variable, we first construct our original table between our independent and dependent variables. Then we sort our data into subsets based on the categories of our third variable and reconstruct new tables using our IV and DV for each subset of our data.</p>
<p>Suppose we hypothesize that people who are contacted about voting are more likely to vote. Table <a href="6-1-cross-tabulation.html#fig:control1">6.5</a> illustrates what we might find. (Remember all of these data are fabricated to illustrate our points.) According to the first table, people who are contacted are 50% more likely to vote than those who are not. But, a skeptic might say campaigns target previous voters for contact and that previous voters are more likely to vote in subsequent elections. That skeptic is making the argument that the relationship between contact and voting is spurious and that the true cause of voting is voting history. To test that theory, we control for voting history by sorting respondents into two sets – those who voted in the last election and those who did not. We then reconstruct the original table for the two sets of respondents. The new tables indicate that previous voters are 50% more likely to vote when contacted, and that those who did not vote previously are 50% more likely to vote when contacted. The skeptic is wrong; the pattern found in our original data persists even after controlling for the alternative explanation. We still remain reluctant to use causal language because another skeptic might have another alternative explanation (which would require us to go through the same process with the new third variable), but we do have more confidence in the possible causal nature of the relationship between contact and voting.</p>
<p>The next example tests the hypothesis that those who are optimistic about the future are more likely to vote for the incumbent than those who are pessimistic. Table <a href="6-1-cross-tabulation.html#fig:control2">6.6</a> shows that optimistic people are 25% more likely to vote for the incumbent than are pessimistic people. But our skeptic friend might argue that feelings about the world are not nearly as important as real life conditions. People with jobs vote for the incumbent more often than those without a job and, of course, those with a job are more likely to feel good about the world. To test that alternative, we control for whether the respondent has a job and reconstruct new tables. When we do, we find that among those with a job, 70% vote for the incumbent - regardless of their level of optimism about the world. And, among those without a job, 40% vote for the incumbent, regardless of their optimism. In other words, after controlling for job status, there is no relationship between level of optimism and voting behavior. The original relationship was spurious.</p>
<div class="figure"><span id="fig:control1"></span>
<img src="control1.PNG" alt="Controlling for a Third Variable: Nothing Changes" width="320" />
<p class="caption">
Figure 6.5: Controlling for a Third Variable: Nothing Changes
</p>
</div>
<div class="figure"><span id="fig:control2"></span>
<img src="control2.PNG" alt="Controlling for a Third Variable: Spurious" width="188" />
<p class="caption">
Figure 6.6: Controlling for a Third Variable: Spurious
</p>
</div>
<p>A third outcome of controlling for a third variable might be some form of interaction or specification effect. The third variable affects how the first two are related, but it does not completely undermine the original relationship. For example, we might find the original relationship to be stronger for one category of the control variable than another - or even to be present in one case and not the other. The pattern might also suggest that both variables have an influence on the dependent variable, resembling some form of joint causation. In fact, it is possible for your relationship to appear to be null in your original table, but when you control you might find a positive relationship for one category of your control variable and negative for another.</p>
<p>Using an example from the Climate and Weather survey, we might hypothesize that liberals are more likely to think that greenhouse gases are causing global warming. We start by recoding ideology from 7 levels to 3, then construct a frequency table and convert it to a percentage table of the relationship.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  recode variables ideology to 3 categories</span>
<span class="kw">library</span>(car)
ds<span class="op">$</span>r.ideol&lt;-car<span class="op">::</span><span class="kw">recode</span>(ds<span class="op">$</span>ideol, <span class="st">&quot;1:2=1; 3:5=2; 6:7=3; NA=NA&quot;</span>)

<span class="co">#  factor the variables to add labels.</span>
ds<span class="op">$</span>f.ideol&lt;-<span class="st"> </span><span class="kw">factor</span>(ds<span class="op">$</span>r.ideol, <span class="dt">levels=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;Liberal&quot;</span>, 
                        <span class="st">&quot;Moderate&quot;</span>, <span class="st">&quot;Conservative&quot;</span>))
ds<span class="op">$</span>f.glbcc &lt;-<span class="st"> </span><span class="kw">factor</span>(ds<span class="op">$</span>glbcc, <span class="dt">levels=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), 
                          <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;GLBCC No&quot;</span>, <span class="st">&quot;GLBCC Yes&quot;</span>))

<span class="co">#  3 Two variable table glbcc~ideology</span>
v2.glbcc.table &lt;-<span class="st"> </span><span class="kw">table</span>(ds<span class="op">$</span>f.glbcc, ds<span class="op">$</span>f.ideol)
v2.glbcc.table</code></pre></div>
<pre><code>##            
##             Liberal Moderate Conservative
##   GLBCC No       26      322          734
##   GLBCC Yes     375      762          305</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Percentages by Column</span>
<span class="kw">prop.table</span>(v2.glbcc.table, <span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></code></pre></div>
<pre><code>##            
##               Liberal  Moderate Conservative
##   GLBCC No   6.483791 29.704797    70.644851
##   GLBCC Yes 93.516209 70.295203    29.355149</code></pre>
<p>It appears that our hypothesis is supported, as there is more than a 40% difference between liberals and conservatives with moderates in between. However, let’s consider the chi square before we reject our null hypothesis:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Chi-squared</span>
<span class="kw">chisq.test</span>(v2.glbcc.table, <span class="dt">correct =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  v2.glbcc.table
## X-squared = 620.76, df = 2, p-value &lt; 0.00000000000000022</code></pre>
<p>The chi square is very large and our p-value is very small. We can therefore reject our null hypothesis with great confidence. Next, we consider the strength of the association using Cramer’s V (since either Phi nor the contingency coefficient is appropriate for a 3 X 2 table):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Cramer&#39;s V</span>
<span class="kw">library</span>(vcd)
<span class="kw">assocstats</span>(v2.glbcc.table)</code></pre></div>
<pre><code>##                     X^2 df P(&gt; X^2)
## Likelihood Ratio 678.24  2        0
## Pearson          620.76  2        0
## 
## Phi-Coefficient   : NA 
## Contingency Coeff.: 0.444 
## Cramer&#39;s V        : 0.496</code></pre>
<p>The Cramer’s V value of .496 indicates that we have a strong relationship between political ideology and beliefs about climate change.</p>
<p>We might, though, want to look at gender as a control variable since we know gender is related both to perceptions on the climate and ideology. First we need to generate a new table with the control variable gender added. We start by factoring the gender variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  factor the variables to add labels.</span>
ds<span class="op">$</span>f.gend &lt;-<span class="st"> </span><span class="kw">factor</span>(ds<span class="op">$</span>gend, <span class="dt">levels=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;Women&quot;</span>, <span class="st">&quot;Men&quot;</span>))</code></pre></div>
<p>We then create the new table. The R output is shown, in which the line <code>\#\# ,  ,  = Women&quot; indicates the results for women and &quot;\#\# ,  ,  = Men</code> displays the results for men.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  3 Two variable table glbcc~ideology+gend</span>
v3.glbcc.table &lt;-<span class="st"> </span><span class="kw">table</span>(ds<span class="op">$</span>f.glbcc, ds<span class="op">$</span>f.ideol, ds<span class="op">$</span>f.gend) 
v3.glbcc.table</code></pre></div>
<pre><code>## , ,  = Women
## 
##            
##             Liberal Moderate Conservative
##   GLBCC No       18      206          375
##   GLBCC Yes     239      470          196
## 
## , ,  = Men
## 
##            
##             Liberal Moderate Conservative
##   GLBCC No        8      116          358
##   GLBCC Yes     136      292          109</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Percentages by Column for Women </span>
<span class="kw">prop.table</span>(v3.glbcc.table[,,<span class="dv">1</span>], <span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span>       </code></pre></div>
<pre><code>##            
##               Liberal  Moderate Conservative
##   GLBCC No   7.003891 30.473373    65.674256
##   GLBCC Yes 92.996109 69.526627    34.325744</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(v3.glbcc.table[,,<span class="dv">1</span>])</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  v3.glbcc.table[, , 1]
## X-squared = 299.39, df = 2, p-value &lt; 0.00000000000000022</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">assocstats</span>(v3.glbcc.table[,,<span class="dv">1</span>])</code></pre></div>
<pre><code>##                     X^2 df P(&gt; X^2)
## Likelihood Ratio 326.13  2        0
## Pearson          299.39  2        0
## 
## Phi-Coefficient   : NA 
## Contingency Coeff.: 0.407 
## Cramer&#39;s V        : 0.446</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#  Percentages by Column for  Men </span>
<span class="kw">prop.table</span>(v3.glbcc.table[,,<span class="dv">2</span>], <span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span>    </code></pre></div>
<pre><code>##            
##               Liberal  Moderate Conservative
##   GLBCC No   5.555556 28.431373    76.659529
##   GLBCC Yes 94.444444 71.568627    23.340471</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(v3.glbcc.table[,,<span class="dv">2</span>])</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  v3.glbcc.table[, , 2]
## X-squared = 320.43, df = 2, p-value &lt; 0.00000000000000022</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">assocstats</span>(v3.glbcc.table[,,<span class="dv">2</span>])</code></pre></div>
<pre><code>##                     X^2 df P(&gt; X^2)
## Likelihood Ratio 353.24  2        0
## Pearson          320.43  2        0
## 
## Phi-Coefficient   : NA 
## Contingency Coeff.: 0.489 
## Cramer&#39;s V        : 0.561</code></pre>
<p>For both men and women, we still see more than a 40% difference and the p value for both tables chi square is 2.2e-16 and both Cramer’s V’s are greater than .30. It is clear that even when controlling for gender, there is a robust relationship between ideology and perceived risk of climate change. However, these tables also suggest that women are slightly more inclined to believe greenhouse gases play a role in climate change than are men. We may have an instance of joint causation, where both ideology and gender affect (``cause&quot; is still too strong a word) views concerning the impact of greenhouse gases on climate change.</p>
<p>Crosstabs, chi square, and measures of association are used with nominal and ordinal data to provide an overview of a relationship, its statistical significance, and the strength of a relationship. In the next section, we turn to ways to consider the same set of questions with interval level data before turning to the more advanced technique of regression analysis in Part 2 of this book.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="12">
<li id="fn12"><p>To reiterate the general decision rule: if the probability that we could have a 20% difference in our sample if the null hypothesis is true is less than .05, we will reject our null hypothesis.<a href="6-1-cross-tabulation.html#fnref12">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="6-association-of-variables.html"><button class="btn btn-default">Previous</button></a>
<a href="6-2-covariance.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
