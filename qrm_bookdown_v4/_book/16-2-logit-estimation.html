<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="16.2 Logit Estimation | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R" />
<meta property="og:type" content="book" />





<meta name="author" content="Hank Jenkins-Smith" />
<meta name="author" content="Joseph Ripberger" />
<meta name="author" content="Gary Copeland" />
<meta name="author" content="Matthew Nowlin" />
<meta name="author" content="Tyler Hughes" />
<meta name="author" content="Aaron Fister" />
<meta name="author" content="Wesley Wehde" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="16.2 Logit Estimation | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R">

<title>16.2 Logit Estimation | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#preface-and-acknowledgments">Preface and Acknowledgments</a><ul>
<li><a href="copyright.html#copyright">Copyright</a></li>
</ul></li>
<li class="has-sub"><a href="1-theories-and-social-science.html#theories-and-social-science"><span class="toc-section-number">1</span> Theories and Social Science</a><ul>
<li><a href="1-1-the-scientific-method.html#the-scientific-method"><span class="toc-section-number">1.1</span> The Scientific Method</a></li>
<li class="has-sub"><a href="1-2-theory-and-empirical-research.html#theory-and-empirical-research"><span class="toc-section-number">1.2</span> Theory and Empirical Research</a><ul>
<li><a href="1-2-theory-and-empirical-research.html#coherent-and-internally-consistent"><span class="toc-section-number">1.2.1</span> Coherent and Internally Consistent</a></li>
<li><a href="1-2-theory-and-empirical-research.html#theories-and-causality"><span class="toc-section-number">1.2.2</span> Theories and Causality</a></li>
<li><a href="1-2-theory-and-empirical-research.html#generation-of-testable-hypothesis"><span class="toc-section-number">1.2.3</span> Generation of Testable Hypothesis</a></li>
</ul></li>
<li><a href="1-3-theory-and-functions.html#theory-and-functions"><span class="toc-section-number">1.3</span> Theory and Functions</a></li>
<li><a href="1-4-theory-in-social-science.html#theory-in-social-science"><span class="toc-section-number">1.4</span> Theory in Social Science</a></li>
<li><a href="1-5-outline-of-the-book.html#outline-of-the-book"><span class="toc-section-number">1.5</span> Outline of the Book</a></li>
</ul></li>
<li class="has-sub"><a href="2-research-design.html#research-design"><span class="toc-section-number">2</span> Research Design</a><ul>
<li><a href="2-1-overview-of-the-research-process.html#overview-of-the-research-process"><span class="toc-section-number">2.1</span> Overview of the Research Process</a></li>
<li><a href="2-2-internal-and-external-validity.html#internal-and-external-validity"><span class="toc-section-number">2.2</span> Internal and External Validity</a></li>
<li><a href="2-3-major-classes-of-designs.html#major-classes-of-designs"><span class="toc-section-number">2.3</span> Major Classes of Designs</a></li>
<li><a href="2-4-threats-to-validity.html#threats-to-validity"><span class="toc-section-number">2.4</span> Threats to Validity</a></li>
<li><a href="2-5-some-common-designs.html#some-common-designs"><span class="toc-section-number">2.5</span> Some Common Designs</a></li>
<li><a href="2-6-plan-meets-reality.html#plan-meets-reality"><span class="toc-section-number">2.6</span> Plan Meets Reality</a></li>
</ul></li>
<li class="has-sub"><a href="3-exploring-and-visualizing-data.html#exploring-and-visualizing-data"><span class="toc-section-number">3</span> Exploring and Visualizing Data</a><ul>
<li class="has-sub"><a href="3-1-characterizing-data.html#characterizing-data"><span class="toc-section-number">3.1</span> Characterizing Data</a><ul>
<li><a href="3-1-characterizing-data.html#central-tendency"><span class="toc-section-number">3.1.1</span> Central Tendency</a></li>
<li><a href="3-1-characterizing-data.html#level-of-measurement-and-central-tendency"><span class="toc-section-number">3.1.2</span> Level of Measurement and Central Tendency</a></li>
<li><a href="3-1-characterizing-data.html#moments"><span class="toc-section-number">3.1.3</span> Moments</a></li>
<li><a href="3-1-characterizing-data.html#first-moment-expected-value"><span class="toc-section-number">3.1.4</span> First Moment – Expected Value</a></li>
<li><a href="3-1-characterizing-data.html#the-second-moment-variance-and-standard-deviation"><span class="toc-section-number">3.1.5</span> The Second Moment – Variance and Standard Deviation</a></li>
<li><a href="3-1-characterizing-data.html#the-third-moment-skewness"><span class="toc-section-number">3.1.6</span> The Third Moment – Skewness</a></li>
<li><a href="3-1-characterizing-data.html#the-fourth-moment-kurtosis"><span class="toc-section-number">3.1.7</span> The Fourth Moment – Kurtosis</a></li>
<li><a href="3-1-characterizing-data.html#order-statistics"><span class="toc-section-number">3.1.8</span> Order Statistics</a></li>
</ul></li>
<li><a href="3-2-summary.html#summary"><span class="toc-section-number">3.2</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="4-probability.html#probability"><span class="toc-section-number">4</span> Probability</a><ul>
<li><a href="4-1-finding-probabilities.html#finding-probabilities"><span class="toc-section-number">4.1</span> Finding Probabilities</a></li>
<li><a href="4-2-finding-probabilities-with-the-normal-curve.html#finding-probabilities-with-the-normal-curve"><span class="toc-section-number">4.2</span> Finding Probabilities with the Normal Curve</a></li>
<li><a href="4-3-summary-1.html#summary-1"><span class="toc-section-number">4.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a><ul>
<li class="has-sub"><a href="5-1-inference-populations-and-samples.html#inference-populations-and-samples"><span class="toc-section-number">5.1</span> Inference: Populations and Samples</a><ul>
<li><a href="5-1-inference-populations-and-samples.html#populations-and-samples"><span class="toc-section-number">5.1.1</span> Populations and Samples</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-and-knowing"><span class="toc-section-number">5.1.2</span> Sampling and Knowing</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-strategies"><span class="toc-section-number">5.1.3</span> Sampling Strategies</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-techniques"><span class="toc-section-number">5.1.4</span> Sampling Techniques</a></li>
<li><a href="5-1-inference-populations-and-samples.html#so-how-is-it-that-we-know"><span class="toc-section-number">5.1.5</span> So How is it That We Know?</a></li>
</ul></li>
<li class="has-sub"><a href="5-2-the-normal-distribution.html#the-normal-distribution"><span class="toc-section-number">5.2</span> The Normal Distribution</a><ul>
<li><a href="5-2-the-normal-distribution.html#standardizing-a-normal-distribution-and-z-scores"><span class="toc-section-number">5.2.1</span> Standardizing a Normal Distribution and Z-scores</a></li>
<li><a href="5-2-the-normal-distribution.html#the-central-limit-theorem"><span class="toc-section-number">5.2.2</span> The Central Limit Theorem</a></li>
<li><a href="5-2-the-normal-distribution.html#populations-samples-and-symbols"><span class="toc-section-number">5.2.3</span> Populations, Samples and Symbols</a></li>
</ul></li>
<li class="has-sub"><a href="5-3-inferences-to-the-population-from-the-sample.html#inferences-to-the-population-from-the-sample"><span class="toc-section-number">5.3</span> Inferences to the Population from the Sample</a><ul>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#confidence-intervals"><span class="toc-section-number">5.3.1</span> Confidence Intervals</a></li>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#the-logic-of-hypothesis-testing"><span class="toc-section-number">5.3.2</span> The Logic of Hypothesis Testing</a></li>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#some-miscellaneous-notes-about-hypothesis-testing"><span class="toc-section-number">5.3.3</span> Some Miscellaneous Notes about Hypothesis Testing</a></li>
</ul></li>
<li class="has-sub"><a href="5-4-differences-between-groups.html#differences-between-groups"><span class="toc-section-number">5.4</span> Differences Between Groups</a><ul>
<li><a href="5-4-differences-between-groups.html#t-tests"><span class="toc-section-number">5.4.1</span> <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li><a href="5-5-summary-2.html#summary-2"><span class="toc-section-number">5.5</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="6-association-of-variables.html#association-of-variables"><span class="toc-section-number">6</span> Association of Variables</a><ul>
<li class="has-sub"><a href="6-1-cross-tabulation.html#cross-tabulation"><span class="toc-section-number">6.1</span> Cross-Tabulation</a><ul>
<li><a href="6-1-cross-tabulation.html#crosstabulation-and-control"><span class="toc-section-number">6.1.1</span> Crosstabulation and Control</a></li>
</ul></li>
<li><a href="6-2-covariance.html#covariance"><span class="toc-section-number">6.2</span> Covariance</a></li>
<li><a href="6-3-correlation.html#correlation"><span class="toc-section-number">6.3</span> Correlation</a></li>
<li><a href="6-4-scatterplots.html#scatterplots"><span class="toc-section-number">6.4</span> Scatterplots</a></li>
</ul></li>
<li class="has-sub"><a href="7-the-logic-of-ordinary-least-squares-estimation.html#the-logic-of-ordinary-least-squares-estimation"><span class="toc-section-number">7</span> The Logic of Ordinary Least Squares Estimation</a><ul>
<li class="has-sub"><a href="7-1-theoretical-models.html#theoretical-models"><span class="toc-section-number">7.1</span> Theoretical Models</a><ul>
<li><a href="7-1-theoretical-models.html#deterministic-linear-model"><span class="toc-section-number">7.1.1</span> Deterministic Linear Model</a></li>
<li><a href="7-1-theoretical-models.html#stochastic-linear-model"><span class="toc-section-number">7.1.2</span> Stochastic Linear Model</a></li>
<li><a href="7-1-theoretical-models.html#assumptions-about-the-error-term"><span class="toc-section-number">7.1.3</span> Assumptions about the Error Term</a></li>
</ul></li>
<li class="has-sub"><a href="7-2-estimating-linear-models.html#estimating-linear-models"><span class="toc-section-number">7.2</span> Estimating Linear Models</a><ul>
<li><a href="7-2-estimating-linear-models.html#residuals"><span class="toc-section-number">7.2.1</span> Residuals</a></li>
</ul></li>
<li><a href="7-3-an-example-of-simple-regression.html#an-example-of-simple-regression"><span class="toc-section-number">7.3</span> An Example of Simple Regression</a></li>
</ul></li>
<li class="has-sub"><a href="8-linear-estimation-and-minimizing-error.html#linear-estimation-and-minimizing-error"><span class="toc-section-number">8</span> Linear Estimation and Minimizing Error</a><ul>
<li class="has-sub"><a href="8-1-minimizing-error-using-derivatives.html#minimizing-error-using-derivatives"><span class="toc-section-number">8.1</span> Minimizing Error using Derivatives</a><ul>
<li><a href="8-1-minimizing-error-using-derivatives.html#rules-of-derivation"><span class="toc-section-number">8.1.1</span> Rules of Derivation</a></li>
<li><a href="8-1-minimizing-error-using-derivatives.html#critical-points"><span class="toc-section-number">8.1.2</span> Critical Points</a></li>
<li><a href="8-1-minimizing-error-using-derivatives.html#partial-derivation"><span class="toc-section-number">8.1.3</span> Partial Derivation</a></li>
</ul></li>
<li class="has-sub"><a href="8-2-deriving-ols-estimators.html#deriving-ols-estimators"><span class="toc-section-number">8.2</span> Deriving OLS Estimators</a><ul>
<li><a href="8-2-deriving-ols-estimators.html#ols-derivation-of-hatalpha"><span class="toc-section-number">8.2.1</span> OLS Derivation of <span class="math inline">\(\hat{\alpha}\)</span></a></li>
<li><a href="8-2-deriving-ols-estimators.html#ols-derivation-of-hatbeta"><span class="toc-section-number">8.2.2</span> OLS Derivation of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li><a href="8-2-deriving-ols-estimators.html#interpreting-hatbeta-and-hatalpha"><span class="toc-section-number">8.2.3</span> Interpreting <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\alpha}\)</span></a></li>
</ul></li>
<li><a href="8-3-summary-3.html#summary-3"><span class="toc-section-number">8.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="9-bi-variate-hypothesis-testing-and-model-fit.html#bi-variate-hypothesis-testing-and-model-fit"><span class="toc-section-number">9</span> Bi-Variate Hypothesis Testing and Model Fit</a><ul>
<li class="has-sub"><a href="9-1-hypothesis-tests-for-regression-coefficients.html#hypothesis-tests-for-regression-coefficients"><span class="toc-section-number">9.1</span> Hypothesis Tests for Regression Coefficients</a><ul>
<li><a href="9-1-hypothesis-tests-for-regression-coefficients.html#residual-standard-error"><span class="toc-section-number">9.1.1</span> Residual Standard Error</a></li>
</ul></li>
<li class="has-sub"><a href="9-2-measuring-goodness-of-fit.html#measuring-goodness-of-fit"><span class="toc-section-number">9.2</span> Measuring Goodness of Fit</a><ul>
<li><a href="9-2-measuring-goodness-of-fit.html#sample-covariance-and-correlations"><span class="toc-section-number">9.2.1</span> Sample Covariance and Correlations</a></li>
<li><a href="9-2-measuring-goodness-of-fit.html#coefficient-of-determination-r2"><span class="toc-section-number">9.2.2</span> Coefficient of Determination: <span class="math inline">\(R^{2}\)</span></a></li>
<li><a href="9-2-measuring-goodness-of-fit.html#visualizing-bivariate-regression"><span class="toc-section-number">9.2.3</span> Visualizing Bivariate Regression</a></li>
</ul></li>
<li><a href="9-3-summary-4.html#summary-4"><span class="toc-section-number">9.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="10-ols-assumptions-and-simple-regression-diagnostics.html#ols-assumptions-and-simple-regression-diagnostics"><span class="toc-section-number">10</span> OLS Assumptions and Simple Regression Diagnostics</a><ul>
<li><a href="10-1-a-recap-of-modeling-assumptions.html#a-recap-of-modeling-assumptions"><span class="toc-section-number">10.1</span> A Recap of Modeling Assumptions</a></li>
<li class="has-sub"><a href="10-2-when-things-go-bad-with-residuals.html#when-things-go-bad-with-residuals"><span class="toc-section-number">10.2</span> When Things Go Bad with Residuals</a><ul>
<li><a href="10-2-when-things-go-bad-with-residuals.html#outlier-data"><span class="toc-section-number">10.2.1</span> “Outlier” Data</a></li>
<li><a href="10-2-when-things-go-bad-with-residuals.html#non-constant-variance"><span class="toc-section-number">10.2.2</span> Non-Constant Variance</a></li>
<li><a href="10-2-when-things-go-bad-with-residuals.html#non-linearity-in-the-parameters"><span class="toc-section-number">10.2.3</span> Non-Linearity in the Parameters</a></li>
</ul></li>
<li class="has-sub"><a href="10-3-application-of-residual-diagnostics.html#application-of-residual-diagnostics"><span class="toc-section-number">10.3</span> Application of Residual Diagnostics</a><ul>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-non-linearity"><span class="toc-section-number">10.3.1</span> Testing for Non-Linearity</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-normality-in-model-residuals"><span class="toc-section-number">10.3.2</span> Testing for Normality in Model Residuals</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-non-constant-variance-in-the-residuals"><span class="toc-section-number">10.3.3</span> Testing for Non-Constant Variance in the Residuals</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#examining-outlier-data"><span class="toc-section-number">10.3.4</span> Examining Outlier Data</a></li>
</ul></li>
<li><a href="10-4-so-now-what-implications-of-residual-analysis.html#so-now-what-implications-of-residual-analysis"><span class="toc-section-number">10.4</span> So Now What? Implications of Residual Analysis</a></li>
<li><a href="10-5-summary-5.html#summary-5"><span class="toc-section-number">10.5</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="11-introduction-to-multiple-regression.html#introduction-to-multiple-regression"><span class="toc-section-number">11</span> Introduction to Multiple Regression</a><ul>
<li><a href="11-1-matrix-algebra-and-multiple-regression.html#matrix-algebra-and-multiple-regression"><span class="toc-section-number">11.1</span> Matrix Algebra and Multiple Regression</a></li>
<li class="has-sub"><a href="11-2-the-basics-of-matrix-algebra.html#the-basics-of-matrix-algebra"><span class="toc-section-number">11.2</span> The Basics of Matrix Algebra</a><ul>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-basics"><span class="toc-section-number">11.2.1</span> Matrix Basics</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#vectors"><span class="toc-section-number">11.2.2</span> Vectors</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-operations"><span class="toc-section-number">11.2.3</span> Matrix Operations</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#transpose"><span class="toc-section-number">11.2.4</span> Transpose</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#adding-matrices"><span class="toc-section-number">11.2.5</span> Adding Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#multiplication-of-matrices"><span class="toc-section-number">11.2.6</span> Multiplication of Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#identity-matrices"><span class="toc-section-number">11.2.7</span> Identity Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-inversion"><span class="toc-section-number">11.2.8</span> Matrix Inversion</a></li>
</ul></li>
<li><a href="11-3-ols-regression-in-matrix-form.html#ols-regression-in-matrix-form"><span class="toc-section-number">11.3</span> OLS Regression in Matrix Form</a></li>
<li><a href="11-4-summary-6.html#summary-6"><span class="toc-section-number">11.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="12-the-logic-of-multiple-regression.html#the-logic-of-multiple-regression"><span class="toc-section-number">12</span> The Logic of Multiple Regression</a><ul>
<li class="has-sub"><a href="12-1-theoretical-specification.html#theoretical-specification"><span class="toc-section-number">12.1</span> Theoretical Specification</a><ul>
<li><a href="12-1-theoretical-specification.html#assumptions-of-ols-regression"><span class="toc-section-number">12.1.1</span> Assumptions of OLS Regression</a></li>
</ul></li>
<li><a href="12-2-partial-effects.html#partial-effects"><span class="toc-section-number">12.2</span> Partial Effects</a></li>
<li class="has-sub"><a href="12-3-multiple-regression-example.html#multiple-regression-example"><span class="toc-section-number">12.3</span> Multiple Regression Example</a><ul>
<li><a href="12-3-multiple-regression-example.html#hypothesis-testing-and-t-tests"><span class="toc-section-number">12.3.1</span> Hypothesis Testing and <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li><a href="12-4-summary-7.html#summary-7"><span class="toc-section-number">12.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="13-multiple-regression-and-model-building.html#multiple-regression-and-model-building"><span class="toc-section-number">13</span> Multiple Regression and Model Building</a><ul>
<li class="has-sub"><a href="13-1-model-building.html#model-building"><span class="toc-section-number">13.1</span> Model Building</a><ul>
<li><a href="13-1-model-building.html#theory-and-hypotheses"><span class="toc-section-number">13.1.1</span> Theory and Hypotheses</a></li>
<li><a href="13-1-model-building.html#empirical-indicators"><span class="toc-section-number">13.1.2</span> Empirical Indicators</a></li>
<li><a href="13-1-model-building.html#risks-in-model-building"><span class="toc-section-number">13.1.3</span> Risks in Model Building</a></li>
</ul></li>
<li><a href="13-2-evils-of-stepwise-regression.html#evils-of-stepwise-regression"><span class="toc-section-number">13.2</span> Evils of Stepwise Regression</a></li>
<li><a href="13-3-summary-8.html#summary-8"><span class="toc-section-number">13.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="14-topics-in-multiple-regression.html#topics-in-multiple-regression"><span class="toc-section-number">14</span> Topics in Multiple Regression</a><ul>
<li><a href="14-1-dummy-variables.html#dummy-variables"><span class="toc-section-number">14.1</span> Dummy Variables</a></li>
<li><a href="14-2-interaction-effects.html#interaction-effects"><span class="toc-section-number">14.2</span> Interaction Effects</a></li>
<li><a href="14-3-standardized-regression-coefficients.html#standardized-regression-coefficients"><span class="toc-section-number">14.3</span> Standardized Regression Coefficients</a></li>
<li><a href="14-4-summary-9.html#summary-9"><span class="toc-section-number">14.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="15-the-art-of-regression-diagnostics.html#the-art-of-regression-diagnostics"><span class="toc-section-number">15</span> The Art of Regression Diagnostics</a><ul>
<li><a href="15-1-ols-error-assumptions-revisited.html#ols-error-assumptions-revisited"><span class="toc-section-number">15.1</span> OLS Error Assumptions Revisited</a></li>
<li class="has-sub"><a href="15-2-ols-diagnostic-techniques.html#ols-diagnostic-techniques"><span class="toc-section-number">15.2</span> OLS Diagnostic Techniques</a><ul>
<li><a href="15-2-ols-diagnostic-techniques.html#non-linearity"><span class="toc-section-number">15.2.1</span> Non-Linearity</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#non-constant-variance-or-heteroscedasticity"><span class="toc-section-number">15.2.2</span> Non-Constant Variance, or Heteroscedasticity</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#independence-of-e"><span class="toc-section-number">15.2.3</span> Independence of <span class="math inline">\(E\)</span></a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#normality-of-the-residuals"><span class="toc-section-number">15.2.4</span> Normality of the Residuals</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#outliers-leverage-and-influence"><span class="toc-section-number">15.2.5</span> Outliers, Leverage, and Influence</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#outliers"><span class="toc-section-number">15.2.6</span> Outliers</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#multicollinearity"><span class="toc-section-number">15.2.7</span> Multicollinearity</a></li>
</ul></li>
<li><a href="15-3-summary-10.html#summary-10"><span class="toc-section-number">15.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="16-logit-regression.html#logit-regression"><span class="toc-section-number">16</span> Logit Regression</a><ul>
<li><a href="16-1-generalized-linear-models.html#generalized-linear-models"><span class="toc-section-number">16.1</span> Generalized Linear Models</a></li>
<li class="has-sub"><a href="16-2-logit-estimation.html#logit-estimation"><span class="toc-section-number">16.2</span> Logit Estimation</a><ul>
<li><a href="16-2-logit-estimation.html#logit-hypothesis-tests"><span class="toc-section-number">16.2.1</span> Logit Hypothesis Tests</a></li>
<li><a href="16-2-logit-estimation.html#goodness-of-fit"><span class="toc-section-number">16.2.2</span> Goodness of Fit</a></li>
<li><a href="16-2-logit-estimation.html#interpreting-logits"><span class="toc-section-number">16.2.3</span> Interpreting Logits</a></li>
</ul></li>
<li><a href="16-3-summary-11.html#summary-11"><span class="toc-section-number">16.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="17-appendix-basic-r.html#appendix-basic-r"><span class="toc-section-number">17</span> Appendix: Basic R</a><ul>
<li><a href="17-1-introduction-to-r.html#introduction-to-r"><span class="toc-section-number">17.1</span> Introduction to R</a></li>
<li><a href="17-2-downloading-r-and-rstudio.html#downloading-r-and-rstudio"><span class="toc-section-number">17.2</span> Downloading R and RStudio</a></li>
<li><a href="17-3-introduction-to-programming.html#introduction-to-programming"><span class="toc-section-number">17.3</span> Introduction to Programming</a></li>
<li><a href="17-4-uploadingreading-data.html#uploadingreading-data"><span class="toc-section-number">17.4</span> Uploading/Reading Data</a></li>
<li><a href="17-5-data-manipulation-in-r.html#data-manipulation-in-r"><span class="toc-section-number">17.5</span> Data Manipulation in R</a></li>
<li><a href="17-6-savingwriting-data.html#savingwriting-data"><span class="toc-section-number">17.6</span> Saving/Writing Data</a></li>
<li><a href="17-7-the-tidyverse.html#the-tidyverse"><span class="toc-section-number">17.7</span> The Tidyverse</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="logit-estimation" class="section level2">
<h2><span class="header-section-number">16.2</span> Logit Estimation</h2>
<p>Logit is used when predicting limited dependent variables, specifically those in which <span class="math inline">\(Y\)</span> is represented by <span class="math inline">\(0\)</span>’s and <span class="math inline">\(1\)</span>’s. By virtue of the binary dependent variable, these models do not meet the key assumptions of OLS. Logit uses maximum likelihood estimation (MLE), which is a counterpart to minimizing least squares. MLE identifies the probability of obtaining the sample as a function of the model parameters (i.e., the <span class="math inline">\(X\)</span>’s). It answers the question, what are the values for <span class="math inline">\(B\)</span>’s that make the sample most likely? In other words, the likelihood function expresses the probability of obtaining the observed data as a function of the model parameters. Estimates of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are based on maximizing a likelihood function of the observed <span class="math inline">\(Y\)</span> values.<br />
In logit estimation we seek <span class="math inline">\(P(Y=1)\)</span>, the probability that <span class="math inline">\(Y=1\)</span>. The odds that <span class="math inline">\(Y=1\)</span> are expressed as:</p>
<span class="math display">\[\begin{equation*}
  O(Y=1)=\frac{P(Y=1)}{1-P(Y=1)}
\end{equation*}\]</span>
<p>Logits, <span class="math inline">\(L\)</span>, are the natural logarithm of the odds:</p>
<span class="math display">\[\begin{align*}
  L &amp;= log_e O \\
  &amp;=log_e \frac{P}{1-P}
\end{align*}\]</span>
<p>They can range from <span class="math inline">\(-\infty\)</span>, when <span class="math inline">\(P=0\)</span>, to <span class="math inline">\(\infty\)</span>, when <span class="math inline">\(P=1\)</span>. <span class="math inline">\(L\)</span> is the estimated systematic linear component:</p>
<span class="math display">\[\begin{equation*}
  L = A+B_1 X_{i1}+\ldots+B_k X_{ik}
\end{equation*}\]</span>
<p>By reversing the logit we can obtain the predicted probability that <span class="math inline">\(Y=1\)</span> for each of the <span class="math inline">\(i\)</span> observations:</p>
<span class="math display" id="eq:16-2">\[\begin{equation}
  P_{i} = \frac{1}{1-e^{-L_{i}}}
  \tag{16.2}
\end{equation}\]</span>
<p>where <span class="math inline">\(e=2.71828 \ldots\)</span>, the base number of natural logarithms. Note that <span class="math inline">\(L\)</span> is a linear function, but <span class="math inline">\(P\)</span> is a non-linear <span class="math inline">\(S\)</span>-shaped function as shown in Figure <a href="16-2-logit-estimation.html#fig:logex">16.2</a>. Also note, that Equation 16.2 is the link function that relates the linear component to the non-linear response variable.</p>
<div class="figure"><span id="fig:logex"></span>
<img src="_main_files/figure-html/logex-1.png" alt="Predicted Probability as a Logit Function of $X$" width="672" />
<p class="caption">
Figure 16.2: Predicted Probability as a Logit Function of <span class="math inline">\(X\)</span>
</p>
</div>
<p>In more formal terms, each observation, <span class="math inline">\(i\)</span>, contributes to the likelihood function by <span class="math inline">\(P_i\)</span> if <span class="math inline">\(Y_i=1\)</span>, and by <span class="math inline">\(1-P_i\)</span> if <span class="math inline">\(Y_i=0\)</span>. This is defined as:</p>
<span class="math display">\[\begin{equation*}
  P^{Y_{i}}_i(1-P_i)^{1-Y_i}
\end{equation*}\]</span>
<p>The likelihood function is the product (multiplication) of all these individual contributions:</p>
<span class="math display">\[\begin{equation*}
  \ell = \prod P^{Y_{i}}_i(1-P_i)^{1-Y_i}
\end{equation*}\]</span>
<p>The likelihood function is the largest for the model that best predicts <span class="math inline">\(Y=1\)</span> or <span class="math inline">\(Y=0\)</span>; therefore when the predicted value of <span class="math inline">\(Y\)</span> is correct and close to <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>, the likelihood function is maximized.</p>
<p>To estimate the model parameters, we seek to maximize the log of the likelihood function. We use the log because it converts the multiplication into addition, and is therefore easier to calculate. The log likelihood is:</p>
<span class="math display">\[\begin{equation*}
  \text{log}_e \ell = \sum^{n}_{i=1}[Y_i \text{log}_e P_i+(1-Y_i)\text{log}_e(1-P_i)]
\end{equation*}\]</span>
<p>The solution involves taking the first derivative of the log likelihood with respect to each of the <span class="math inline">\(B\)</span>’s, setting them to zero, and solving the simultaneous equation. The solution of the equation isn’t linear, so it can’t be solved directly. Instead, it’s solved through a sequential estimation process that looks for successively better ``fits’’ of the model.</p>
<p>For the most part, the key assumptions required for logit models are analogous to those required for OLS. The key differences are that (a) we do not assume a linear relationship between the <span class="math inline">\(X\)</span>s and <span class="math inline">\(Y\)</span>, and (b) we do not assume normally distributed, homoscedastistic residuals. The key assumptions that are retained are shown below.</p>
<p><strong>Logit Assumptions and Qualifiers</strong> - The model is correctly specified - True conditional probabilities are logistic function of the <span class="math inline">\(X\)</span>’s - No important <span class="math inline">\(X\)</span>’s omitted; no extraneous <span class="math inline">\(X\)</span>’s included - No significant measurement error - The cases are independent - No <span class="math inline">\(X\)</span> is a linear function of other <span class="math inline">\(X\)</span>’s - Increased multicollinearity leads to greater imprecision - Influential cases can bias estimates - Sample size: <span class="math inline">\(n-k-1\)</span> should exceed <span class="math inline">\(100\)</span> - Independent covariation between the <span class="math inline">\(X\)</span>s and <span class="math inline">\(Y\)</span> is critical</p>
<p>The following example uses demographic information to predict beliefs about anthropogenic climate change.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ds.temp &lt;-<span class="st"> </span>ds <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(glbcc, age, education, income, ideol, gender) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">na.omit</span>()

logit1 &lt;-<span class="st"> </span><span class="kw">glm</span>(glbcc <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>income, <span class="dt">data =</span> ds.temp, <span class="dt">family =</span> <span class="kw">binomial</span>())
<span class="kw">summary</span>(logit1)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = glbcc ~ age + gender + education + income, family = binomial(), 
##     data = ds.temp)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.707  -1.250   0.880   1.053   1.578  
## 
## Coefficients:
##                  Estimate    Std. Error z value       Pr(&gt;|z|)    
## (Intercept)  0.4431552007  0.2344093710   1.891       0.058689 .  
## age         -0.0107882966  0.0031157929  -3.462       0.000535 ***
## gender      -0.3131329979  0.0880376089  -3.557       0.000375 ***
## education    0.1580178789  0.0251302944   6.288 0.000000000322 ***
## income      -0.0000023799  0.0000008013  -2.970       0.002977 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 3114.5  on 2281  degrees of freedom
## Residual deviance: 3047.4  on 2277  degrees of freedom
## AIC: 3057.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>As we can see, age and gender are both negative and statistically significant predictors of climate change opinion. Below we discuss logit hypothesis tests, goodness of fit, and how to interpret the logit coefficients.</p>
<div id="logit-hypothesis-tests" class="section level3">
<h3><span class="header-section-number">16.2.1</span> Logit Hypothesis Tests</h3>
<p>In some ways, hypothesis testing with logit is quite similar to that using OLS. The same use of <span class="math inline">\(p\)</span>-values is employed; however, they differ in how they are derived. The logit analysis makes use of the Wald <span class="math inline">\(z\)</span>-statistic, which is similar to the <span class="math inline">\(t\)</span>-stat in OLS. The Wald <span class="math inline">\(z\)</span> score compares the estimated coefficient to the asymptotic standard error, (aka the normal distribution). The <span class="math inline">\(p\)</span>-value is derived from the asymptotic standard-normal distribution. Each estimated coefficient has a Wald <span class="math inline">\(z\)</span>-score and a <span class="math inline">\(p\)</span>-value that shows the probability that the null hypothesis is correct, given the data.</p>
<span class="math display" id="eq:16-3">\[\begin{equation}
  z = \frac{B_j}{SE(B_j)} 
  \tag{16.3}
\end{equation}\]</span>
</div>
<div id="goodness-of-fit" class="section level3">
<h3><span class="header-section-number">16.2.2</span> Goodness of Fit</h3>
<p>Given that logit regression is estimated using MLE, the goodness-of-fit statistics differ from those of OLS. Here we examine three measures of fit: log-likelihood, the pseudo <span class="math inline">\(R^2\)</span>, and the Akaike information criteria (AIC).</p>
<div id="log-likelihood" class="section level4 unnumbered">
<h4>Log-Likelihood</h4>
<p>To test for the overall null hypothesis that all <span class="math inline">\(B\)</span>’s are equal to zero (similar to an overall <span class="math inline">\(F\)</span>-test in OLS), we can compare the log-likelihood of the demographic model with 4 IVs to the initial ``null model,&quot; which includes only the intercept term. In general, a smaller log-likelihood indicates a better fit. Using the deviance statistic <span class="math inline">\(G^2\)</span> (aka the likelihood-ratio test statistic), we can determine whether the difference is statistically significant. <span class="math inline">\(G^2\)</span> is expressed as:</p>
<span class="math display" id="eq:16-4">\[\begin{equation}
   G^2 = 2(\text{log}_e L_1 - \text{log}_e L_0)
   \tag{16.4}
\end{equation}\]</span>
<p>where <span class="math inline">\(L_1\)</span> is the demographic model and <span class="math inline">\(L_0\)</span> is the null model. The <span class="math inline">\(G^2\)</span> test statistic takes the difference between the log likelihoods of the two models and compares that to a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(q\)</span> degrees of freedom, where <span class="math inline">\(q\)</span> is the difference in the number of IVs. We can calculate this in <code>R</code>. First, we run a null model predicting belief that greenhouse gases are causing the climate to change, using only the intercept:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit0 &lt;-<span class="st"> </span><span class="kw">glm</span>(glbcc <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> ds.temp) 
<span class="kw">summary</span>(logit0)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = glbcc ~ 1, data = ds.temp)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.5732  -0.5732   0.4268   0.4268   0.4268  
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept)  0.57318    0.01036   55.35 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 0.2447517)
## 
##     Null deviance: 558.28  on 2281  degrees of freedom
## Residual deviance: 558.28  on 2281  degrees of freedom
## AIC: 3267.1
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>We then calculate the log likelihood for the null model,</p>
<span class="math display" id="eq:16-5">\[\begin{equation}\text{log}_e L_0
\tag{16.5}
\end{equation}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(logit0)</code></pre></div>
<pre><code>## &#39;log Lik.&#39; -1631.548 (df=2)</code></pre>
<p>Next, we calculate the log likelihood for the demographic model,</p>
<span class="math display" id="eq:16-6">\[\begin{equation}\text{log}_e L_0
\tag{16.6}
\end{equation}\]</span>
<p>Recall that we generated this model (dubbed “logit1”) earlier:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">logLik</span>(logit1)</code></pre></div>
<pre><code>## &#39;log Lik.&#39; -1523.724 (df=5)</code></pre>
<p>Finally, we calculate the <span class="math inline">\(G\)</span> statistic and perform the chi-square test for statistical significance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">G &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span>(<span class="op">-</span><span class="dv">1523</span> <span class="op">-</span><span class="st"> </span>(<span class="op">-</span><span class="dv">1631</span>))
G</code></pre></div>
<pre><code>## [1] 216</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(G, <span class="dt">df =</span> <span class="dv">3</span>, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## [1] 0.0000000000000000000000000000000000000000000001470144</code></pre>
<p>We can see by the very low p-value that the demographic model offers a significant improvement in fit.</p>
<p>The same approach can be used to compare nested models, similar to nested <span class="math inline">\(F\)</span>-tests in OLS. For example, we can include ideology in the model and use the <code>anova</code> function to see if the ideology variable improves model fit. Note that we specify the <span class="math inline">\(\chi^2\)</span> test.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2 &lt;-<span class="st"> </span><span class="kw">glm</span>(glbcc <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>gender <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>ideol,
              <span class="dt">family =</span> <span class="kw">binomial</span>(), <span class="dt">data =</span> ds.temp)
<span class="kw">summary</span>(logit2)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = glbcc ~ age + gender + education + income + ideol, 
##     family = binomial(), data = ds.temp)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.6661  -0.8939   0.3427   0.8324   2.0212  
## 
## Coefficients:
##                  Estimate    Std. Error z value             Pr(&gt;|z|)    
## (Intercept)  4.0545788430  0.3210639034  12.629 &lt; 0.0000000000000002 ***
## age         -0.0042866683  0.0036304540  -1.181             0.237701    
## gender      -0.2044012213  0.1022959122  -1.998             0.045702 *  
## education    0.1009422741  0.0293429371   3.440             0.000582 ***
## income      -0.0000010425  0.0000008939  -1.166             0.243485    
## ideol       -0.7900118618  0.0376321895 -20.993 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 3114.5  on 2281  degrees of freedom
## Residual deviance: 2404.0  on 2276  degrees of freedom
## AIC: 2416
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(logit1, logit2, <span class="dt">test =</span> <span class="st">&quot;Chisq&quot;</span>)</code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: glbcc ~ age + gender + education + income
## Model 2: glbcc ~ age + gender + education + income + ideol
##   Resid. Df Resid. Dev Df Deviance              Pr(&gt;Chi)    
## 1      2277     3047.4                                      
## 2      2276     2404.0  1   643.45 &lt; 0.00000000000000022 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As we can see, adding ideology significantly improves the model.</p>
</div>
<div id="pseudo-r2" class="section level4 unnumbered">
<h4>Pseudo <span class="math inline">\(R^2\)</span></h4>
<p>A measure that is equivalent to the <span class="math inline">\(R^2\)</span> in OLS does not exist for logit. Remember that explaining variance in <span class="math inline">\(Y\)</span> is not the goal of MLE. However, a ``pseudo’’ <span class="math inline">\(R^2\)</span> measure exists that compares the residual deviance of the null model with that of the full model. Like the <span class="math inline">\(R^2\)</span> measure, pseudo <span class="math inline">\(R^2\)</span> ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> with values closer to <span class="math inline">\(1\)</span> indicating improved model fit.</p>
<p>Deviance is analogous to the residual sum of squares for a linear model. It is expressed as:</p>
<span class="math display" id="eq:16-7">\[\begin{equation}
  \text{deviance} = -2(\text{log}_e L)
  \tag{16.7}
\end{equation}\]</span>
<p>It is simply the log-likelihood of the model multiplied by a <span class="math inline">\(-2\)</span>. The pseudo <span class="math inline">\(R^2\)</span> is <span class="math inline">\(1\)</span> minus the ratio of the deviance of the full model <span class="math inline">\(L_1\)</span> to the deviance of the null model <span class="math inline">\(L_0\)</span>:</p>
<span class="math display" id="eq:16-8">\[\begin{equation}
  \text{pseudo} R^2 = 1-\frac{-2(\text{log}_e L_1)}{-2(\text{log}_e L_0)} 
  \tag{16.8}
\end{equation}\]</span>
<p>This can be calculated in ‘R’ using the full model with ideology.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pseudoR2 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(logit2<span class="op">$</span>deviance<span class="op">/</span>logit2<span class="op">$</span>null.deviance)
pseudoR2</code></pre></div>
<pre><code>## [1] 0.2281165</code></pre>
<p>The pseudo <span class="math inline">\(R^2\)</span> of the model is . Note that the psuedo <span class="math inline">\(R^2\)</span> is only an approximation of explained variance, and should be used in combination with other measures of fit such as AIC.</p>
</div>
<div id="akaike-information-criteria" class="section level4 unnumbered">
<h4>Akaike Information Criteria</h4>
<p>Another way to examine goodness-of-fit is the Akaike information criteria (AIC). Like the adjusted <span class="math inline">\(R^2\)</span> for OLS, the AIC takes into account the parsimony of the model by penalizing for the number of parameters. But AIC is useful only in a comparative manner – either with the null model or an alternative model. It does not purport to describe the percent of variance in <span class="math inline">\(Y\)</span> accounted for, as does the pseudo <span class="math inline">\(R^2\)</span>.</p>
<p>AIC is defined as -2 times the residual deviance of the model plus two times the number of parameters, or <span class="math inline">\(k\)</span> IVs plus the intercept:</p>
<span class="math display" id="eq:16-9">\[\begin{equation}
  \text{AIC} = -2(\text{log}_e L) + 2(k+1) 
  \tag{16.9}
\end{equation}\]</span>
<p>Note that smaller values are indicative of a better fit. The AIC is most useful when comparing the fit of alternative (not necessarily nested) models. In <code>R</code>, AIC is given as part of the <code>summary</code> output for a <code>glm</code> object, but we can also calculate it and verify.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">aic.logit2 &lt;-<span class="st"> </span>logit2<span class="op">$</span>deviance <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="dv">6</span>
aic.logit2</code></pre></div>
<pre><code>## [1] 2416.002</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2<span class="op">$</span>aic</code></pre></div>
<pre><code>## [1] 2416.002</code></pre>
</div>
</div>
<div id="interpreting-logits" class="section level3">
<h3><span class="header-section-number">16.2.3</span> Interpreting Logits</h3>
<p>The logits, <span class="math inline">\(L\)</span>, are logged odds, and therefore the coefficients that are produced must be interpreted as logged odds. This means that for each unit change in ideology, the predicted logged odds of believing climate change has an anthropogenic cause decrease by . This interpretation, though mathematically straightforward, is not terribly informative. Below we discuss two ways to make the interpretation of logit analysis more intuitive.</p>
<div id="calculate-odds" class="section level4 unnumbered">
<h4>Calculate Odds</h4>
<p>Logits can be used to directly calculate odds by taking the antilog of any of the coefficients:</p>
<span class="math display">\[\begin{equation*}
  \textit{anti}\text{log} = e^B 
\end{equation*}\]</span>
<p>For example, the following retuns odds for all the IVs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">coef</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">exp</span>()</code></pre></div>
<pre><code>## (Intercept)         age      gender   education      income       ideol 
##  57.6608736   0.9957225   0.8151353   1.1062128   0.9999990   0.4538394</code></pre>
<p>Therefore, for each 1-unit increase in the ideology scale (i.e., becoming more conservative), the odds of believing that climate change is human caused decrease by .</p>
</div>
<div id="predicted-probabilities" class="section level4 unnumbered">
<h4>Predicted Probabilities</h4>
<p>The most straightforward way to interpret logits is to transform them into predicted probabilities. To calculate the effect of a particular independent variable, <span class="math inline">\(X_i\)</span>, on the probability of <span class="math inline">\(Y = 1\)</span>, set all <span class="math inline">\(X_j\)</span>’s at their means, then calculate:</p>
<span class="math display">\[\begin{equation*}
  \hat{P} = \frac{1}{1+e^{-\hat{L}}}
\end{equation*}\]</span>
<p>We can then evaluate the change in predicted probabilities that <span class="math inline">\(Y\)</span>=1 across the range of values in <span class="math inline">\(X_i\)</span>.</p>
<p>This procedure can be demonstrated in two steps. First, create a data frame holding all the variables except ideology at their mean. Second, use the <code>augment</code> function to calculate the predicted probabilities for each level of ideology. Indicate <code>type.predict = &quot;response&quot;</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)
log.data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">age =</span> <span class="kw">mean</span>(ds.temp<span class="op">$</span>age),
                       <span class="dt">gender =</span> <span class="kw">mean</span>(ds.temp<span class="op">$</span>gender),
                       <span class="dt">education =</span> <span class="kw">mean</span>(ds.temp<span class="op">$</span>education),
                       <span class="dt">income =</span> <span class="kw">mean</span>(ds.temp<span class="op">$</span>income),
                       <span class="dt">ideol =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>)
log.data &lt;-<span class="st"> </span>logit2 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">augment</span>(<span class="dt">newdata =</span> log.data, <span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>)
log.data</code></pre></div>
<pre><code>## # A tibble: 7 x 7
##     age gender education income ideol .fitted .se.fit
## * &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1  60.1  0.412      5.09 70627.     1   0.967 0.00523
## 2  60.1  0.412      5.09 70627.     2   0.929 0.00833
## 3  60.1  0.412      5.09 70627.     3   0.856 0.0115 
## 4  60.1  0.412      5.09 70627.     4   0.730 0.0127 
## 5  60.1  0.412      5.09 70627.     5   0.551 0.0124 
## 6  60.1  0.412      5.09 70627.     6   0.357 0.0139 
## 7  60.1  0.412      5.09 70627.     7   0.202 0.0141</code></pre>
<p>The output shows, for each case, the ideology measure for the respondent followed by the estimated probability (<span class="math inline">\(p\)</span>) that the individual believes man-made greenhouse gasses are causing climate change. We can also graph the results with <span class="math inline">\(95\%\)</span> confidence intervals. This is shown in Figure <a href="16-2-logit-estimation.html#fig:logitplot">16.3</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log.df &lt;-<span class="st"> </span>log.data <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">upper =</span> .fitted <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit,
         <span class="dt">lower =</span> .fitted <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>.se.fit)

<span class="kw">ggplot</span>(log.df, <span class="kw">aes</span>(ideol, .fitted)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> lower, <span class="dt">ymax =</span> upper, <span class="dt">width =</span> .<span class="dv">2</span>)) </code></pre></div>
<div class="figure"><span id="fig:logitplot"></span>
<img src="_main_files/figure-html/logitplot-1.png" alt="Predicted Probability of believing that Greenhouse Gases cause Climate Change by Ideology" width="672" />
<p class="caption">
Figure 16.3: Predicted Probability of believing that Greenhouse Gases cause Climate Change by Ideology
</p>
</div>
<p>We can see that as respondents become more conservative, the probability of believing that climate change is man-made decreases at what appears to be an increasing rate.</p>
</div>
</div>
</div>
<p style="text-align: center;">
<a href="16-1-generalized-linear-models.html"><button class="btn btn-default">Previous</button></a>
<a href="16-3-summary-11.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
