<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="3.1 Characterizing Data | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R" />
<meta property="og:type" content="book" />





<meta name="author" content="Hank Jenkins-Smith" />
<meta name="author" content="Joseph Ripberger" />
<meta name="author" content="Gary Copeland" />
<meta name="author" content="Matthew Nowlin" />
<meta name="author" content="Tyler Hughes" />
<meta name="author" content="Aaron Fister" />
<meta name="author" content="Wesley Wehde" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="3.1 Characterizing Data | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R">

<title>3.1 Characterizing Data | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 3rd Edition With Applications in R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#preface-and-acknowledgments">Preface and Acknowledgments</a><ul>
<li><a href="copyright.html#copyright">Copyright</a></li>
</ul></li>
<li class="has-sub"><a href="1-theories-and-social-science.html#theories-and-social-science"><span class="toc-section-number">1</span> Theories and Social Science</a><ul>
<li><a href="1-1-the-scientific-method.html#the-scientific-method"><span class="toc-section-number">1.1</span> The Scientific Method</a></li>
<li class="has-sub"><a href="1-2-theory-and-empirical-research.html#theory-and-empirical-research"><span class="toc-section-number">1.2</span> Theory and Empirical Research</a><ul>
<li><a href="1-2-theory-and-empirical-research.html#coherent-and-internally-consistent"><span class="toc-section-number">1.2.1</span> Coherent and Internally Consistent</a></li>
<li><a href="1-2-theory-and-empirical-research.html#theories-and-causality"><span class="toc-section-number">1.2.2</span> Theories and Causality</a></li>
<li><a href="1-2-theory-and-empirical-research.html#generation-of-testable-hypothesis"><span class="toc-section-number">1.2.3</span> Generation of Testable Hypothesis</a></li>
</ul></li>
<li><a href="1-3-theory-and-functions.html#theory-and-functions"><span class="toc-section-number">1.3</span> Theory and Functions</a></li>
<li><a href="1-4-theory-in-social-science.html#theory-in-social-science"><span class="toc-section-number">1.4</span> Theory in Social Science</a></li>
<li><a href="1-5-outline-of-the-book.html#outline-of-the-book"><span class="toc-section-number">1.5</span> Outline of the Book</a></li>
</ul></li>
<li class="has-sub"><a href="2-research-design.html#research-design"><span class="toc-section-number">2</span> Research Design</a><ul>
<li><a href="2-1-overview-of-the-research-process.html#overview-of-the-research-process"><span class="toc-section-number">2.1</span> Overview of the Research Process</a></li>
<li><a href="2-2-internal-and-external-validity.html#internal-and-external-validity"><span class="toc-section-number">2.2</span> Internal and External Validity</a></li>
<li><a href="2-3-major-classes-of-designs.html#major-classes-of-designs"><span class="toc-section-number">2.3</span> Major Classes of Designs</a></li>
<li><a href="2-4-threats-to-validity.html#threats-to-validity"><span class="toc-section-number">2.4</span> Threats to Validity</a></li>
<li><a href="2-5-some-common-designs.html#some-common-designs"><span class="toc-section-number">2.5</span> Some Common Designs</a></li>
<li><a href="2-6-plan-meets-reality.html#plan-meets-reality"><span class="toc-section-number">2.6</span> Plan Meets Reality</a></li>
</ul></li>
<li class="has-sub"><a href="3-exploring-and-visualizing-data.html#exploring-and-visualizing-data"><span class="toc-section-number">3</span> Exploring and Visualizing Data</a><ul>
<li class="has-sub"><a href="3-1-characterizing-data.html#characterizing-data"><span class="toc-section-number">3.1</span> Characterizing Data</a><ul>
<li><a href="3-1-characterizing-data.html#central-tendency"><span class="toc-section-number">3.1.1</span> Central Tendency</a></li>
<li><a href="3-1-characterizing-data.html#level-of-measurement-and-central-tendency"><span class="toc-section-number">3.1.2</span> Level of Measurement and Central Tendency</a></li>
<li><a href="3-1-characterizing-data.html#moments"><span class="toc-section-number">3.1.3</span> Moments</a></li>
<li><a href="3-1-characterizing-data.html#first-moment-expected-value"><span class="toc-section-number">3.1.4</span> First Moment – Expected Value</a></li>
<li><a href="3-1-characterizing-data.html#the-second-moment-variance-and-standard-deviation"><span class="toc-section-number">3.1.5</span> The Second Moment – Variance and Standard Deviation</a></li>
<li><a href="3-1-characterizing-data.html#the-third-moment-skewness"><span class="toc-section-number">3.1.6</span> The Third Moment – Skewness</a></li>
<li><a href="3-1-characterizing-data.html#the-fourth-moment-kurtosis"><span class="toc-section-number">3.1.7</span> The Fourth Moment – Kurtosis</a></li>
<li><a href="3-1-characterizing-data.html#order-statistics"><span class="toc-section-number">3.1.8</span> Order Statistics</a></li>
</ul></li>
<li><a href="3-2-summary.html#summary"><span class="toc-section-number">3.2</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="4-probability.html#probability"><span class="toc-section-number">4</span> Probability</a><ul>
<li><a href="4-1-finding-probabilities.html#finding-probabilities"><span class="toc-section-number">4.1</span> Finding Probabilities</a></li>
<li><a href="4-2-finding-probabilities-with-the-normal-curve.html#finding-probabilities-with-the-normal-curve"><span class="toc-section-number">4.2</span> Finding Probabilities with the Normal Curve</a></li>
<li><a href="4-3-summary-1.html#summary-1"><span class="toc-section-number">4.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="5-inference.html#inference"><span class="toc-section-number">5</span> Inference</a><ul>
<li class="has-sub"><a href="5-1-inference-populations-and-samples.html#inference-populations-and-samples"><span class="toc-section-number">5.1</span> Inference: Populations and Samples</a><ul>
<li><a href="5-1-inference-populations-and-samples.html#populations-and-samples"><span class="toc-section-number">5.1.1</span> Populations and Samples</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-and-knowing"><span class="toc-section-number">5.1.2</span> Sampling and Knowing</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-strategies"><span class="toc-section-number">5.1.3</span> Sampling Strategies</a></li>
<li><a href="5-1-inference-populations-and-samples.html#sampling-techniques"><span class="toc-section-number">5.1.4</span> Sampling Techniques</a></li>
<li><a href="5-1-inference-populations-and-samples.html#so-how-is-it-that-we-know"><span class="toc-section-number">5.1.5</span> So How is it That We Know?</a></li>
</ul></li>
<li class="has-sub"><a href="5-2-the-normal-distribution.html#the-normal-distribution"><span class="toc-section-number">5.2</span> The Normal Distribution</a><ul>
<li><a href="5-2-the-normal-distribution.html#standardizing-a-normal-distribution-and-z-scores"><span class="toc-section-number">5.2.1</span> Standardizing a Normal Distribution and Z-scores</a></li>
<li><a href="5-2-the-normal-distribution.html#the-central-limit-theorem"><span class="toc-section-number">5.2.2</span> The Central Limit Theorem</a></li>
<li><a href="5-2-the-normal-distribution.html#populations-samples-and-symbols"><span class="toc-section-number">5.2.3</span> Populations, Samples and Symbols</a></li>
</ul></li>
<li class="has-sub"><a href="5-3-inferences-to-the-population-from-the-sample.html#inferences-to-the-population-from-the-sample"><span class="toc-section-number">5.3</span> Inferences to the Population from the Sample</a><ul>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#confidence-intervals"><span class="toc-section-number">5.3.1</span> Confidence Intervals</a></li>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#the-logic-of-hypothesis-testing"><span class="toc-section-number">5.3.2</span> The Logic of Hypothesis Testing</a></li>
<li><a href="5-3-inferences-to-the-population-from-the-sample.html#some-miscellaneous-notes-about-hypothesis-testing"><span class="toc-section-number">5.3.3</span> Some Miscellaneous Notes about Hypothesis Testing</a></li>
</ul></li>
<li class="has-sub"><a href="5-4-differences-between-groups.html#differences-between-groups"><span class="toc-section-number">5.4</span> Differences Between Groups</a><ul>
<li><a href="5-4-differences-between-groups.html#t-tests"><span class="toc-section-number">5.4.1</span> <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li><a href="5-5-summary-2.html#summary-2"><span class="toc-section-number">5.5</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="6-association-of-variables.html#association-of-variables"><span class="toc-section-number">6</span> Association of Variables</a><ul>
<li class="has-sub"><a href="6-1-cross-tabulation.html#cross-tabulation"><span class="toc-section-number">6.1</span> Cross-Tabulation</a><ul>
<li><a href="6-1-cross-tabulation.html#crosstabulation-and-control"><span class="toc-section-number">6.1.1</span> Crosstabulation and Control</a></li>
</ul></li>
<li><a href="6-2-covariance.html#covariance"><span class="toc-section-number">6.2</span> Covariance</a></li>
<li><a href="6-3-correlation.html#correlation"><span class="toc-section-number">6.3</span> Correlation</a></li>
<li><a href="6-4-scatterplots.html#scatterplots"><span class="toc-section-number">6.4</span> Scatterplots</a></li>
</ul></li>
<li class="has-sub"><a href="7-the-logic-of-ordinary-least-squares-estimation.html#the-logic-of-ordinary-least-squares-estimation"><span class="toc-section-number">7</span> The Logic of Ordinary Least Squares Estimation</a><ul>
<li class="has-sub"><a href="7-1-theoretical-models.html#theoretical-models"><span class="toc-section-number">7.1</span> Theoretical Models</a><ul>
<li><a href="7-1-theoretical-models.html#deterministic-linear-model"><span class="toc-section-number">7.1.1</span> Deterministic Linear Model</a></li>
<li><a href="7-1-theoretical-models.html#stochastic-linear-model"><span class="toc-section-number">7.1.2</span> Stochastic Linear Model</a></li>
<li><a href="7-1-theoretical-models.html#assumptions-about-the-error-term"><span class="toc-section-number">7.1.3</span> Assumptions about the Error Term</a></li>
</ul></li>
<li class="has-sub"><a href="7-2-estimating-linear-models.html#estimating-linear-models"><span class="toc-section-number">7.2</span> Estimating Linear Models</a><ul>
<li><a href="7-2-estimating-linear-models.html#residuals"><span class="toc-section-number">7.2.1</span> Residuals</a></li>
</ul></li>
<li><a href="7-3-an-example-of-simple-regression.html#an-example-of-simple-regression"><span class="toc-section-number">7.3</span> An Example of Simple Regression</a></li>
</ul></li>
<li class="has-sub"><a href="8-linear-estimation-and-minimizing-error.html#linear-estimation-and-minimizing-error"><span class="toc-section-number">8</span> Linear Estimation and Minimizing Error</a><ul>
<li class="has-sub"><a href="8-1-minimizing-error-using-derivatives.html#minimizing-error-using-derivatives"><span class="toc-section-number">8.1</span> Minimizing Error using Derivatives</a><ul>
<li><a href="8-1-minimizing-error-using-derivatives.html#rules-of-derivation"><span class="toc-section-number">8.1.1</span> Rules of Derivation</a></li>
<li><a href="8-1-minimizing-error-using-derivatives.html#critical-points"><span class="toc-section-number">8.1.2</span> Critical Points</a></li>
<li><a href="8-1-minimizing-error-using-derivatives.html#partial-derivation"><span class="toc-section-number">8.1.3</span> Partial Derivation</a></li>
</ul></li>
<li class="has-sub"><a href="8-2-deriving-ols-estimators.html#deriving-ols-estimators"><span class="toc-section-number">8.2</span> Deriving OLS Estimators</a><ul>
<li><a href="8-2-deriving-ols-estimators.html#ols-derivation-of-hatalpha"><span class="toc-section-number">8.2.1</span> OLS Derivation of <span class="math inline">\(\hat{\alpha}\)</span></a></li>
<li><a href="8-2-deriving-ols-estimators.html#ols-derivation-of-hatbeta"><span class="toc-section-number">8.2.2</span> OLS Derivation of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li><a href="8-2-deriving-ols-estimators.html#interpreting-hatbeta-and-hatalpha"><span class="toc-section-number">8.2.3</span> Interpreting <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\alpha}\)</span></a></li>
</ul></li>
<li><a href="8-3-summary-3.html#summary-3"><span class="toc-section-number">8.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="9-bi-variate-hypothesis-testing-and-model-fit.html#bi-variate-hypothesis-testing-and-model-fit"><span class="toc-section-number">9</span> Bi-Variate Hypothesis Testing and Model Fit</a><ul>
<li class="has-sub"><a href="9-1-hypothesis-tests-for-regression-coefficients.html#hypothesis-tests-for-regression-coefficients"><span class="toc-section-number">9.1</span> Hypothesis Tests for Regression Coefficients</a><ul>
<li><a href="9-1-hypothesis-tests-for-regression-coefficients.html#residual-standard-error"><span class="toc-section-number">9.1.1</span> Residual Standard Error</a></li>
</ul></li>
<li class="has-sub"><a href="9-2-measuring-goodness-of-fit.html#measuring-goodness-of-fit"><span class="toc-section-number">9.2</span> Measuring Goodness of Fit</a><ul>
<li><a href="9-2-measuring-goodness-of-fit.html#sample-covariance-and-correlations"><span class="toc-section-number">9.2.1</span> Sample Covariance and Correlations</a></li>
<li><a href="9-2-measuring-goodness-of-fit.html#coefficient-of-determination-r2"><span class="toc-section-number">9.2.2</span> Coefficient of Determination: <span class="math inline">\(R^{2}\)</span></a></li>
<li><a href="9-2-measuring-goodness-of-fit.html#visualizing-bivariate-regression"><span class="toc-section-number">9.2.3</span> Visualizing Bivariate Regression</a></li>
</ul></li>
<li><a href="9-3-summary-4.html#summary-4"><span class="toc-section-number">9.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="10-ols-assumptions-and-simple-regression-diagnostics.html#ols-assumptions-and-simple-regression-diagnostics"><span class="toc-section-number">10</span> OLS Assumptions and Simple Regression Diagnostics</a><ul>
<li><a href="10-1-a-recap-of-modeling-assumptions.html#a-recap-of-modeling-assumptions"><span class="toc-section-number">10.1</span> A Recap of Modeling Assumptions</a></li>
<li class="has-sub"><a href="10-2-when-things-go-bad-with-residuals.html#when-things-go-bad-with-residuals"><span class="toc-section-number">10.2</span> When Things Go Bad with Residuals</a><ul>
<li><a href="10-2-when-things-go-bad-with-residuals.html#outlier-data"><span class="toc-section-number">10.2.1</span> “Outlier” Data</a></li>
<li><a href="10-2-when-things-go-bad-with-residuals.html#non-constant-variance"><span class="toc-section-number">10.2.2</span> Non-Constant Variance</a></li>
<li><a href="10-2-when-things-go-bad-with-residuals.html#non-linearity-in-the-parameters"><span class="toc-section-number">10.2.3</span> Non-Linearity in the Parameters</a></li>
</ul></li>
<li class="has-sub"><a href="10-3-application-of-residual-diagnostics.html#application-of-residual-diagnostics"><span class="toc-section-number">10.3</span> Application of Residual Diagnostics</a><ul>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-non-linearity"><span class="toc-section-number">10.3.1</span> Testing for Non-Linearity</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-normality-in-model-residuals"><span class="toc-section-number">10.3.2</span> Testing for Normality in Model Residuals</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#testing-for-non-constant-variance-in-the-residuals"><span class="toc-section-number">10.3.3</span> Testing for Non-Constant Variance in the Residuals</a></li>
<li><a href="10-3-application-of-residual-diagnostics.html#examining-outlier-data"><span class="toc-section-number">10.3.4</span> Examining Outlier Data</a></li>
</ul></li>
<li><a href="10-4-so-now-what-implications-of-residual-analysis.html#so-now-what-implications-of-residual-analysis"><span class="toc-section-number">10.4</span> So Now What? Implications of Residual Analysis</a></li>
<li><a href="10-5-summary-5.html#summary-5"><span class="toc-section-number">10.5</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="11-introduction-to-multiple-regression.html#introduction-to-multiple-regression"><span class="toc-section-number">11</span> Introduction to Multiple Regression</a><ul>
<li><a href="11-1-matrix-algebra-and-multiple-regression.html#matrix-algebra-and-multiple-regression"><span class="toc-section-number">11.1</span> Matrix Algebra and Multiple Regression</a></li>
<li class="has-sub"><a href="11-2-the-basics-of-matrix-algebra.html#the-basics-of-matrix-algebra"><span class="toc-section-number">11.2</span> The Basics of Matrix Algebra</a><ul>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-basics"><span class="toc-section-number">11.2.1</span> Matrix Basics</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#vectors"><span class="toc-section-number">11.2.2</span> Vectors</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-operations"><span class="toc-section-number">11.2.3</span> Matrix Operations</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#transpose"><span class="toc-section-number">11.2.4</span> Transpose</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#adding-matrices"><span class="toc-section-number">11.2.5</span> Adding Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#multiplication-of-matrices"><span class="toc-section-number">11.2.6</span> Multiplication of Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#identity-matrices"><span class="toc-section-number">11.2.7</span> Identity Matrices</a></li>
<li><a href="11-2-the-basics-of-matrix-algebra.html#matrix-inversion"><span class="toc-section-number">11.2.8</span> Matrix Inversion</a></li>
</ul></li>
<li><a href="11-3-ols-regression-in-matrix-form.html#ols-regression-in-matrix-form"><span class="toc-section-number">11.3</span> OLS Regression in Matrix Form</a></li>
<li><a href="11-4-summary-6.html#summary-6"><span class="toc-section-number">11.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="12-the-logic-of-multiple-regression.html#the-logic-of-multiple-regression"><span class="toc-section-number">12</span> The Logic of Multiple Regression</a><ul>
<li class="has-sub"><a href="12-1-theoretical-specification.html#theoretical-specification"><span class="toc-section-number">12.1</span> Theoretical Specification</a><ul>
<li><a href="12-1-theoretical-specification.html#assumptions-of-ols-regression"><span class="toc-section-number">12.1.1</span> Assumptions of OLS Regression</a></li>
</ul></li>
<li><a href="12-2-partial-effects.html#partial-effects"><span class="toc-section-number">12.2</span> Partial Effects</a></li>
<li class="has-sub"><a href="12-3-multiple-regression-example.html#multiple-regression-example"><span class="toc-section-number">12.3</span> Multiple Regression Example</a><ul>
<li><a href="12-3-multiple-regression-example.html#hypothesis-testing-and-t-tests"><span class="toc-section-number">12.3.1</span> Hypothesis Testing and <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li><a href="12-4-summary-7.html#summary-7"><span class="toc-section-number">12.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="13-multiple-regression-and-model-building.html#multiple-regression-and-model-building"><span class="toc-section-number">13</span> Multiple Regression and Model Building</a><ul>
<li class="has-sub"><a href="13-1-model-building.html#model-building"><span class="toc-section-number">13.1</span> Model Building</a><ul>
<li><a href="13-1-model-building.html#theory-and-hypotheses"><span class="toc-section-number">13.1.1</span> Theory and Hypotheses</a></li>
<li><a href="13-1-model-building.html#empirical-indicators"><span class="toc-section-number">13.1.2</span> Empirical Indicators</a></li>
<li><a href="13-1-model-building.html#risks-in-model-building"><span class="toc-section-number">13.1.3</span> Risks in Model Building</a></li>
</ul></li>
<li><a href="13-2-evils-of-stepwise-regression.html#evils-of-stepwise-regression"><span class="toc-section-number">13.2</span> Evils of Stepwise Regression</a></li>
<li><a href="13-3-summary-8.html#summary-8"><span class="toc-section-number">13.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="14-topics-in-multiple-regression.html#topics-in-multiple-regression"><span class="toc-section-number">14</span> Topics in Multiple Regression</a><ul>
<li><a href="14-1-dummy-variables.html#dummy-variables"><span class="toc-section-number">14.1</span> Dummy Variables</a></li>
<li><a href="14-2-interaction-effects.html#interaction-effects"><span class="toc-section-number">14.2</span> Interaction Effects</a></li>
<li><a href="14-3-standardized-regression-coefficients.html#standardized-regression-coefficients"><span class="toc-section-number">14.3</span> Standardized Regression Coefficients</a></li>
<li><a href="14-4-summary-9.html#summary-9"><span class="toc-section-number">14.4</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="15-the-art-of-regression-diagnostics.html#the-art-of-regression-diagnostics"><span class="toc-section-number">15</span> The Art of Regression Diagnostics</a><ul>
<li><a href="15-1-ols-error-assumptions-revisited.html#ols-error-assumptions-revisited"><span class="toc-section-number">15.1</span> OLS Error Assumptions Revisited</a></li>
<li class="has-sub"><a href="15-2-ols-diagnostic-techniques.html#ols-diagnostic-techniques"><span class="toc-section-number">15.2</span> OLS Diagnostic Techniques</a><ul>
<li><a href="15-2-ols-diagnostic-techniques.html#non-linearity"><span class="toc-section-number">15.2.1</span> Non-Linearity</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#non-constant-variance-or-heteroscedasticity"><span class="toc-section-number">15.2.2</span> Non-Constant Variance, or Heteroscedasticity</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#independence-of-e"><span class="toc-section-number">15.2.3</span> Independence of <span class="math inline">\(E\)</span></a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#normality-of-the-residuals"><span class="toc-section-number">15.2.4</span> Normality of the Residuals</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#outliers-leverage-and-influence"><span class="toc-section-number">15.2.5</span> Outliers, Leverage, and Influence</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#outliers"><span class="toc-section-number">15.2.6</span> Outliers</a></li>
<li><a href="15-2-ols-diagnostic-techniques.html#multicollinearity"><span class="toc-section-number">15.2.7</span> Multicollinearity</a></li>
</ul></li>
<li><a href="15-3-summary-10.html#summary-10"><span class="toc-section-number">15.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="16-logit-regression.html#logit-regression"><span class="toc-section-number">16</span> Logit Regression</a><ul>
<li><a href="16-1-generalized-linear-models.html#generalized-linear-models"><span class="toc-section-number">16.1</span> Generalized Linear Models</a></li>
<li class="has-sub"><a href="16-2-logit-estimation.html#logit-estimation"><span class="toc-section-number">16.2</span> Logit Estimation</a><ul>
<li><a href="16-2-logit-estimation.html#logit-hypothesis-tests"><span class="toc-section-number">16.2.1</span> Logit Hypothesis Tests</a></li>
<li><a href="16-2-logit-estimation.html#goodness-of-fit"><span class="toc-section-number">16.2.2</span> Goodness of Fit</a></li>
<li><a href="16-2-logit-estimation.html#interpreting-logits"><span class="toc-section-number">16.2.3</span> Interpreting Logits</a></li>
</ul></li>
<li><a href="16-3-summary-11.html#summary-11"><span class="toc-section-number">16.3</span> Summary</a></li>
</ul></li>
<li class="has-sub"><a href="17-appendix-basic-r.html#appendix-basic-r"><span class="toc-section-number">17</span> Appendix: Basic R</a><ul>
<li><a href="17-1-introduction-to-r.html#introduction-to-r"><span class="toc-section-number">17.1</span> Introduction to R</a></li>
<li><a href="17-2-downloading-r-and-rstudio.html#downloading-r-and-rstudio"><span class="toc-section-number">17.2</span> Downloading R and RStudio</a></li>
<li><a href="17-3-introduction-to-programming.html#introduction-to-programming"><span class="toc-section-number">17.3</span> Introduction to Programming</a></li>
<li><a href="17-4-uploadingreading-data.html#uploadingreading-data"><span class="toc-section-number">17.4</span> Uploading/Reading Data</a></li>
<li><a href="17-5-data-manipulation-in-r.html#data-manipulation-in-r"><span class="toc-section-number">17.5</span> Data Manipulation in R</a></li>
<li><a href="17-6-savingwriting-data.html#savingwriting-data"><span class="toc-section-number">17.6</span> Saving/Writing Data</a></li>
<li><a href="17-7-the-tidyverse.html#the-tidyverse"><span class="toc-section-number">17.7</span> The Tidyverse</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="characterizing-data" class="section level2">
<h2><span class="header-section-number">3.1</span> Characterizing Data</h2>
<p>What does it mean to characterize your data? First, it means knowing how many observations are contained in your data and the distribution of those observations over the range of your variable(s). What kinds of measures (interval, ordinal, nominal) do you have, and what are the ranges of valid measures for each variable? How many cases of missing (no data) or mis-coded (measures that fall outside the valid range) do you have? What do the coded values represent? While seemingly trivial, checking and evaluating your data for these attributes can save you major headaches later. For example, missing values for an observation often get a special code – say, ``-99&quot; – to distinguish them from valid observations. If you neglect to treat these values properly, R (or any other statistics program) will treat that value as if it were valid and thereby turn your results into a royal hairball. We know of cases in which even seasoned quantitative scholars have made the embarrassing mistake of failing to properly handle missing values in their analyses. In at least one case, a published paper had to be retracted for this reason. So don’t skimp on the most basic forms of data characterization!</p>
<p>The dataset used for purposes of illustration in this version of this text is taken from a survey of Oklahomans, conducted in 2016, by OU’s Center for Risk and Crisis Management. The survey question wording and background will be provided in class. However, for purposes of this chapter, note that the measure of <strong>ideology</strong> consists of a self-report of political ideology on a scale that ranges from 1 (strong liberal) to 7 (strong conservative); the measure of the <strong>perceived risk of climate change</strong> ranges from zero (no risk) to 10 (extreme risk). <strong>Age</strong> was measured in years.</p>
<p>It is often useful to graph the variables in your dataset to get a better idea of their distribution. In addition, we may want to compare the distribution of a variable to a theoretical distribution (typically a normal distribution). This can be accomplished in several ways, but we will show two here—a histogram and a density curve—and more will be discussed in later chapters. For now we examine the distribution of the variable measuring age. The red line on the density visualization presents the normal distribution given the mean and standard deviation of our variable.</p>
<p>% Omitted: A histogram creates intervals of equal length, called bins, and displays the frequency of observations in each of the bins. To produce a histogram in R simply use the geom_histogram command in the <code>ggplot2</code> package. Next, we plot the density of the observed data along with a normal curve. This can be done with the <code>geom_density</code> command in the <code>ggplot2</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(age)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>() </code></pre></div>
<div class="figure"><span id="fig:hist"></span>
<img src="_main_files/figure-html/hist-1.png" alt="Histogram" width="672" />
<p class="caption">
Figure 3.1: Histogram
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(age)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span>
<span class="st">   </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(ds<span class="op">$</span>age, <span class="dt">na.rm =</span> T), 
                                            <span class="dt">sd =</span> <span class="kw">sd</span>(ds<span class="op">$</span>age, <span class="dt">na.rm =</span> T)), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) </code></pre></div>
<div class="figure"><span id="fig:dens"></span>
<img src="_main_files/figure-html/dens-1.png" alt="Density Curve" width="672" />
<p class="caption">
Figure 3.2: Density Curve
</p>
</div>
<p>You can also get an overview of your data using a table known as a frequency distribution. The frequency distribution summarizes how often each value of your variable occurs in the dataset. If your variable has a limited number of values that it can take on, you can report all values, but if it has a large number of possible values (e.g., age of respondent), then you will want to create categories, or bins, to report those frequencies. In such cases, it is generally easier to make sense of the percentage distribution. Table <a href="3-1-characterizing-data.html#fig:ideo">3.3</a> is a frequency distribution for the ideology variable. From that table we see, for example, that about one-third of all respondents are moderates. We see the numbers decrease as we move away from that category, but not uniformly. There are a few more people on the conservative extreme than on the liberal side and that the number of people placing themselves in the penultimate categories on either end is greater than those towards the middle. The histogram and density curve would, of course, show the same pattern.</p>
<p>The other thing to watch for here (or in the charts) is whether there is an unusual observation. If one person scored 17 in this table, you could be pretty sure a coding error was made somewhere. You cannot find all your errors this way, but you can find some, including the ones that have the potential to most seriously adversely affect your analysis.</p>
<div class="figure"><span id="fig:ideo"></span>
<img src="freq-ideo.png" alt="Frequency Distribbution for Ideology" width="252" />
<p class="caption">
Figure 3.3: Frequency Distribbution for Ideology
</p>
</div>
<p>In R, we can obtain the data for the above table with the following functions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># frequency counts for each level</span>
<span class="kw">table</span>(ds<span class="op">$</span>ideol)</code></pre></div>
<pre><code>## 
##   1   2   3   4   5   6   7 
## 122 279 185 571 328 688 351</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># To view percentages</span>
<span class="kw">library</span>(dplyr)
<span class="kw">table</span>(ds<span class="op">$</span>ideol) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">prop.table</span>()</code></pre></div>
<pre><code>## 
##          1          2          3          4          5          6 
## 0.04833597 0.11053883 0.07329635 0.22622821 0.12995246 0.27258320 
##          7 
## 0.13906498</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># multiply the numbers by 100 </span>
<span class="kw">table</span>(ds<span class="op">$</span>ideol) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">prop.table</span>() <span class="op">*</span><span class="st"> </span><span class="dv">100</span> </code></pre></div>
<pre><code>## 
##         1         2         3         4         5         6         7 
##  4.833597 11.053883  7.329635 22.622821 12.995246 27.258320 13.906498</code></pre>
<p>Having obtained a sample, it is important to be able to characterize that sample. In particular, it is important to understand the probability distributions associated with each variable in the sample.</p>
<div id="central-tendency" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Central Tendency</h3>
<p>Measures of central tendency are useful because a single statistic can be used to describe the distribution. We focus on three measures of central tendency: the mean, the median, and the mode.</p>
<blockquote>
<p><strong>Measures of Central Tendency</strong></p>
<p>The Mean: The arithmetic average of the values</p>
<p>The Median: The value at the center of the distribution</p>
<p>The Mode: The most frequently occurring value</p>
</blockquote>
<p>We will primarily rely on the mean, because of its efficient property of representing the data. But medians – particularly when used in conjunction with the mean - can tell us a great deal about the shape of the distribution of our data. We will return to this point shortly.</p>
</div>
<div id="level-of-measurement-and-central-tendency" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Level of Measurement and Central Tendency</h3>
<p>The three measures of central tendency – the mean, median, and mode – each tell us something different about our data, but each has some limitations as well (especially when used alone). Knowing the mode tells us what is most common, but we do not know how common and, using it alone, would not even leave us confident that it is an indicator of anything very <em>central</em>. When rolling in your data, it is generally a good idea to roll in all the descriptive statistics that you can to get a good feel for them.</p>
<p>One issue, though, is that your ability to use any statistic is dependent on the level of measurement for the variable. The mean requires you to add all your observations together. But you cannot perform mathematical functions on ordinal or nominal level measures. Your data must be measured at the interval level to calculate a meaningful mean. (If you ask R to calculate the mean student id number, it will, but what you get will be nonsense.) Finding the middle item in an order listing of your observations (the median) requires the ability to order your data, so your level of measurement must be at least ordinal. Therefore, if you have nominal level data, you can only report the mode (but no median or mean), so it is critical that you also look beyond central tendency to the overall distribution of the data.</p>
</div>
<div id="moments" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Moments</h3>
<p>In addition to measures of central tendency, ``moments&quot; are important ways to characterize the shape of the distribution of a sample variable. Moments are applicable when the data measured is interval type (the level of measurement). The first four moments are those that are used most often.</p>
<div id="the-first-four-moments" class="section level4 unnumbered">
<h4>The First Four Moments</h4>
<ol style="list-style-type: decimal">
<li><em>Expected Value</em>: The expected value of a variable, <span class="math inline">\(E(X)\)</span> is its mean.</li>
</ol>
<p><span class="math inline">\(E(X) = \bar{X}=\frac{\sum X_{i}}{n}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li><em>Variance</em>: The variance of a variable concerns the way that the observed values are spread around either side of the mean.</li>
</ol>
<p><span class="math inline">\(s^{2}_{x}=\frac{\sum (X-\bar{X})^{2}}{(n-1)}\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li><p><em>Skewness</em>: The skewness of a variable is a measure of its asymmetry.</p>
<p><span class="math inline">\(S = \frac{\sum (X-\bar{X})^{3}}{(n-1)}\)</span></p></li>
<li><p><em>Kurtosis</em>: The kurtosis of a variable is a measure of its peakedness.</p>
<p><span class="math inline">\(K = \frac{\sum (X-\bar{X})^{4}}{(n-1)}\)</span></p></li>
</ol>
</div>
</div>
<div id="first-moment-expected-value" class="section level3">
<h3><span class="header-section-number">3.1.4</span> First Moment – Expected Value</h3>
The <em>expected value</em> of a variable is the value you would obtain if you could multiply all possible values within a population by their probability of occurrence. Alternatively, it can be understood as the mean value for a population variable. An expected value is a theoretical number , because we usually cannot observe all possible occurrences of a variable. The mean value for a sample is the average value for the variable <span class="math inline">\(X\)</span>, and is calculated by adding the values of <span class="math inline">\(X\)</span> and dividing by the sample size <span class="math inline">\(n\)</span>:<br />

<span class="math display" id="eq:03-1">\[\begin{equation}
\bar{X} = \frac{(x_{1}+x_{2}+x_{3}+x_{n})}{n}
\tag{3.1}
\end{equation}\]</span>
This can be more compactly expressed as:
<span class="math display" id="eq:03-2">\[\begin{equation}
\bar{X}=\frac{\sum X_{i}}{n}
\tag{3.2}
\end{equation}\]</span>
<p>The mean of a variable can be calculated in <code>R</code> using the <code>mean</code> function. Here we illustrate the calculation of means for our measures of <code>ideology</code>, <code>age</code>, and <code>perceived risk of climate change</code>.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(ds<span class="op">$</span>ideol, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 4.652932</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(ds<span class="op">$</span>age, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 60.36749</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(ds<span class="op">$</span>glbcc_risk, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 5.945978</code></pre>
</div>
<div id="the-second-moment-variance-and-standard-deviation" class="section level3">
<h3><span class="header-section-number">3.1.5</span> The Second Moment – Variance and Standard Deviation</h3>
The <em>variance</em> of a variable is a measure that illustrates how a variable is spread, or distributed, around its mean. For samples, it is expressed as:
<span class="math display" id="eq:03-3">\[\begin{equation}
s^{2}_{x}=\frac{\sum (X-\bar{X})^{2}}{(n-1)}
\tag{3.3}
\end{equation}\]</span>
<p>The population variance is expressed as: <span class="math inline">\(\sigma^{2}_{X}\)</span>.</p>
<p>Variance is measured in <code>squared</code> deviations from the mean, and the sum of these squared variations is termed the <code>total sum of squares</code>. Why squared deviations? Why not just sum the differences? While the latter strategy would seemingly be simpler, it would always sum to zero. By squaring the deviations we make them all positive, so the sum of squares will always be a positive number.</p>
<blockquote>
<p><strong>Total Sum of Squares</strong> is the squared summed total of the variation of a variable around its mean.</p>
</blockquote>
This can be expressed as:
<span class="math display" id="eq:03-4">\[\begin{equation}
TSS_{x} = \sum(X_{i}-\bar{X})^{2}
\tag{3.4}
\end{equation}\]</span>
therefore;
<span class="math display" id="eq:03-5">\[\begin{equation}
s^{2}_{x} = \frac{TSS_{x}}{(n-1)}
\tag{3.5}
\end{equation}\]</span>
The square root of variance, <span class="math inline">\(\sigma^{2}_{x}\)</span>, is the <em>standard deviation</em> (s.d.) of a variable, <span class="math inline">\(\sigma_{x}\)</span>. The sample s.d. is expressed as:
<span class="math display" id="eq:03-6">\[\begin{equation}
s_{x} = \sqrt{\frac{\sum(X-\bar{X})^{2}}{(n-1)}}
\tag{3.6}
\end{equation}\]</span>
<p>This can also be expressed as <span class="math inline">\(\sqrt{s^2_{x}}\)</span>. The standard deviation of a variable can be obtained in <code>R</code> with the <code>sd</code> function.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(ds<span class="op">$</span>ideol, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 1.731246</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(ds<span class="op">$</span>age, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 14.20894</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(ds<span class="op">$</span>glbcc_risk, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 3.071251</code></pre>
</div>
<div id="the-third-moment-skewness" class="section level3">
<h3><span class="header-section-number">3.1.6</span> The Third Moment – Skewness</h3>
<em>Skewness</em> is a measure of the asymmetry of a distribution. It is based on the third moment and is expressed as:
<span class="math display" id="eq:03-7">\[\begin{equation}
\frac{\sum (X-\bar{X})^{3}}{(n-1)}
\tag{3.7}
\end{equation}\]</span>
Skewness is calculated by dividing the third moment by the the cube of the s.d.
<span class="math display" id="eq:03-8">\[\begin{equation}
S = \frac{\frac{\sum (X-\bar{X})^{3}}{(n-1)}}{(\sqrt{\frac{\sum
    (X-\bar{X})^{2}}{(n-1)})^{3}}}
    \tag{3.8}
\end{equation}\]</span>
<p>Specifically, skewness refers to the position of the expected value (i.e., mean) of a variable distribution relative to its median. When the mean and median of a variable are roughly equal, <span class="math inline">\(\bar{Y} \approx Md_{Y}\)</span>, then the distribution is considered approximately symmetrical, <span class="math inline">\(S = 0\)</span>. This means that an equal proportion of the distribution of the variable lies on either side of the mean. However, when the mean is larger than the median, <span class="math inline">\(\bar{Y} &gt; Md_{Y}\)</span>, then the distribution has a <em>positive</em> skew, <span class="math inline">\(S &gt; 0\)</span>. When the median is larger than the mean, <span class="math inline">\(\bar{Y} &lt; Md_{Y}\)</span>, this is a <em>negative</em> skew, <span class="math inline">\(S &lt; 0\)</span>. This is illustrated in Figure <a href="3-1-characterizing-data.html#fig:disshape">3.4</a>. Note that for a normal distribution, <span class="math inline">\(S=0\)</span>.</p>
<div class="figure"><span id="fig:disshape"></span>
<img src="distroshapes.pdf" alt="Distributional Shapes"  />
<p class="caption">
Figure 3.4: Distributional Shapes
</p>
</div>
</div>
<div id="the-fourth-moment-kurtosis" class="section level3">
<h3><span class="header-section-number">3.1.7</span> The Fourth Moment – Kurtosis</h3>
The <em>kurtosis</em> of a distribution refers to the the peak of a variable (i.e., the mode) and the relative frequency of observations in the tails. It is based on the fourth moment which is expressed as:
<span class="math display" id="eq:03-9">\[\begin{equation}
\frac{\sum (X-\bar{X})^{4}}{(n-1)}
\tag{3.9}
\end{equation}\]</span>
Kurtosis is calculated by dividing the fourth moment by the square of the second moment (i.e., variance).
<span class="math display" id="eq:03-10">\[\begin{equation}
 K =  \frac{\frac{\sum (X-\bar{X})^{4}}{(n-1)}}{(\frac{\sum (X-\bar{X})^{2}}{(n-1)})^{2}}
 \tag{3.10}
\end{equation}\]</span>
<p>In general, higher kurtosis is indicative of a distribution where the variance is a result of low frequency yet more extreme observed values. In addition, when <span class="math inline">\(K &lt; 3\)</span>, the distribution is <em>platykurtic</em>, which is flatter and/or more “short-tailed” than a normal distribution. When <span class="math inline">\(K &gt; 3\)</span> the distribution is <em>leptokurtic</em>, which is a slim, high peak and long tails. In a normal distribution <span class="math inline">\(K=3\)</span>.</p>
</div>
<div id="order-statistics" class="section level3">
<h3><span class="header-section-number">3.1.8</span> Order Statistics</h3>
<p>Apart from central tendency and moments, probability distributions can also be characterized by <strong>order statistics</strong>. Order statistics are based on the position of a value in an ordered list. Typically, the list is ordered from low values to high values.</p>
<blockquote>
<p><strong>Order Statistics</strong></p>
<p>Summaries of values based on position in an ordered list of all values. Types of order statistics include the minimum value, the maximum value, the median, quartiles, and percentiles.</p>
<ul>
<li><em>Minimum Value</em>: The lowest value of a distribution</li>
<li><em>Maximum Value</em>: The highest value of a distribution</li>
<li><em>Median</em>: The value at the center of a distribution</li>
<li><em>Quartiles</em>: Divides the values into quarters</li>
<li><em>Percentiles</em>: Divides the values into hundredths</li>
</ul>
</blockquote>
<div id="median" class="section level4 unnumbered">
<h4>Median</h4>
<p>The <em>median</em> is the value at the center of the distribution, therefore 50% of the observations in the distribution will have values above the median and 50% will have values below. For samples with a <span class="math inline">\(n\)</span>-size that is an odd number, the median is simply the value in the middle. For example, with a sample consisting of the observed values of <span class="math inline">\(1, 2, 3, 4, 5\)</span>, the median is <span class="math inline">\(3\)</span>. Distributions with an even numbered <span class="math inline">\(n\)</span>-size, the median is the average of the two middle values. The median of a sample consisting of the observed values of <span class="math inline">\(1, 2, 3, 4, 5, 6\)</span> would be <span class="math inline">\(\frac{3+4}{2}\)</span> or 3.5.</p>
<p>The the median is the order statistic for central tendency. In addition, it is more “robust” in terms of extreme values than the mean. Extremely high values in a distribution can pull the mean higher, and extremely low values pull the mean lower. The median is less sensitive to these extreme values. The median is therefore the basis for ``robust estimators,&quot; to be discussed later in this book.</p>
</div>
<div id="quartiles" class="section level4 unnumbered">
<h4>Quartiles</h4>
<p><em>Quartiles</em> split the observations in a distribution into quarters. The first quartile, <span class="math inline">\(Q1\)</span>, consists of observations whose values are within the first 25% of the distribution. The values of the second quartile, <span class="math inline">\(Q2\)</span>, are contained within the first half (50%) of the distribution, and is marked by the distribution’s median. The third quartile, <span class="math inline">\(Q3\)</span>, includes the first 75% of the observations in the distribution.</p>
The interquartile range (IQR) measures the spread of the ordered values. It is calculated by subtracting <span class="math inline">\(Q1\)</span> from <span class="math inline">\(Q3\)</span>.
<span class="math display" id="eq:03-11">\[\begin{equation}
 IQR = Q_{3}-Q_{1}
 \tag{3.11}
\end{equation}\]</span>
<p>The IQR contains the middle 50% of the distribution.</p>
<p>We can visually examine the order statistics of a variable with a boxplot. A boxplot displays the range of the data, the first and third quartile, the median, and any outliers. The following returns a boxplot (Figure <a href="3-1-characterizing-data.html#fig:boxrsk">3.5</a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="st">&quot;&quot;</span>, glbcc_risk)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>()</code></pre></div>
<div class="figure"><span id="fig:boxrsk"></span>
<img src="_main_files/figure-html/boxrsk-1.png" alt="Box-plot of Climate Change Risk" width="672" />
<p class="caption">
Figure 3.5: Box-plot of Climate Change Risk
</p>
</div>
</div>
<div id="percentiles" class="section level4 unnumbered">
<h4>Percentiles</h4>
<p><em>Percentiles-</em> list the data in hundredths. For example, scoring in the 99th percentile on the GRE means that 99% of the other test takers had a lower score. Percentiles can be incorporated with quartiles (and/or other order statistics) such that: - First Quartile: 25th percentile - Second Quartile: 50th percentile (the median) - Third Quartile: 75th percentile</p>
<p>Another way to compare a variable distribution to a theoretical distribution is with a quantile-comparison plot (qq plot). A qq plot displays the observed percentiles against those that would be expected in a normal distribution. This plot is often useful for examining the tails of the distribution, and deviations of a distribution from normality. This is shown in Figure <a href="3-1-characterizing-data.html#fig:qqrsk">3.6</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(ds, <span class="kw">aes</span>(<span class="dt">sample =</span> glbcc_risk)) <span class="op">+</span>
<span class="st">  </span><span class="kw">stat_qq</span>()</code></pre></div>
<div class="figure"><span id="fig:qqrsk"></span>
<img src="_main_files/figure-html/qqrsk-1.png" alt="QQ Plot of Climate Change Risk" width="672" />
<p class="caption">
Figure 3.6: QQ Plot of Climate Change Risk
</p>
</div>
<p>The qq plot provides an easy way to observe departures of a distribution from normality. For example, the plot shown in Figure <a href="3-1-characterizing-data.html#fig:qqrsk">3.6</a> indicates that the perceived risk measure has more observations in the tails of the distribution than would be expected if the variable was normally distributed.</p>
<p><code>R</code> provides several ways to examine the central tendency, moments, and order statistics for individual variables and for entire data sets. The <code>summary</code> function produces the minimum value, the first quartile, median, mean, third quartile, max value, and the number of missing values (Na’s).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(ds<span class="op">$</span>ideol, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   1.000   4.000   5.000   4.653   6.000   7.000      23</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(ds<span class="op">$</span>age, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   18.00   52.00   62.00   60.37   70.00   99.00</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(ds<span class="op">$</span>glbcc_risk, <span class="dt">na.rm=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA&#39;s 
##   0.000   4.000   6.000   5.946   9.000  10.000      11</code></pre>
<p>We can also use the <code>describe</code> function in the <code>psych</code> package to obtain more descriptive statistics, including skewness and kurtosis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(psych)
<span class="kw">describe</span>(ds<span class="op">$</span>ideol)</code></pre></div>
<pre><code>##    vars    n mean   sd median trimmed  mad min max range  skew kurtosis
## X1    1 2524 4.65 1.73      5    4.75 1.48   1   7     6 -0.45     -0.8
##      se
## X1 0.03</code></pre>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>The <code>na.rm=TRUE</code> portion of the following code simply tells R to exclude the missing (NA) values from calculation.<a href="3-1-characterizing-data.html#fnref5">↩</a></p></li>
<li id="fn6"><p>What’s with those (n-1) terms in the denominators? These represent the “degrees of freedom” we need to calculate average squared deviations and variance. We “use up” one of our observations to be able to calculate the first deviation – because without that first observation, what would there be to deviate from?<a href="3-1-characterizing-data.html#fnref6">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="3-exploring-and-visualizing-data.html"><button class="btn btn-default">Previous</button></a>
<a href="3-2-summary.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
