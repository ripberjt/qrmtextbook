<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>15 The Art of Regression Diagnostics | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R</title>
  <meta name="description" content="15 The Art of Regression Diagnostics | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="15 The Art of Regression Diagnostics | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="15 The Art of Regression Diagnostics | Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R" />
  
  
  

<meta name="author" content="Hank Jenkins-Smith, Joseph Ripberger, Gary Copeland, Matthew Nowlin, Tyler Hughes, Aaron Fister, Wesley Wehde, and Josie Davis" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="topics-in-multiple-regression.html">
<link rel="next" href="logit-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface and Acknowledgments</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#copyright"><i class="fa fa-check"></i>Copyright</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html"><i class="fa fa-check"></i><b>1</b> Theories and Social Science</a><ul>
<li class="chapter" data-level="1.1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#the-scientific-method"><i class="fa fa-check"></i><b>1.1</b> The Scientific Method</a></li>
<li class="chapter" data-level="1.2" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-and-empirical-research"><i class="fa fa-check"></i><b>1.2</b> Theory and Empirical Research</a><ul>
<li class="chapter" data-level="1.2.1" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#coherent-and-internally-consistent"><i class="fa fa-check"></i><b>1.2.1</b> Coherent and Internally Consistent</a></li>
<li class="chapter" data-level="1.2.2" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theories-and-causality"><i class="fa fa-check"></i><b>1.2.2</b> Theories and Causality</a></li>
<li class="chapter" data-level="1.2.3" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#generation-of-testable-hypothesis"><i class="fa fa-check"></i><b>1.2.3</b> Generation of Testable Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-and-functions"><i class="fa fa-check"></i><b>1.3</b> Theory and Functions</a></li>
<li class="chapter" data-level="1.4" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#theory-in-social-science"><i class="fa fa-check"></i><b>1.4</b> Theory in Social Science</a></li>
<li class="chapter" data-level="1.5" data-path="theories-and-social-science.html"><a href="theories-and-social-science.html#outline-of-the-book"><i class="fa fa-check"></i><b>1.5</b> Outline of the Book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="research-design.html"><a href="research-design.html"><i class="fa fa-check"></i><b>2</b> Research Design</a><ul>
<li class="chapter" data-level="2.1" data-path="research-design.html"><a href="research-design.html#overview-of-the-research-process"><i class="fa fa-check"></i><b>2.1</b> Overview of the Research Process</a></li>
<li class="chapter" data-level="2.2" data-path="research-design.html"><a href="research-design.html#internal-and-external-validity"><i class="fa fa-check"></i><b>2.2</b> Internal and External Validity</a></li>
<li class="chapter" data-level="2.3" data-path="research-design.html"><a href="research-design.html#major-classes-of-designs"><i class="fa fa-check"></i><b>2.3</b> Major Classes of Designs</a></li>
<li class="chapter" data-level="2.4" data-path="research-design.html"><a href="research-design.html#threats-to-validity"><i class="fa fa-check"></i><b>2.4</b> Threats to Validity</a></li>
<li class="chapter" data-level="2.5" data-path="research-design.html"><a href="research-design.html#some-common-designs"><i class="fa fa-check"></i><b>2.5</b> Some Common Designs</a></li>
<li class="chapter" data-level="2.6" data-path="research-design.html"><a href="research-design.html#plan-meets-reality"><i class="fa fa-check"></i><b>2.6</b> Plan Meets Reality</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html"><i class="fa fa-check"></i><b>3</b> Exploring and Visualizing Data</a><ul>
<li class="chapter" data-level="3.1" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#characterizing-data"><i class="fa fa-check"></i><b>3.1</b> Characterizing Data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#central-tendency"><i class="fa fa-check"></i><b>3.1.1</b> Central Tendency</a></li>
<li class="chapter" data-level="3.1.2" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#level-of-measurement-and-central-tendency"><i class="fa fa-check"></i><b>3.1.2</b> Level of Measurement and Central Tendency</a></li>
<li class="chapter" data-level="3.1.3" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#moments"><i class="fa fa-check"></i><b>3.1.3</b> Moments</a></li>
<li class="chapter" data-level="3.1.4" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#first-moment-expected-value"><i class="fa fa-check"></i><b>3.1.4</b> First Moment – Expected Value</a></li>
<li class="chapter" data-level="3.1.5" data-path="exploring-and-visualizing-data.html"><a href="exploring-and-visualizing-data.html#the-second-moment-variance-and-standard-deviation"><i class="fa fa-check"></i><b>3.1.5</b> The Second Moment – Variance and Standard Deviation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>4</b> Probability</a><ul>
<li class="chapter" data-level="4.1" data-path="probability.html"><a href="probability.html#finding-probabilities"><i class="fa fa-check"></i><b>4.1</b> Finding Probabilities</a></li>
<li class="chapter" data-level="4.2" data-path="probability.html"><a href="probability.html#finding-probabilities-with-the-normal-curve"><i class="fa fa-check"></i><b>4.2</b> Finding Probabilities with the Normal Curve</a></li>
<li class="chapter" data-level="4.3" data-path="probability.html"><a href="probability.html#summary"><i class="fa fa-check"></i><b>4.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>5</b> Inference</a><ul>
<li class="chapter" data-level="5.1" data-path="inference.html"><a href="inference.html#inference-populations-and-samples"><i class="fa fa-check"></i><b>5.1</b> Inference: Populations and Samples</a><ul>
<li class="chapter" data-level="5.1.1" data-path="inference.html"><a href="inference.html#populations-and-samples"><i class="fa fa-check"></i><b>5.1.1</b> Populations and Samples</a></li>
<li class="chapter" data-level="5.1.2" data-path="inference.html"><a href="inference.html#sampling-and-knowing"><i class="fa fa-check"></i><b>5.1.2</b> Sampling and Knowing</a></li>
<li class="chapter" data-level="5.1.3" data-path="inference.html"><a href="inference.html#sampling-strategies"><i class="fa fa-check"></i><b>5.1.3</b> Sampling Strategies</a></li>
<li class="chapter" data-level="5.1.4" data-path="inference.html"><a href="inference.html#sampling-techniques"><i class="fa fa-check"></i><b>5.1.4</b> Sampling Techniques</a></li>
<li class="chapter" data-level="5.1.5" data-path="inference.html"><a href="inference.html#so-how-is-it-that-we-know"><i class="fa fa-check"></i><b>5.1.5</b> So How is it That We Know?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inference.html"><a href="inference.html#the-normal-distribution"><i class="fa fa-check"></i><b>5.2</b> The Normal Distribution</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inference.html"><a href="inference.html#standardizing-a-normal-distribution-and-z-scores"><i class="fa fa-check"></i><b>5.2.1</b> Standardizing a Normal Distribution and Z-scores</a></li>
<li class="chapter" data-level="5.2.2" data-path="inference.html"><a href="inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="5.2.3" data-path="inference.html"><a href="inference.html#populations-samples-and-symbols"><i class="fa fa-check"></i><b>5.2.3</b> Populations, Samples and Symbols</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference.html"><a href="inference.html#inferences-to-the-population-from-the-sample"><i class="fa fa-check"></i><b>5.3</b> Inferences to the Population from the Sample</a><ul>
<li class="chapter" data-level="5.3.1" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference.html"><a href="inference.html#the-logic-of-hypothesis-testing"><i class="fa fa-check"></i><b>5.3.2</b> The Logic of Hypothesis Testing</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference.html"><a href="inference.html#some-miscellaneous-notes-about-hypothesis-testing"><i class="fa fa-check"></i><b>5.3.3</b> Some Miscellaneous Notes about Hypothesis Testing</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="inference.html"><a href="inference.html#differences-between-groups"><i class="fa fa-check"></i><b>5.4</b> Differences Between Groups</a><ul>
<li class="chapter" data-level="5.4.1" data-path="inference.html"><a href="inference.html#t-tests"><i class="fa fa-check"></i><b>5.4.1</b> <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="inference.html"><a href="inference.html#summary-1"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="association-of-variables.html"><a href="association-of-variables.html"><i class="fa fa-check"></i><b>6</b> Association of Variables</a><ul>
<li class="chapter" data-level="6.1" data-path="association-of-variables.html"><a href="association-of-variables.html#cross-tabulation"><i class="fa fa-check"></i><b>6.1</b> Cross-Tabulation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="association-of-variables.html"><a href="association-of-variables.html#crosstabulation-and-control"><i class="fa fa-check"></i><b>6.1.1</b> Crosstabulation and Control</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="association-of-variables.html"><a href="association-of-variables.html#covariance"><i class="fa fa-check"></i><b>6.2</b> Covariance</a></li>
<li class="chapter" data-level="6.3" data-path="association-of-variables.html"><a href="association-of-variables.html#correlation"><i class="fa fa-check"></i><b>6.3</b> Correlation</a></li>
<li class="chapter" data-level="6.4" data-path="association-of-variables.html"><a href="association-of-variables.html#scatterplots"><i class="fa fa-check"></i><b>6.4</b> Scatterplots</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html"><i class="fa fa-check"></i><b>7</b> The Logic of Ordinary Least Squares Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#theoretical-models"><i class="fa fa-check"></i><b>7.1</b> Theoretical Models</a><ul>
<li class="chapter" data-level="7.1.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#deterministic-linear-model"><i class="fa fa-check"></i><b>7.1.1</b> Deterministic Linear Model</a></li>
<li class="chapter" data-level="7.1.2" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#stochastic-linear-model"><i class="fa fa-check"></i><b>7.1.2</b> Stochastic Linear Model</a></li>
<li class="chapter" data-level="7.1.3" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#assumptions-about-the-error-term"><i class="fa fa-check"></i><b>7.1.3</b> Assumptions about the Error Term</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#estimating-linear-models"><i class="fa fa-check"></i><b>7.2</b> Estimating Linear Models</a><ul>
<li class="chapter" data-level="7.2.1" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#residuals"><i class="fa fa-check"></i><b>7.2.1</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="the-logic-of-ordinary-least-squares-estimation.html"><a href="the-logic-of-ordinary-least-squares-estimation.html#an-example-of-simple-regression"><i class="fa fa-check"></i><b>7.3</b> An Example of Simple Regression</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html"><i class="fa fa-check"></i><b>8</b> Linear Estimation and Minimizing Error</a><ul>
<li class="chapter" data-level="8.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#minimizing-error-using-derivatives"><i class="fa fa-check"></i><b>8.1</b> Minimizing Error using Derivatives</a><ul>
<li class="chapter" data-level="8.1.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#rules-of-derivation"><i class="fa fa-check"></i><b>8.1.1</b> Rules of Derivation</a></li>
<li class="chapter" data-level="8.1.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#critical-points"><i class="fa fa-check"></i><b>8.1.2</b> Critical Points</a></li>
<li class="chapter" data-level="8.1.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#partial-derivation"><i class="fa fa-check"></i><b>8.1.3</b> Partial Derivation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#deriving-ols-estimators"><i class="fa fa-check"></i><b>8.2</b> Deriving OLS Estimators</a><ul>
<li class="chapter" data-level="8.2.1" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#ols-derivation-of-hatalpha"><i class="fa fa-check"></i><b>8.2.1</b> OLS Derivation of <span class="math inline">\(\hat{\alpha}\)</span></a></li>
<li class="chapter" data-level="8.2.2" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#ols-derivation-of-hatbeta"><i class="fa fa-check"></i><b>8.2.2</b> OLS Derivation of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li class="chapter" data-level="8.2.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#interpreting-hatbeta-and-hatalpha"><i class="fa fa-check"></i><b>8.2.3</b> Interpreting <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\alpha}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="linear-estimation-and-minimizing-error.html"><a href="linear-estimation-and-minimizing-error.html#summary-2"><i class="fa fa-check"></i><b>8.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html"><i class="fa fa-check"></i><b>9</b> Bi-Variate Hypothesis Testing and Model Fit</a><ul>
<li class="chapter" data-level="9.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#hypothesis-tests-for-regression-coefficients"><i class="fa fa-check"></i><b>9.1</b> Hypothesis Tests for Regression Coefficients</a><ul>
<li class="chapter" data-level="9.1.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#residual-standard-error"><i class="fa fa-check"></i><b>9.1.1</b> Residual Standard Error</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#measuring-goodness-of-fit"><i class="fa fa-check"></i><b>9.2</b> Measuring Goodness of Fit</a><ul>
<li class="chapter" data-level="9.2.1" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#sample-covariance-and-correlations"><i class="fa fa-check"></i><b>9.2.1</b> Sample Covariance and Correlations</a></li>
<li class="chapter" data-level="9.2.2" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#coefficient-of-determination-r2"><i class="fa fa-check"></i><b>9.2.2</b> Coefficient of Determination: <span class="math inline">\(R^{2}\)</span></a></li>
<li class="chapter" data-level="9.2.3" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#visualizing-bivariate-regression"><i class="fa fa-check"></i><b>9.2.3</b> Visualizing Bivariate Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="bi-variate-hypothesis-testing-and-model-fit.html"><a href="bi-variate-hypothesis-testing-and-model-fit.html#summary-3"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html"><i class="fa fa-check"></i><b>10</b> OLS Assumptions and Simple Regression Diagnostics</a><ul>
<li class="chapter" data-level="10.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#a-recap-of-modeling-assumptions"><i class="fa fa-check"></i><b>10.1</b> A Recap of Modeling Assumptions</a></li>
<li class="chapter" data-level="10.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#when-things-go-bad-with-residuals"><i class="fa fa-check"></i><b>10.2</b> When Things Go Bad with Residuals</a><ul>
<li class="chapter" data-level="10.2.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#outlier-data"><i class="fa fa-check"></i><b>10.2.1</b> “Outlier” Data</a></li>
<li class="chapter" data-level="10.2.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#non-constant-variance"><i class="fa fa-check"></i><b>10.2.2</b> Non-Constant Variance</a></li>
<li class="chapter" data-level="10.2.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#non-linearity-in-the-parameters"><i class="fa fa-check"></i><b>10.2.3</b> Non-Linearity in the Parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#application-of-residual-diagnostics"><i class="fa fa-check"></i><b>10.3</b> Application of Residual Diagnostics</a><ul>
<li class="chapter" data-level="10.3.1" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-non-linearity"><i class="fa fa-check"></i><b>10.3.1</b> Testing for Non-Linearity</a></li>
<li class="chapter" data-level="10.3.2" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-normality-in-model-residuals"><i class="fa fa-check"></i><b>10.3.2</b> Testing for Normality in Model Residuals</a></li>
<li class="chapter" data-level="10.3.3" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#testing-for-non-constant-variance-in-the-residuals"><i class="fa fa-check"></i><b>10.3.3</b> Testing for Non-Constant Variance in the Residuals</a></li>
<li class="chapter" data-level="10.3.4" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#examining-outlier-data"><i class="fa fa-check"></i><b>10.3.4</b> Examining Outlier Data</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#so-now-what-implications-of-residual-analysis"><i class="fa fa-check"></i><b>10.4</b> So Now What? Implications of Residual Analysis</a></li>
<li class="chapter" data-level="10.5" data-path="ols-assumptions-and-simple-regression-diagnostics.html"><a href="ols-assumptions-and-simple-regression-diagnostics.html#summary-4"><i class="fa fa-check"></i><b>10.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html"><i class="fa fa-check"></i><b>11</b> Introduction to Multiple Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-algebra-and-multiple-regression"><i class="fa fa-check"></i><b>11.1</b> Matrix Algebra and Multiple Regression</a></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#the-basics-of-matrix-algebra"><i class="fa fa-check"></i><b>11.2</b> The Basics of Matrix Algebra</a><ul>
<li class="chapter" data-level="11.2.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-basics"><i class="fa fa-check"></i><b>11.2.1</b> Matrix Basics</a></li>
<li class="chapter" data-level="11.2.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#vectors"><i class="fa fa-check"></i><b>11.2.2</b> Vectors</a></li>
<li class="chapter" data-level="11.2.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-operations"><i class="fa fa-check"></i><b>11.2.3</b> Matrix Operations</a></li>
<li class="chapter" data-level="11.2.4" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#transpose"><i class="fa fa-check"></i><b>11.2.4</b> Transpose</a></li>
<li class="chapter" data-level="11.2.5" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#adding-matrices"><i class="fa fa-check"></i><b>11.2.5</b> Adding Matrices</a></li>
<li class="chapter" data-level="11.2.6" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#multiplication-of-matrices"><i class="fa fa-check"></i><b>11.2.6</b> Multiplication of Matrices</a></li>
<li class="chapter" data-level="11.2.7" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#identity-matrices"><i class="fa fa-check"></i><b>11.2.7</b> Identity Matrices</a></li>
<li class="chapter" data-level="11.2.8" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#matrix-inversion"><i class="fa fa-check"></i><b>11.2.8</b> Matrix Inversion</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#ols-regression-in-matrix-form"><i class="fa fa-check"></i><b>11.3</b> OLS Regression in Matrix Form</a></li>
<li class="chapter" data-level="11.4" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#summary-5"><i class="fa fa-check"></i><b>11.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html"><i class="fa fa-check"></i><b>12</b> The Logic of Multiple Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#theoretical-specification"><i class="fa fa-check"></i><b>12.1</b> Theoretical Specification</a><ul>
<li class="chapter" data-level="12.1.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#assumptions-of-ols-regression"><i class="fa fa-check"></i><b>12.1.1</b> Assumptions of OLS Regression</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#partial-effects"><i class="fa fa-check"></i><b>12.2</b> Partial Effects</a></li>
<li class="chapter" data-level="12.3" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#multiple-regression-example"><i class="fa fa-check"></i><b>12.3</b> Multiple Regression Example</a><ul>
<li class="chapter" data-level="12.3.1" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#hypothesis-testing-and-t-tests"><i class="fa fa-check"></i><b>12.3.1</b> Hypothesis Testing and <span class="math inline">\(t\)</span>-tests</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="the-logic-of-multiple-regression.html"><a href="the-logic-of-multiple-regression.html#summary-6"><i class="fa fa-check"></i><b>12.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html"><i class="fa fa-check"></i><b>13</b> Multiple Regression and Model Building</a><ul>
<li class="chapter" data-level="13.1" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#model-building"><i class="fa fa-check"></i><b>13.1</b> Model Building</a><ul>
<li class="chapter" data-level="13.1.1" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#theory-and-hypotheses"><i class="fa fa-check"></i><b>13.1.1</b> Theory and Hypotheses</a></li>
<li class="chapter" data-level="13.1.2" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#empirical-indicators"><i class="fa fa-check"></i><b>13.1.2</b> Empirical Indicators</a></li>
<li class="chapter" data-level="13.1.3" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#risks-in-model-building"><i class="fa fa-check"></i><b>13.1.3</b> Risks in Model Building</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#evils-of-stepwise-regression"><i class="fa fa-check"></i><b>13.2</b> Evils of Stepwise Regression</a></li>
<li class="chapter" data-level="13.3" data-path="multiple-regression-and-model-building.html"><a href="multiple-regression-and-model-building.html#summary-7"><i class="fa fa-check"></i><b>13.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html"><i class="fa fa-check"></i><b>14</b> Topics in Multiple Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#dummy-variables"><i class="fa fa-check"></i><b>14.1</b> Dummy Variables</a></li>
<li class="chapter" data-level="14.2" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#interaction-effects"><i class="fa fa-check"></i><b>14.2</b> Interaction Effects</a></li>
<li class="chapter" data-level="14.3" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#standardized-regression-coefficients"><i class="fa fa-check"></i><b>14.3</b> Standardized Regression Coefficients</a></li>
<li class="chapter" data-level="14.4" data-path="topics-in-multiple-regression.html"><a href="topics-in-multiple-regression.html#summary-8"><i class="fa fa-check"></i><b>14.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html"><i class="fa fa-check"></i><b>15</b> The Art of Regression Diagnostics</a><ul>
<li class="chapter" data-level="15.1" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#ols-error-assumptions-revisited"><i class="fa fa-check"></i><b>15.1</b> OLS Error Assumptions Revisited</a></li>
<li class="chapter" data-level="15.2" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#ols-diagnostic-techniques"><i class="fa fa-check"></i><b>15.2</b> OLS Diagnostic Techniques</a><ul>
<li class="chapter" data-level="15.2.1" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#non-linearity"><i class="fa fa-check"></i><b>15.2.1</b> Non-Linearity</a></li>
<li class="chapter" data-level="15.2.2" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#non-constant-variance-or-heteroscedasticity"><i class="fa fa-check"></i><b>15.2.2</b> Non-Constant Variance, or Heteroscedasticity</a></li>
<li class="chapter" data-level="15.2.3" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#independence-of-e"><i class="fa fa-check"></i><b>15.2.3</b> Independence of <span class="math inline">\(E\)</span></a></li>
<li class="chapter" data-level="15.2.4" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#normality-of-the-residuals"><i class="fa fa-check"></i><b>15.2.4</b> Normality of the Residuals</a></li>
<li class="chapter" data-level="15.2.5" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#outliers-leverage-and-influence"><i class="fa fa-check"></i><b>15.2.5</b> Outliers, Leverage, and Influence</a></li>
<li class="chapter" data-level="15.2.6" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#outliers"><i class="fa fa-check"></i><b>15.2.6</b> Outliers</a></li>
<li class="chapter" data-level="15.2.7" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#multicollinearity"><i class="fa fa-check"></i><b>15.2.7</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="the-art-of-regression-diagnostics.html"><a href="the-art-of-regression-diagnostics.html#summary-9"><i class="fa fa-check"></i><b>15.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="logit-regression.html"><a href="logit-regression.html"><i class="fa fa-check"></i><b>16</b> Logit Regression</a><ul>
<li class="chapter" data-level="16.1" data-path="logit-regression.html"><a href="logit-regression.html#generalized-linear-models"><i class="fa fa-check"></i><b>16.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="16.2" data-path="logit-regression.html"><a href="logit-regression.html#logit-estimation"><i class="fa fa-check"></i><b>16.2</b> Logit Estimation</a><ul>
<li class="chapter" data-level="16.2.1" data-path="logit-regression.html"><a href="logit-regression.html#logit-hypothesis-tests"><i class="fa fa-check"></i><b>16.2.1</b> Logit Hypothesis Tests</a></li>
<li class="chapter" data-level="16.2.2" data-path="logit-regression.html"><a href="logit-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>16.2.2</b> Goodness of Fit</a></li>
<li class="chapter" data-level="16.2.3" data-path="logit-regression.html"><a href="logit-regression.html#interpreting-logits"><i class="fa fa-check"></i><b>16.2.3</b> Interpreting Logits</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="logit-regression.html"><a href="logit-regression.html#summary-10"><i class="fa fa-check"></i><b>16.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html"><i class="fa fa-check"></i><b>17</b> Appendix: Basic R</a><ul>
<li class="chapter" data-level="17.1" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#introduction-to-r"><i class="fa fa-check"></i><b>17.1</b> Introduction to R</a></li>
<li class="chapter" data-level="17.2" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#downloading-r-and-rstudio"><i class="fa fa-check"></i><b>17.2</b> Downloading R and RStudio</a></li>
<li class="chapter" data-level="17.3" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#introduction-to-programming"><i class="fa fa-check"></i><b>17.3</b> Introduction to Programming</a></li>
<li class="chapter" data-level="17.4" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#uploadingreading-data"><i class="fa fa-check"></i><b>17.4</b> Uploading/Reading Data</a></li>
<li class="chapter" data-level="17.5" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#data-manipulation-in-r"><i class="fa fa-check"></i><b>17.5</b> Data Manipulation in R</a></li>
<li class="chapter" data-level="17.6" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#savingwriting-data"><i class="fa fa-check"></i><b>17.6</b> Saving/Writing Data</a></li>
<li class="chapter" data-level="17.7" data-path="appendix-basic-r.html"><a href="appendix-basic-r.html#the-tidyverse"><i class="fa fa-check"></i><b>17.7</b> The Tidyverse</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Research Methods for Political Science, Public Policy and Public Administration: 4th Edition With Applications in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-art-of-regression-diagnostics" class="section level1">
<h1><span class="header-section-number">15</span> The Art of Regression Diagnostics</h1>
<p>The previous chapters have focused on the mathematical bases of multiple OLS regression, the use of partial regression coefficients, and aspects of model design and construction. This chapter returns our focus to the assessment of the statistical adequacy of our models, first by revisiting the key assumptions necessary for OLS to provide the best, linear, unbiased estimates (BLUE) of the relationships between our model <span class="math inline">\(Xs\)</span> and <span class="math inline">\(Y\)</span>. We will then discuss the “art” of diagnosing the results of OLS for potential violations of the key OLS assumptions. We refer to this diagnostic process as an art because there is no “cook book approach” that defines precisely what to do when problems are detected. Note that the examples in this chapter use a subset dataset. This is a smaller data set, <span class="math inline">\(n=500\)</span>, based on the first 500 observations of the full data set used in prior chapters. We use this smaller dataset in order to be able to illustrate, graphically, the diagnostic results described in this chapter.</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb315-1" data-line-number="1"><span class="co">#  create a new data frame with the first 500 observations</span></a>
<a class="sourceLine" id="cb315-2" data-line-number="2">ds.small &lt;-<span class="st"> </span><span class="kw">filter</span>(ds) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb315-3" data-line-number="3"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="st">&quot;glbcc_risk&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;income&quot;</span>, <span class="st">&quot;ideol&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb315-4" data-line-number="4"><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb315-5" data-line-number="5"><span class="st">  </span><span class="kw">na.omit</span>()</a>
<a class="sourceLine" id="cb315-6" data-line-number="6"></a>
<a class="sourceLine" id="cb315-7" data-line-number="7"><span class="co">#  For reference and experimentation here is code </span></a>
<a class="sourceLine" id="cb315-8" data-line-number="8"><span class="co">#  to randomly draw 500 observations from the subset.</span></a>
<a class="sourceLine" id="cb315-9" data-line-number="9"><span class="co">#  tbur.data.small &lt;- tbur.data[sample(1:nrow(tbur.data), 500, replace=FALSE),]</span></a></code></pre></div>
<div id="ols-error-assumptions-revisited" class="section level2">
<h2><span class="header-section-number">15.1</span> OLS Error Assumptions Revisited</h2>
<p>As described in earlier chapters, there is a set of key assumptions that must be met to justify the use of the <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> distributions in the interpretation of OLS model results. In particular, these assumptions are necessary for hypotheses tests and the generation of confidence intervals. When met, the assumptions make OLS more efficient than any other unbiased estimator.</p>
<blockquote>
<p><strong>OLS Assumptions</strong></p>
<p><em>Systematic Component</em></p>
<ul>
<li><p>Linearity</p></li>
<li><p>Fixed <span class="math inline">\(X\)</span></p></li>
</ul>
<p><em>Stochastic Component</em></p>
<ul>
<li><p>Errors have constant variance across the range of <span class="math inline">\(X\)</span></p>
<p><span class="math inline">\(E(\epsilon^2_i) = \sigma^2_\epsilon\)</span></p></li>
<li><p>Errors are independent of <span class="math inline">\(X\)</span> and other <span class="math inline">\(\epsilon_i\)</span></p>
<p><span class="math inline">\(E(\epsilon_i) \equiv E(\epsilon|x_i) = 0\)</span></p></li>
</ul>
<p>and</p>
<p><span class="math inline">\(E(\epsilon_i) \neq E(\epsilon_j)\)</span> for <span class="math inline">\(i \neq j\)</span></p>
<ul>
<li><p>Errors are normally distributed</p>
<p><span class="math inline">\(\epsilon_i \sim N(0,\sigma^2_\epsilon)\)</span></p></li>
</ul>
</blockquote>
<p>There is an additional set of assumptions needed for “correct” model specification. An ideal model OLS would have the following characteristics:
- <span class="math inline">\(Y\)</span> is a linear function of modeled <span class="math inline">\(X\)</span> variables
- No <span class="math inline">\(X\)</span>’s are omitted that affect <span class="math inline">\(E(Y)\)</span> and that are correlated with included <span class="math inline">\(X\)</span>’s. Note that exclusion of other <span class="math inline">\(X\)</span>s that are related to <span class="math inline">\(Y\)</span>, but are not related to the <span class="math inline">\(X\)</span>s in the model, does not critically undermine the model estimates. However, it does reduce the overall ability to explain <span class="math inline">\(Y\)</span>. All <span class="math inline">\(X\)</span>’s in the model affect <span class="math inline">\(E(Y)\)</span>.</p>
<p>Note that if we omit an <span class="math inline">\(X\)</span> that is related to <span class="math inline">\(Y\)</span> and other <span class="math inline">\(X\)</span>s in the model, we will bias the estimate of the included <span class="math inline">\(X\)</span>s. Also consider the problem of including <span class="math inline">\(X\)</span>s that are related to other <span class="math inline">\(X\)</span>s in the model, but not related to <span class="math inline">\(Y\)</span>. This scenario would reduce the independent variance in <span class="math inline">\(X\)</span> used to predict <span class="math inline">\(Y\)</span>.</p>
<p>Table <a href="the-art-of-regression-diagnostics.html#fig:olsassum">15.1</a> summarizes the various classes of assumption failures and their implications.</p>
<div class="figure"><span id="fig:olsassum"></span>
<img src="olsassum.PNG" alt="Summary of OLS Assumption Failures and their Implications" width="100%" />
<p class="caption">
Figure 15.1: Summary of OLS Assumption Failures and their Implications
</p>
</div>
<p>When considering the assumptions, our data permit empirical tests for some assumptions, but not all. Specifically, we can check for linearity, normality of the residuals, homoscedasticity, data “outliers” and multicollinearity. However, we can’t check for correlation between error and <span class="math inline">\(X\)</span>’s, whether the mean error equals zero, and whether all the relevant <span class="math inline">\(X\)</span>’s are included.</p>
</div>
<div id="ols-diagnostic-techniques" class="section level2">
<h2><span class="header-section-number">15.2</span> OLS Diagnostic Techniques</h2>
<p>In this section, we examine the residuals from a multiple regression model for potential problems. Note that we use a subsample of the first 500 observations, drawn from the larger ``tbur.data&quot; dataset, to permit easier evaluation of the plots of residuals. We begin with an evaluation of the assumption of the linearity of the relationship between the <span class="math inline">\(X\)</span>s and <span class="math inline">\(Y\)</span>, and then evaluate assumptions regarding the error term.</p>
<p>Our multiple regression model predicts survey respondents’ levels of risk perceived of climate change (<span class="math inline">\(Y\)</span>) using political ideology, age, household income, and educational achievement as independent variables (<span class="math inline">\(X\)</span>s). The results of the regression model as follows:</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb316-1" data-line-number="1">ols1 &lt;-<span class="st">  </span><span class="kw">lm</span>(glbcc_risk <span class="op">~</span><span class="st"> </span>age <span class="op">+</span><span class="st"> </span>education <span class="op">+</span><span class="st"> </span>income <span class="op">+</span><span class="st"> </span>ideol, <span class="dt">data =</span> ds.small)</a>
<a class="sourceLine" id="cb316-2" data-line-number="2"><span class="kw">summary</span>(ols1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glbcc_risk ~ age + education + income + ideol, data = ds.small)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.1617 -1.7131 -0.0584  1.7216  6.8981 
## 
## Coefficients:
##                  Estimate    Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 12.0848259959  0.7246993630  16.676 &lt;0.0000000000000002 ***
## age         -0.0055585796  0.0084072695  -0.661               0.509    
## education   -0.0186146680  0.0697901408  -0.267               0.790    
## income       0.0000001923  0.0000022269   0.086               0.931    
## ideol       -1.2235648372  0.0663035792 -18.454 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.353 on 445 degrees of freedom
## Multiple R-squared:  0.4365, Adjusted R-squared:  0.4315 
## F-statistic: 86.19 on 4 and 445 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>On the basis of the <span class="math inline">\(R\)</span> output, the model appears to be quite reasonable, with a statistically significant estimated partial regression coefficient for political ideology. But let’s take a closer look.</p>
<div id="non-linearity" class="section level3">
<h3><span class="header-section-number">15.2.1</span> Non-Linearity</h3>
<p>One of the most critical assumptions of OLS is that the relationships between variables are linear in their functional form. We start with a stylized example (a fancy way of saying we made it up!) of what a linear and nonlinear pattern of residuals would look like. Figure <a href="the-art-of-regression-diagnostics.html#fig:convar2">15.2</a> shows an illustration of how the residuals would look with a clearly linear relationship, and Figure <a href="the-art-of-regression-diagnostics.html#fig:nonlin">15.3</a> illustrates how the the residuals would look with a clearly non-linear relationship.</p>
<div class="figure"><span id="fig:convar2"></span>
<img src="_main_files/figure-html/convar2-1.png" alt="Linear" width="672" />
<p class="caption">
Figure 15.2: Linear
</p>
</div>
<div class="figure"><span id="fig:nonlin"></span>
<img src="_main_files/figure-html/nonlin-1.png" alt="Non-Linear" width="672" />
<p class="caption">
Figure 15.3: Non-Linear
</p>
</div>
<p>Now let’s look at the residuals from our example model. We can check the linear nature of the relationship between the DV and the IVs in several ways. First we can plot the residuals by the values of the IVs. We also can add a lowess line to demonstrate the relationship between each of the IVs and the residuals, and add a line at <span class="math inline">\(0\)</span> for comparison.</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb318-1" data-line-number="1">ds.small<span class="op">$</span>fit.r &lt;-<span class="st"> </span>ols1<span class="op">$</span>residuals</a>
<a class="sourceLine" id="cb318-2" data-line-number="2">ds.small<span class="op">$</span>fit.p &lt;-<span class="st"> </span>ols1<span class="op">$</span>fitted.values</a></code></pre></div>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" data-line-number="1"><span class="kw">library</span>(reshape2)</a>
<a class="sourceLine" id="cb319-2" data-line-number="2">ds.small <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb319-3" data-line-number="3"><span class="st">  </span><span class="kw">melt</span>(<span class="dt">measure.vars =</span> <span class="kw">c</span>(<span class="st">&quot;age&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;income&quot;</span>, <span class="st">&quot;ideol&quot;</span>, <span class="st">&quot;fit.p&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb319-4" data-line-number="4"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(value, fit.r, <span class="dt">group =</span> variable)) <span class="op">+</span></a>
<a class="sourceLine" id="cb319-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb319-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> loess) <span class="op">+</span></a>
<a class="sourceLine" id="cb319-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb319-8" data-line-number="8"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>variable, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:multlin"></span>
<img src="_main_files/figure-html/multlin-1.png" alt="Checking for Non-Linearity" width="672" />
<p class="caption">
Figure 15.4: Checking for Non-Linearity
</p>
</div>
<p>As we can see in Figure <a href="the-art-of-regression-diagnostics.html#fig:multlin">15.4</a>, the plots of residuals by both income and ideology seem to indicate non-linear relationships. We can check this “ocular impression” by squaring each term and using the <code>anova</code> function to compare model fit.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb320-1" data-line-number="1">ds.small<span class="op">$</span>age2 &lt;-<span class="st"> </span>ds.small<span class="op">$</span>age<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb320-2" data-line-number="2">ds.small<span class="op">$</span>edu2 &lt;-<span class="st"> </span>ds.small<span class="op">$</span>education<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb320-3" data-line-number="3">ds.small<span class="op">$</span>inc2 &lt;-<span class="st"> </span>ds.small<span class="op">$</span>income<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb320-4" data-line-number="4">ds.small<span class="op">$</span>ideology2&lt;-ds.small<span class="op">$</span>ideol<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb320-5" data-line-number="5">ols2 &lt;-<span class="st"> </span><span class="kw">lm</span>(glbcc_risk <span class="op">~</span><span class="st"> </span>age<span class="op">+</span>age2<span class="op">+</span>education<span class="op">+</span>edu2<span class="op">+</span>income<span class="op">+</span>inc2<span class="op">+</span>ideol<span class="op">+</span>ideology2, <span class="dt">data=</span>ds.small)</a>
<a class="sourceLine" id="cb320-6" data-line-number="6"><span class="kw">summary</span>(ols2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glbcc_risk ~ age + age2 + education + edu2 + income + 
##     inc2 + ideol + ideology2, data = ds.small)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.1563 -1.5894  0.0389  1.4898  7.3417 
## 
## Coefficients:
##                      Estimate        Std. Error t value    Pr(&gt;|t|)    
## (Intercept)  9.66069872535646  1.93057305147186   5.004 0.000000812 ***
## age          0.02973349791714  0.05734762412523   0.518    0.604385    
## age2        -0.00028910659305  0.00050097599702  -0.577    0.564175    
## education   -0.48137978481400  0.35887879735475  -1.341    0.180499    
## edu2         0.05131569933892  0.03722361864679   1.379    0.168723    
## income       0.00000285263412  0.00000534134363   0.534    0.593564    
## inc2        -0.00000000001131  0.00000000001839  -0.615    0.538966    
## ideol       -0.05726196851107  0.35319018414228  -0.162    0.871279    
## ideology2   -0.13270718319750  0.03964680646295  -3.347    0.000886 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.33 on 441 degrees of freedom
## Multiple R-squared:  0.4528, Adjusted R-squared:  0.4429 
## F-statistic: 45.61 on 8 and 441 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>The model output indicates that ideology may have a non-linear relationships with risk perceptions of climate change. For ideology, only the squared term is significant, indicating that levels of perceived risk of climate change decline at an increasing rate for those on the most conservative end of the scale. Again, this is consistent with the visual inspection of the relationship between ideology and the residuals in Figure <a href="the-art-of-regression-diagnostics.html#fig:multlin">15.4</a>. The question remains whether the introduction of these non-linear (polynomial) terms improves overall model fit. We can check that with an analysis of variance across the simple model (without polynomial terms) and the models with the squared terms.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb322-1" data-line-number="1"><span class="kw">anova</span>(ols1,ols2)</a></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: glbcc_risk ~ age + education + income + ideol
## Model 2: glbcc_risk ~ age + age2 + education + edu2 + income + inc2 + 
##     ideol + ideology2
##   Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
## 1    445 2464.2                              
## 2    441 2393.2  4    71.059 3.2736 0.01161 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As we can see, the Anova test indicates that including the squared terms improves model fit, therefore the relationships include nonlinear components.</p>
<p>A final way to check for non-linearity is Ramsey’s Regression Error Specification Test (RESET). This tests the functional form of the model. Similar to our test using squared terms, the RESET tests calculates an <span class="math inline">\(F\)</span> statistic that compares the linear model with a model(s) that raises the IVs to various powers. Specifically, it tests whether there are statistically significant differences in the <span class="math inline">\(R^2\)</span> of each of the models. Similar to a nested <span class="math inline">\(F\)</span> test, it is calculated by:</p>
<p><span class="math display" id="eq:15-1">\[\begin{equation}
  F = \frac{\frac{R^2_1-R^2_0}{q}}{\frac{1-R^2_1}{n-k_1}}
  \tag{15.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(R^2_0\)</span> is the <span class="math inline">\(R^2\)</span> of the linear model, <span class="math inline">\(R^2_1\)</span> is the <span class="math inline">\(R^2\)</span> of the polynomial model(s), <span class="math inline">\(q\)</span> is the number of new regressors, and <span class="math inline">\(k_1\)</span> is the number of IVs in the polynomial model(s). The null hypothesis is that the functional relationship between the <span class="math inline">\(X\)</span>’s and <span class="math inline">\(Y\)</span> is linear, therefore the coefficients of the second and third powers to the IVs are zero. If there is a low <span class="math inline">\(p\)</span>-value (i.e., if we can reject the null hypothesis), non-linear relationships are suspected. This test can be run using the <code>resettest</code> function from the <code>lmtest</code> package. Here we are setting the IVs to the second and third powers and we are examining the regressor variables.<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a></p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb324-1" data-line-number="1"><span class="kw">library</span>(lmtest)</a>
<a class="sourceLine" id="cb324-2" data-line-number="2"><span class="kw">resettest</span>(ols1,<span class="dt">power=</span><span class="dv">2</span><span class="op">:</span><span class="dv">3</span>,<span class="dt">type=</span><span class="st">&quot;regressor&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  RESET test
## 
## data:  ols1
## RESET = 2.2752, df1 = 8, df2 = 437, p-value = 0.02157</code></pre>
<p>Again, the test provides evidence that we have a non-linear relationship.</p>
<p>What should we do when we identify a nonlinear relationship between our <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>s? The first step is to look closely at the bi-variate plots, to try to discern the correct functional form for each <span class="math inline">\(X\)</span> regressor. If the relationship looks curvilinear, try a polynomial regression in which you include both <span class="math inline">\(X\)</span> and <span class="math inline">\(X^2\)</span> for the relevant IVs. It may also be the case that a skewed DV or IV is causing the problem. This is not unusual when, for example, the income variable plays an important role in the model, and the distribution of income is skewed upward. In such a case, you can try transforming the skewed variable, using an appropriate log form.</p>
<p>It is possible that variable transformations won’t suffice, however. In that case, you may have no other option by to try non-linear forms of regression. These non-OLS kinds of models typically use maximal likelihood functions (see the next chapter) to fit the model to the data. But that takes us considerably beyond the focus of this book.</p>
</div>
<div id="non-constant-variance-or-heteroscedasticity" class="section level3">
<h3><span class="header-section-number">15.2.2</span> Non-Constant Variance, or Heteroscedasticity</h3>
<p>Recall that OLS requires constant variance because the even spread of residuals is assumed for both <span class="math inline">\(F\)</span> and <span class="math inline">\(t\)</span> tests. To examine constant variance, we can produce (read as “make up”) a baseline plot to demonstrate what constant variance in the residuals ``should&quot; look like.</p>
<div class="figure"><span id="fig:convar15"></span>
<img src="_main_files/figure-html/convar15-1.png" alt="Constant Variance" width="672" />
<p class="caption">
Figure 15.5: Constant Variance
</p>
</div>
<p>As we can see in Figure <a href="the-art-of-regression-diagnostics.html#fig:convar15">15.5</a>, the residuals are spread evenly and in a seemingly random fashion, much like the ``sneeze plot&quot; discussed in Chapter 10. This is the ideal pattern, indicating that the residuals do not vary systematically over the range of the predicted value for <span class="math inline">\(X\)</span>. The residuals are homoscedastistic, and thus provide the appropriate basis for the <span class="math inline">\(F\)</span> and <span class="math inline">\(t\)</span> tests needed for evaluating your hypotheses.</p>
<p>We can also present a clearly heteroscedastistic residual term. In this case the residuals do vary systematically over the range of <span class="math inline">\(X\)</span>, indicating that the precision of the estimates of <span class="math inline">\(Y\)</span> will vary considerably over the range of predicted values. Note the distinctive fan shape in Figure <a href="the-art-of-regression-diagnostics.html#fig:hetero15">15.6</a>, indicating that predictions of <span class="math inline">\(Y\)</span> lose precision as the value of <span class="math inline">\(X\)</span> increases.</p>
<div class="figure"><span id="fig:hetero15"></span>
<img src="_main_files/figure-html/hetero15-1.png" alt="Heteroscedasticity" width="672" />
<p class="caption">
Figure 15.6: Heteroscedasticity
</p>
</div>
<p>The first step in determining whether we have constant variance is to plot the the residuals by the fitted values for <span class="math inline">\(Y\)</span>, as follows:<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a></p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb326-1" data-line-number="1">ds.small<span class="op">$</span>fit.r &lt;-<span class="st"> </span>ols1<span class="op">$</span>residuals</a>
<a class="sourceLine" id="cb326-2" data-line-number="2">ds.small<span class="op">$</span>fit.p &lt;-<span class="st"> </span>ols1<span class="op">$</span>fitted.values</a>
<a class="sourceLine" id="cb326-3" data-line-number="3"><span class="kw">ggplot</span>(ds.small, <span class="kw">aes</span>(fit.p, fit.r)) <span class="op">+</span></a>
<a class="sourceLine" id="cb326-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb326-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb326-6" data-line-number="6"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Residuals&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb326-7" data-line-number="7"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:multregres"></span>
<img src="_main_files/figure-html/multregres-1.png" alt="Multiple Regression Residuals and Fitted Values" width="672" />
<p class="caption">
Figure 15.7: Multiple Regression Residuals and Fitted Values
</p>
</div>
<p>Based on the pattern evident in Figure <a href="the-art-of-regression-diagnostics.html#fig:multregres">15.7</a>, the residuals
appear to show heteroscedasticity. We can test for non-constant error using the Breusch-Pagan (aka Cook-Weisberg) test. This tests the null hypothesis that the error variance is constant, therefore a small p value would indicate that we have heteroscedasticity. In R we can use the ncvTest function from the car package.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" data-line-number="1"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb327-2" data-line-number="2"><span class="kw">ncvTest</span>(ols1)</a></code></pre></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 12.70938, Df = 1, p = 0.00036383</code></pre>
<p>The non-constant variance test provides confirmation that the residuals from our model are heteroscedastistic.</p>
<p>What are the implications? Our <span class="math inline">\(t\)</span>-tests for the estimated partial regression coefficients assumed constant variance. With the evidence of heteroscedasticity, we conclude that these tests are unreliable (the precision of our estimates will be greater in some ranges of <span class="math inline">\(X\)</span> than others).</p>
<p>They are several steps that can be considered when confronted by heteroscedasticity in the residuals. First, we can consider whether we need to re-specify the model, possibly because we have some omitted variables. If model re-specification does not correct the problem, we can use non-OLS regression techniques that include robust estimated standard errors. Robust standard errors are appropriate when error variance is unknown. Robust standard errors do not change the estimate of <span class="math inline">\(B\)</span>, but adjust the estimated standard error of each coefficient, <span class="math inline">\(SE(B)\)</span>, thus giving more accurate <span class="math inline">\(p\)</span> values. In this example, we draw on White’s (1980)<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a> method to calculate robust standard errors.</p>
<p>White uses a <strong>heteroscedasticity consistent covariance matrix</strong> (hccm) to calculate standard errors when the error term has non-constant variance. Under the OLS assumption of constant error variance, the covariance matrix of <span class="math inline">\(b\)</span> is:</p>
<p><span class="math display">\[\begin{equation*}
  V(b) = (X&#39;X)^{-1} X&#39;V(y)X(X&#39;X)^{-1}
\end{equation*}\]</span></p>
<p>where <span class="math inline">\(V(y)=\sigma^{2}_{e}I_n\)</span>,</p>
<p>therefore,</p>
<p><span class="math inline">\(V(b)=\sigma^{2}_{e}(X&#39;X)^{-1}\)</span>.</p>
<p>If the error terms have distinct
variances, a consistent estimator constrains <span class="math inline">\(\Sigma\)</span> to a diagonal
matrix of the squared residuals,</p>
<p><span class="math inline">\(\Sigma=\text{diag}(\sigma^2_1,\ldots,\sigma^2_n)\)</span></p>
<p>where <span class="math inline">\(\sigma^2_i\)</span> is estimated by <span class="math inline">\(e^2_i\)</span>. Therefore the hccm estimator is expressed as:</p>
<p><span class="math display">\[\begin{equation*}
 V_{hccm}(b) = (X&#39;X)^{-1} X&#39;\text{diag}(e^2_i,\ldots,e^2_n) X(X&#39;X)^{-1}
\end{equation*}\]</span></p>
<p>We can use the <code>hccm</code> function from the <code>car</code> package to calculate the robust standard errors for our regression model, predicting perceived environmental risk (<span class="math inline">\(Y\)</span>) with political ideology, age, education and income as the <span class="math inline">\(X\)</span> variables.</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb329-1" data-line-number="1"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb329-2" data-line-number="2"><span class="kw">hccm</span>(ols1) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">diag</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sqrt</span>()</a></code></pre></div>
<pre><code>##    (Intercept)            age      education         income          ideol 
## 0.668778725013 0.008030365625 0.069824489564 0.000002320899 0.060039031426</code></pre>
<p>Using the <code>hccm</code> function we can create a function in <code>R</code> that will calculate the robust standard errors and the subsequent <span class="math inline">\(t\)</span>-values and <span class="math inline">\(p\)</span>-values.</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" data-line-number="1"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb331-2" data-line-number="2">robust.se &lt;-<span class="st"> </span><span class="cf">function</span>(model) {</a>
<a class="sourceLine" id="cb331-3" data-line-number="3">  s &lt;-<span class="st"> </span><span class="kw">summary</span>(model)</a>
<a class="sourceLine" id="cb331-4" data-line-number="4">  wse &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">hccm</span>(ols1)))</a>
<a class="sourceLine" id="cb331-5" data-line-number="5">  t &lt;-<span class="st"> </span>model<span class="op">$</span>coefficients<span class="op">/</span>wse</a>
<a class="sourceLine" id="cb331-6" data-line-number="6">  p &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="kw">pnorm</span>(<span class="op">-</span><span class="kw">abs</span>(t))</a>
<a class="sourceLine" id="cb331-7" data-line-number="7">  results &lt;-<span class="st"> </span><span class="kw">cbind</span>(model<span class="op">$</span>coefficients, wse, t, p)</a>
<a class="sourceLine" id="cb331-8" data-line-number="8">  <span class="kw">dimnames</span>(results) &lt;-<span class="st"> </span><span class="kw">dimnames</span>(s<span class="op">$</span>coefficients)</a>
<a class="sourceLine" id="cb331-9" data-line-number="9">  results</a>
<a class="sourceLine" id="cb331-10" data-line-number="10">}</a></code></pre></div>
<p>We can then compare our results with the original simple regression model results.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb332-1" data-line-number="1"><span class="kw">summary</span>(ols1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glbcc_risk ~ age + education + income + ideol, data = ds.small)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -7.1617 -1.7131 -0.0584  1.7216  6.8981 
## 
## Coefficients:
##                  Estimate    Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 12.0848259959  0.7246993630  16.676 &lt;0.0000000000000002 ***
## age         -0.0055585796  0.0084072695  -0.661               0.509    
## education   -0.0186146680  0.0697901408  -0.267               0.790    
## income       0.0000001923  0.0000022269   0.086               0.931    
## ideol       -1.2235648372  0.0663035792 -18.454 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.353 on 445 degrees of freedom
## Multiple R-squared:  0.4365, Adjusted R-squared:  0.4315 
## F-statistic: 86.19 on 4 and 445 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" data-line-number="1"><span class="kw">robust.se</span>(ols1)</a></code></pre></div>
<pre><code>##                     Estimate     Std. Error      t value
## (Intercept) 12.0848259958670 0.668778725013  18.06999168
## age         -0.0055585796372 0.008030365625  -0.69219509
## education   -0.0186146679570 0.069824489564  -0.26659225
## income       0.0000001922905 0.000002320899   0.08285175
## ideol       -1.2235648372311 0.060039031426 -20.37948994
##                                                                                                         Pr(&gt;|t|)
## (Intercept) 0.00000000000000000000000000000000000000000000000000000000000000000000000054921988962793404323143119
## age         0.48881482326776815039437451559933833777904510498046875000000000000000000000000000000000000000000000
## education   0.78978312137982031870819810137618333101272583007812500000000000000000000000000000000000000000000000
## income      0.93396941638148500697269582815351895987987518310546875000000000000000000000000000000000000000000000
## ideol       0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000002542911</code></pre>
<p>As we see the estimated <span class="math inline">\(B\)</span>’s remain the same, but the estimated standard errors, <span class="math inline">\(t\)</span>-values and <span class="math inline">\(p\)</span>-values are adjusted to reflect the robust estimation. Despite these adjustments, the results of the hypothesis test remain unchanged.</p>
<p>It is important to note that, while robust estimators can help atone for heteroscedasticity in your models, their use <em>should not</em> be seen as an alternative to careful model construction. The first step should always be to evaluate your model specification and functional form (e.g., the use of polynomials, inclusion of relevant variables), as well as possible measurement error, before resorting to robust estimation.</p>
</div>
<div id="independence-of-e" class="section level3">
<h3><span class="header-section-number">15.2.3</span> Independence of <span class="math inline">\(E\)</span></h3>
<p>As noted above, we cannot test for the assumption that the error term <span class="math inline">\(E\)</span> is independent of the <span class="math inline">\(X\)</span>’s. However we can test to see whether the error terms, <span class="math inline">\(E_i\)</span>, are correlated with each other. One of the assumptions of OLS is that <span class="math inline">\(E(\epsilon_i) \neq E(\epsilon_j)\)</span> for <span class="math inline">\(i \neq j\)</span>. When there is a relationship between the residuals, this is referred to as serial correlation or <strong>autocorrelation</strong>. Autocorrelation is most likely to occur with time-series data, however it can occur with cross-sectional data as well. To test for autocorrelation we use the Durbin-Watson, <span class="math inline">\(d\)</span>, test statistic. The <span class="math inline">\(d\)</span> statistic is expressed as:</p>
<p><span class="math display" id="eq:15-2">\[\begin{equation}
  d = \frac{\sum_{i=2}^{n} (E_i-E_{i-1})^{2}}{\sum_{i=1}^{n} E^{2}_i}
  \tag{15.2}
\end{equation}\]</span></p>
<p>The <span class="math inline">\(d\)</span> statistics ranges from <span class="math inline">\(0\)</span> to <span class="math inline">\(4\)</span>; <span class="math inline">\(0 \leq d \leq 4\)</span>. A <span class="math inline">\(0\)</span> indicates perfect positive correction, <span class="math inline">\(4\)</span> indicates perfect negative correlation, and a <span class="math inline">\(2\)</span> indicates no autocorrelation. Therefore, we look for values of <span class="math inline">\(d\)</span> that are close to <span class="math inline">\(2\)</span>.</p>
<p>We can use the <code>dwtest</code> function in the <code>lmtest</code> package to test the null hypothesis that autocorrelation is <span class="math inline">\(0\)</span>, meaning that we don’t have autocorrelation.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" data-line-number="1"><span class="kw">library</span>(lmtest)</a>
<a class="sourceLine" id="cb336-2" data-line-number="2"><span class="kw">dwtest</span>(ols1)</a></code></pre></div>
<pre><code>## 
##  Durbin-Watson test
## 
## data:  ols1
## DW = 1.9008, p-value = 0.1441
## alternative hypothesis: true autocorrelation is greater than 0</code></pre>
<p>Generally, a Durbin-Watson result between 1.5 and 2.5 indicates, that any autocorrelation in the data will not have a discernible effect on your estimates. The test for our example model indicates that we do not have an autocorrelation problem with this model. If we did find autocorrelation, we would need to respecify our model to account for (or estimate) the relationships among the error terms. In time series analysis, where observations are taken sequentially over time, we would typically include a “lag” term (in which the value of <span class="math inline">\(Y\)</span> in period <span class="math inline">\(t\)</span> is predicted by the value of <span class="math inline">\(Y\)</span> in period <span class="math inline">\(t-1\)</span>). This is a typical <span class="math inline">\(AR1\)</span> model, which would be discussed in a time-series analysis course. The entangled residuals can, of course, be much more complex, and require more specialized models (e.g., ARIMA or vector-autoregression models). These approaches are beyond the scope of this text.</p>
</div>
<div id="normality-of-the-residuals" class="section level3">
<h3><span class="header-section-number">15.2.4</span> Normality of the Residuals</h3>
<p>This is a critical assumption for OLS because (along with homoscedasticity) it is required for hypothesis tests and confidence interval estimation. It is particularly sensitive with small samples. Note that non-normality will increase sample-to-sample variation in model estimates.</p>
<p>To examine normality of the residuals we first plot the residuals and then run what is known as the Shapiro-Wilk normality test. Here we run the test on our example model, and plot the residuals.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb338-1" data-line-number="1">p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(ds.small, <span class="kw">aes</span>(fit.r)) <span class="op">+</span></a>
<a class="sourceLine" id="cb338-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">10</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">fill =</span> <span class="st">&quot;white&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1">p2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(ds.small, <span class="kw">aes</span>(fit.r)) <span class="op">+</span></a>
<a class="sourceLine" id="cb339-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_density</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb339-3" data-line-number="3"><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(ds.small<span class="op">$</span>fit.r),</a>
<a class="sourceLine" id="cb339-4" data-line-number="4">                                         <span class="dt">sd =</span> <span class="kw">sd</span>(ds.small<span class="op">$</span>fit.r)),</a>
<a class="sourceLine" id="cb339-5" data-line-number="5">                <span class="dt">color =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">alpha =</span> <span class="fl">.5</span>)</a></code></pre></div>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb340-1" data-line-number="1">p3 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(ds.small, <span class="kw">aes</span>(<span class="st">&quot;&quot;</span>, fit.r)) <span class="op">+</span></a>
<a class="sourceLine" id="cb340-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_boxplot</span>() </a></code></pre></div>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" data-line-number="1">p4 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(ds.small, <span class="kw">aes</span>(<span class="dt">sample =</span> fit.r)) <span class="op">+</span></a>
<a class="sourceLine" id="cb341-2" data-line-number="2"><span class="st">  </span><span class="kw">stat_qq</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb341-3" data-line-number="3"><span class="st">  </span><span class="kw">stat_qq_line</span>(<span class="dt">size =</span> <span class="fl">1.5</span>, <span class="dt">alpha =</span> <span class="fl">.5</span>)</a></code></pre></div>
<div class="figure"><span id="fig:normresids"></span>
<img src="_main_files/figure-html/normresids-1.png" alt="Multiple Regression Residuals" width="672" />
<p class="caption">
Figure 15.8: Multiple Regression Residuals
</p>
</div>
<p>It appears from the graphs, on the basis of an ``ocular test&quot;, that the residuals are potentially normally distributed. Therefore, to perform a statistical test for non-normality, we use the Shapiro-Wilk, <span class="math inline">\(W\)</span>, test statistic. <span class="math inline">\(W\)</span> is expressed as:</p>
<p><span class="math display" id="eq:15-3">\[\begin{equation}
  W = \frac{(\sum_{i=1}^{n} a_i x_{(i)})^{2}}{\sum_{i=1}^{n} (x_i-\bar{x})^{2}}
  \tag{15.3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(x_{(i)}\)</span> are the ordered sample values and <span class="math inline">\(a_i\)</span> are constants generated from the means, variances, and covariances of the order statistics from a normal distribution.
The Shapiro-Wilk tests the null hypothesis that the residuals are normally distributed. To perform this test in <code>R</code>, use the <code>shapiro.test</code> function.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" data-line-number="1"><span class="kw">shapiro.test</span>(ols1<span class="op">$</span>residuals)</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  ols1$residuals
## W = 0.99566, p-value = 0.2485</code></pre>
<p>Since we have a relatively large <span class="math inline">\(p\)</span> value we fail to reject the null hypothesis of normally distributed errors. Our residuals are, accoridng to our visual examination and this test, normally distributed.</p>
<p>To adjust for non-normal errors we can use robust estimators, as discussed earlier with respect to heteroscedasticity. Robust estimators correct for non-normality, but produce estimated standard errors of the partial regression coefficients that tend to be larger, and hence produce less model precision. Other possible steps, where warranted, include transformation of variables that may have non-linear relationships with <span class="math inline">\(Y\)</span>. Typically this involves taking log transformations of the suspect variables.</p>
</div>
<div id="outliers-leverage-and-influence" class="section level3">
<h3><span class="header-section-number">15.2.5</span> Outliers, Leverage, and Influence</h3>
<p>Apart from the distributional behavior of residuals, it is also important to examine the residuals for ``unusual&quot; observations. Unusual observations in the data may be cases of mis-coding (e.g., <span class="math inline">\(-99\)</span>), mis-measurement, or perhaps special cases that require different kinds of treatment in the model. All of these may appear as unusual cases that are observed in your diagnostic analysis. The unusual cases that we should be most concerned about are regression outliers, that are potentially influential and that are suspect because of their differences from other cases.</p>
<p>Why should we worry about outliers? Recall that OLS minimizes the sum of the squared residuals for a model. Unusual cases – which by definition will have large outliers – have the potential to substantially influence our estimates of <span class="math inline">\(B\)</span> because their already large residuals are squared. A large outlier can thus result in OLS estimates that change the model intercept and slope.</p>
<p>There are several steps that can help identify outliers and their effects on your model. The first – and most obvious – is to examine the range of values in your <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> variables. Do they fall within the appropriate ranges?</p>
<p>This step – too often omitted even by experienced analysts – can help you avoid often agonizing mis-steps that result from inclusion of miscoded data or missing values (e.g., ``-99&quot;) that need to be recoded before running your model. If you fail to identify these problems, they will show up in your residual analysis as outliers. But it is much easier to catch the problem <em>before</em> you run your model.</p>
<p>But sometimes we find outliers for reasons other than mis-codes, and identification requires careful examination of your residuals. First we discuss how to find outliers – unusual values of <span class="math inline">\(Y\)</span> – and leverage – unusual values of <span class="math inline">\(X\)</span> – since they are closely related.</p>
</div>
<div id="outliers" class="section level3">
<h3><span class="header-section-number">15.2.6</span> Outliers</h3>
<p>A regression outlier is an observation that has an unusual value on the dependent variable <span class="math inline">\(Y\)</span>, conditioned on the values of the independent variables, <span class="math inline">\(X\)</span>. Note that an outlier can have a large residual value, but not necessarily affect the estimated slope or intercept. Below we examine a few ways to identify potential outliers, and their effects on our estimated slope coefficients.</p>
<p>Using the regression example, we first plot the residuals to look for any possible outliers. In this plot we are plotting the raw residuals for each of the <span class="math inline">\(500\)</span> observations. This is shown in Figure <a href="the-art-of-regression-diagnostics.html#fig:siminresid">15.9</a>.</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb344-1" data-line-number="1"><span class="kw">ggplot</span>(ds.small, <span class="kw">aes</span>(<span class="kw">row.names</span>(ds.small), fit.r)) <span class="op">+</span></a>
<a class="sourceLine" id="cb344-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb344-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:siminresid"></span>
<img src="_main_files/figure-html/siminresid-1.png" alt="Index Plot of Residuals: Multiple Regression" width="672" />
<p class="caption">
Figure 15.9: Index Plot of Residuals: Multiple Regression
</p>
</div>
<p>Next, we can sort the residuals and find the case with the largest absolute value and examine that case.</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1"><span class="co">#  Sort the residuals</span></a>
<a class="sourceLine" id="cb345-2" data-line-number="2">output<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">sort</span>(ols1<span class="op">$</span>residuals)  <span class="co"># smallest first</span></a>
<a class="sourceLine" id="cb345-3" data-line-number="3">output<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">sort</span>(ols1<span class="op">$</span>residuals, <span class="dt">decreasing =</span> <span class="ot">TRUE</span>) <span class="co"># largest first</span></a>
<a class="sourceLine" id="cb345-4" data-line-number="4"></a>
<a class="sourceLine" id="cb345-5" data-line-number="5"><span class="co">#  The head function return the top results, the argument 1 returns 1 variable only</span></a>
<a class="sourceLine" id="cb345-6" data-line-number="6"><span class="kw">head</span>(output<span class="fl">.1</span>, <span class="dv">1</span>) <span class="co"># smallest residual absolute value</span></a></code></pre></div>
<pre><code>##       333 
## -7.161695</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" data-line-number="1"><span class="kw">head</span>(output<span class="fl">.2</span>, <span class="dv">1</span>) <span class="co"># largest residual absolute value</span></a></code></pre></div>
<pre><code>##      104 
## 6.898077</code></pre>
<p>Then, we can examine the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> values of those cases on key variables. Here we examine the values across all independent variables in the model.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb349-1" data-line-number="1">ds.small[<span class="kw">c</span>(<span class="dv">298</span>,<span class="dv">94</span>),<span class="kw">c</span>(<span class="st">&quot;age&quot;</span>,<span class="st">&quot;education&quot;</span>,<span class="st">&quot;income&quot;</span>,<span class="st">&quot;ideol&quot;</span>,<span class="st">&quot;glbcc_risk&quot;</span>)] <span class="co"># [c(row numbers),c(column numbers)]</span></a></code></pre></div>
<pre><code>##     age education income ideol glbcc_risk
## 333  69         6 100000     2          2
## 104  55         7  94000     7         10</code></pre>
<p>By examining the case of 298, we can see that this is outlier because the observed values of <span class="math inline">\(Y\)</span> are far from what would be expected, given the values of <span class="math inline">\(X\)</span>. A wealthy older liberal would most likely rate climate change as riskier than a 2. In case 94, a strong conservaitive rates climate change risk at the lowest possible value. This observation, while not consistent with the estimated relationship between ideology and environmental concern, is certainly not implausible. But the unusual appearance of a case with a strong conservative leaning, and high risk of cliamte change results in a large residual.</p>
<p>What we really want to know is: does any particular case substantially change the regression results? If a case substantively change the results than it is said to have influence. Individual cases can be outliers, but still be influential. Note that DFBETAS are <strong>case statistics</strong>, therefore a DFBETA value will be calculated for each variable for each case.</p>
<div id="dfbetas" class="section level4 unnumbered">
<h4>DFBETAS</h4>
<p>DFBETAS measure the influence of case <span class="math inline">\(i\)</span> on the <span class="math inline">\(j\)</span> estimated coefficients. Specifically, it asks by how many standard errors does <span class="math inline">\(B_j\)</span> change when case <span class="math inline">\(i\)</span> is removed DFBETAS are expressed as:</p>
<p><span class="math display" id="eq:15-4">\[\begin{equation}
  \text{DFBETAS}_{ij} = \frac{B_{j(-i)}-B_j}{SE(B_j)}
  \tag{15.4}
\end{equation}\]</span></p>
<p>Note that if DFBETAS <span class="math inline">\(&gt; 0\)</span>, then case <span class="math inline">\(i\)</span> pulls <span class="math inline">\(B_j\)</span> <em>up</em>, and if DFBETAS <span class="math inline">\(&lt; 0\)</span>, then case <span class="math inline">\(i\)</span> pulls <span class="math inline">\(B_j\)</span> <em>down</em>. In general, if <span class="math inline">\(|\text{DFBETAS}_{ij}| &gt; \frac{2}{\sqrt{n}}\)</span> then these cases warrant further examination. Note that this approach gets the top 5% of influential cases, given the sample size. For both simple (bi-variate) and multiple regression models the DFBETA cut-offs can be calculated in <code>R</code>.</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb351-1" data-line-number="1">df &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">500</span>)</a>
<a class="sourceLine" id="cb351-2" data-line-number="2">df</a></code></pre></div>
<pre><code>## [1] 0.08944272</code></pre>
<p>In this case, if <span class="math inline">\(|\text{DFBETAS}| &gt; 0.0894427\)</span> then they can be examined for possible influence. Note, however, than in large datasets this may prove to be difficult, so you should examine the largest DFBETAS first. In our example, we will look only at the largest 5 DFBETAS.</p>
<p>To calculate the DFBETAS we use the <code>dfbetas</code> function. Then we examine the DFBETA values for the first five rows of our data.</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" data-line-number="1">df.ols1 &lt;-<span class="st"> </span><span class="kw">dfbetas</span>(ols1)</a>
<a class="sourceLine" id="cb353-2" data-line-number="2">df.ols1[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##    (Intercept)          age   education      income        ideol
## 1 -0.004396485  0.005554545  0.01043817 -0.01548697 -0.005616679
## 2  0.046302381 -0.007569305 -0.02671961 -0.01401653 -0.042323468
## 3 -0.002896270  0.018301623 -0.01946054  0.02534233 -0.023111519
## 5 -0.072106074  0.060263914  0.02966501  0.01243482  0.015464937
## 7 -0.057608817 -0.005345142 -0.04948456  0.06456577  0.134103149</code></pre>
<p>We can then plot the DFBETAS for each of the IVs in our regression models, and create lines for <span class="math inline">\(\pm 0.089\)</span>. Figure <a href="the-art-of-regression-diagnostics.html#fig:dfbetas">15.10</a> shows the DFBETAS for each variable in the multiple regression model.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb355-1" data-line-number="1"><span class="kw">melt</span>(df.ols1, <span class="dt">varnames =</span> <span class="kw">c</span>(<span class="st">&quot;index&quot;</span>, <span class="st">&quot;variable&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb355-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(index, value)) <span class="op">+</span></a>
<a class="sourceLine" id="cb355-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb355-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> df) <span class="op">+</span></a>
<a class="sourceLine" id="cb355-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="op">-</span>df) <span class="op">+</span></a>
<a class="sourceLine" id="cb355-6" data-line-number="6"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>variable, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:dfbetas"></span>
<img src="_main_files/figure-html/dfbetas-1.png" alt="Index Plot of DFBETAS: Multiple Regression" width="672" />
<p class="caption">
Figure 15.10: Index Plot of DFBETAS: Multiple Regression
</p>
</div>
<p>As can be seen, several cases seem to exceed the <span class="math inline">\(0.089\)</span> cut-off. Next we find the case with the highest absolute DFBETA value, and examine the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> values for that case.</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb356-1" data-line-number="1"><span class="co">#  Return Absolute Value dfbeta</span></a>
<a class="sourceLine" id="cb356-2" data-line-number="2"><span class="kw">names</span>(df.ols1) &lt;-<span class="st"> </span><span class="kw">row.names</span>(ds.small)</a>
<a class="sourceLine" id="cb356-3" data-line-number="3">df.ols1[<span class="kw">abs</span>(df.ols1) <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(df.ols1))]  </a></code></pre></div>
<pre><code>##      &lt;NA&gt; 
## 0.4112137</code></pre>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb358-1" data-line-number="1"><span class="co"># a observation name may not be returned - let&#39;s figure out the observation</span></a>
<a class="sourceLine" id="cb358-2" data-line-number="2"></a>
<a class="sourceLine" id="cb358-3" data-line-number="3"><span class="co">#  convert df.osl1 from matrix to dataframe </span></a>
<a class="sourceLine" id="cb358-4" data-line-number="4"><span class="kw">class</span>(df.ols1)</a></code></pre></div>
<pre><code>## [1] &quot;matrix&quot;</code></pre>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb360-1" data-line-number="1">df2.ols1 &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(df.ols1)</a>
<a class="sourceLine" id="cb360-2" data-line-number="2"></a>
<a class="sourceLine" id="cb360-3" data-line-number="3"><span class="co">#  add an id variable</span></a>
<a class="sourceLine" id="cb360-4" data-line-number="4">df2.ols1<span class="op">$</span>id &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">450</span> <span class="co">#  generate a new observation number</span></a>
<a class="sourceLine" id="cb360-5" data-line-number="5"></a>
<a class="sourceLine" id="cb360-6" data-line-number="6"><span class="co">#  head function returns one value, based on ,1</span></a>
<a class="sourceLine" id="cb360-7" data-line-number="7"><span class="co">#  syntax - head(data_set[with(data_set, order(+/-variable)), ], 1)</span></a>
<a class="sourceLine" id="cb360-8" data-line-number="8"></a>
<a class="sourceLine" id="cb360-9" data-line-number="9"><span class="co">#  Ideology</span></a>
<a class="sourceLine" id="cb360-10" data-line-number="10"><span class="kw">head</span>(df2.ols1[<span class="kw">with</span>(df2.ols1, <span class="kw">order</span>(<span class="op">-</span>ideol)), ], <span class="dv">1</span>) <span class="co"># order declining</span></a></code></pre></div>
<pre><code>##      (Intercept)        age   education      income     ideol  id
## 333 -0.001083869 -0.1276632 -0.04252348 -0.07591519 0.2438799 298</code></pre>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb362-1" data-line-number="1"><span class="kw">head</span>(df2.ols1[<span class="kw">with</span>(df2.ols1, <span class="kw">order</span>(<span class="op">+</span>ideol)), ], <span class="dv">1</span>) <span class="co"># order increasing</span></a></code></pre></div>
<pre><code>##     (Intercept)       age   education     income       ideol  id
## 148  -0.0477082 0.1279219 -0.03641922 0.04291471 -0.09833372 131</code></pre>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb364-1" data-line-number="1"><span class="co">#  Income</span></a>
<a class="sourceLine" id="cb364-2" data-line-number="2"><span class="kw">head</span>(df2.ols1[<span class="kw">with</span>(df2.ols1, <span class="kw">order</span>(<span class="op">-</span>income)), ], <span class="dv">1</span>) <span class="co"># order declining</span></a></code></pre></div>
<pre><code>##     (Intercept)         age    education    income       ideol  id
## 494 -0.05137992 -0.01514244 -0.009938873 0.4112137 -0.03873292 445</code></pre>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb366-1" data-line-number="1"><span class="kw">head</span>(df2.ols1[<span class="kw">with</span>(df2.ols1, <span class="kw">order</span>(<span class="op">+</span>income)), ], <span class="dv">1</span>) <span class="co"># order increasing</span></a></code></pre></div>
<pre><code>##     (Intercept)         age  education     income      ideol  id
## 284  0.06766781 -0.06611698 0.08166577 -0.4001515 0.04501527 254</code></pre>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb368-1" data-line-number="1"><span class="co">#  Age</span></a>
<a class="sourceLine" id="cb368-2" data-line-number="2"><span class="kw">head</span>(df2.ols1[<span class="kw">with</span>(df2.ols1, <span class="kw">order</span>(<span class="op">-</span>age)), ], <span class="dv">1</span>) <span class="co"># order declining</span></a></code></pre></div>
<pre><code>##    (Intercept)       age  education      income     ideol id
## 87  -0.2146905 0.1786665 0.04131316 -0.01755352 0.1390403 78</code></pre>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb370-1" data-line-number="1"><span class="kw">head</span>(df2.ols1[<span class="kw">with</span>(df2.ols1, <span class="kw">order</span>(<span class="op">+</span>age)), ], <span class="dv">1</span>) <span class="co"># order increasing</span></a></code></pre></div>
<pre><code>##     (Intercept)        age  education     income     ideol  id
## 467    0.183455 -0.2193257 -0.1906404 0.02477437 0.1832784 420</code></pre>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb372-1" data-line-number="1"><span class="co">#  Education - we find the amount - ID 308 for edu</span></a>
<a class="sourceLine" id="cb372-2" data-line-number="2"><span class="kw">head</span>(df2.ols1[<span class="kw">with</span>(df2.ols1, <span class="kw">order</span>(<span class="op">-</span>education)), ], <span class="dv">1</span>) <span class="co"># order declining</span></a></code></pre></div>
<pre><code>##     (Intercept)        age education      income      ideol  id
## 343  -0.1751724 0.06071469 0.1813973 -0.05557382 0.09717012 308</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb374-1" data-line-number="1"><span class="kw">head</span>(df2.ols1[<span class="kw">with</span>(df2.ols1, <span class="kw">order</span>(<span class="op">+</span>education)), ], <span class="dv">1</span>) <span class="co"># order increasing</span></a></code></pre></div>
<pre><code>##     (Intercept)       age  education      income        ideol id
## 105  0.05091437 0.1062966 -0.2033285 -0.02741242 -0.005880984 95</code></pre>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb376-1" data-line-number="1"><span class="co">#  View the output</span></a>
<a class="sourceLine" id="cb376-2" data-line-number="2">df.ols1[<span class="kw">abs</span>(df.ols1) <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(df.ols1))]  </a></code></pre></div>
<pre><code>##      &lt;NA&gt; 
## 0.4112137</code></pre>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb378-1" data-line-number="1">df.ols1[<span class="kw">c</span>(<span class="dv">308</span>),] <span class="co"># dfbeta number is observation 131 - education</span></a></code></pre></div>
<pre><code>## (Intercept)         age   education      income       ideol 
## -0.17517243  0.06071469  0.18139726 -0.05557382  0.09717012</code></pre>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb380-1" data-line-number="1">ds.small[<span class="kw">c</span>(<span class="dv">308</span>), <span class="kw">c</span>(<span class="st">&quot;age&quot;</span>, <span class="st">&quot;education&quot;</span>, <span class="st">&quot;income&quot;</span>, <span class="st">&quot;ideol&quot;</span>, <span class="st">&quot;glbcc_risk&quot;</span>)]</a></code></pre></div>
<pre><code>##     age education income ideol glbcc_risk
## 343  51         2  81000     3          4</code></pre>
<p>Note that this “severe outlier” is indeed an interesting case – a 51 year old with a high school diploma, relatively high income, who is slightly liberal and perceivs low risk for climate change. But this outlier is not implausible, and therefore we can be reassured that – even in this most extreme case – we do not have problematic outliers.</p>
<p>So, having explored the residuals from our model, we found a number of outliers, some with significant influence on our model results. In inspection of the most extreme outlier gave us no cause to worry that the observations were inappropriately distorting our model results. But what should you do if you find puzzling, implausible observations that may influence your model?</p>
<p>First, as always, evaluate your theory. Is it possible that the case represented a class of observations that behave systematically differently than the other cases? This is of particular concern if you have a cluster of cases, all determined to be outliers, that have similar properties. You may need to modify your theory to account for this subgroup. One such example can be found in the study of American politics, wherein the Southern states routinely appeared to behave differently than others. Most careful efforts to model state (and individual) political behavior account for the unique aspects of southern politics, in ways ranging from the addition of dummy variables to interaction terms in regression models.</p>
<p>How would you determine whether the model (and theory) should be revised? Look closely at the deviant cases – what can you learn from them? Try experiments by running the models with controls – dummies and interaction terms. What effects do you observe? If your results suggest theoretical revisions, you will need to collect new data to test your new hypotheses. Remember: In empirical studies, you need to keep your discoveries distinct from your hypothesis tests.</p>
<p>As a last resort, if you have troubling outliers for which you cannot account in theory, you might decide omit those observations from your model and re-run your analyses. We do not recommend this course of action, because it can appear to be a case of ``jiggering the data&quot; to get the results you want.</p>
</div>
</div>
<div id="multicollinearity" class="section level3">
<h3><span class="header-section-number">15.2.7</span> Multicollinearity</h3>
<p>Multicollinearity is the correlation of the IVs in the model. Note that if any <span class="math inline">\(X_i\)</span> is a linear combination of other <span class="math inline">\(X\)</span>’s in the model, <span class="math inline">\(B_i\)</span> cannot be estimated. As discussed previously, the partial regression coefficient strips both the <span class="math inline">\(X\)</span>’s and <span class="math inline">\(Y\)</span> of the overlapping covariation by regressing one <span class="math inline">\(X\)</span> variable on all other <span class="math inline">\(X\)</span> variables:</p>
<p><span class="math display">\[\begin{align*}
  E_{X_{i}|X_{j}} &amp;= X_i - \hat{X}_i \\
  \hat{X}_i &amp;= A + BX_j 
\end{align*}\]</span></p>
<p>If an X is perfectly predicted by the other <span class="math inline">\(X\)</span>’s, then:</p>

<p>where <span class="math inline">\(R^2_k\)</span> is the <span class="math inline">\(R^2\)</span> obtained from regressing all
<span class="math inline">\(X_k\)</span> on all other <span class="math inline">\(X\)</span>’s.</p>
<p>We rarely find perfect multicollinearity in practice, but high multicollinearity results in loss of statistical resolution. Such as:</p>
<ul>
<li><p>Large standard errors</p></li>
<li><p>Low <span class="math inline">\(t\)</span>-stats, high <span class="math inline">\(p\)</span>-values</p>
<ul>
<li>This erodes the resolution of our hypothesis tests</li>
</ul></li>
<li><p>Enormous sensitivity to small changes in:</p>
<ul>
<li><p>Data</p></li>
<li><p>Model specification</p></li>
</ul></li>
</ul>
<p>You should always check the correlations between the IVs during the model building process. This is a way to quickly identify possible multicollinearity issues.</p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb382-1" data-line-number="1">ds <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb382-2" data-line-number="2"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(age, education, income, ideol) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb382-3" data-line-number="3"><span class="st">  </span><span class="kw">na.omit</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb382-4" data-line-number="4"><span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb382-5" data-line-number="5"><span class="st">  </span><span class="kw">cor</span>()</a></code></pre></div>
<pre><code>##                   age   education      income       ideol
## age        1.00000000 -0.06370223 -0.11853753  0.08535126
## education -0.06370223  1.00000000  0.30129917 -0.13770584
## income    -0.11853753  0.30129917  1.00000000  0.04147114
## ideol      0.08535126 -0.13770584  0.04147114  1.00000000</code></pre>
<p>There do not appear to be any variables that are so highly correlated that it would result in problems with multicolinearity.</p>
<p>We will discuss two more formal ways to check for multicollinearity. First, is the <strong>Variance Inflation Factor</strong> (VIF), and the second is <strong>tolerance</strong>. The VIF is the degree to which the variance of other coefficients is increased due to the inclusion of the specified variable. It is expressed as:</p>
<p><span class="math display" id="eq:15-5">\[\begin{equation}
  \text{VIF} = \frac{1}{1-R^2_k}
  \tag{15.5}
\end{equation}\]</span></p>
<p>Note that as <span class="math inline">\(R^2_k\)</span> increases the variance of <span class="math inline">\(X_k\)</span> increases. A general rule of thumb is that <span class="math inline">\(\text{VIF} &gt; 5\)</span> is problematic.</p>
<p>Another, and related, way to measure multicollinearity is tolerance. The tolerance of any <span class="math inline">\(X\)</span>, <span class="math inline">\(X_k\)</span>, is the proportion of its variance not shared with the other <span class="math inline">\(X\)</span>’s.</p>
<p><span class="math display" id="eq:15-6">\[\begin{equation}
  \text{tolerance} = 1-R^2_k 
  \tag{15.6}
\end{equation}\]</span></p>
<p>Note that this is mathematically equivalent to <span class="math inline">\(\frac{1}{VIF}\)</span>. The rule of thumb for acceptable tolerance is partly a function of <span class="math inline">\(n\)</span>-size:</p>
<ul>
<li>If <span class="math inline">\(n &lt; 50\)</span>, tolerance should exceed <span class="math inline">\(0.7\)</span></li>
<li>If <span class="math inline">\(n &lt; 300\)</span>, tolerance should exceed <span class="math inline">\(0.5\)</span></li>
<li>If <span class="math inline">\(n &lt; 600\)</span>, tolerance should exceed <span class="math inline">\(0.3\)</span></li>
<li>If <span class="math inline">\(n &lt; 1000\)</span>, tolerance should exceed <span class="math inline">\(0.1\)</span></li>
</ul>
<p>Both VIF and tolerance can be calculated in <code>R</code>.</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb384-1" data-line-number="1"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb384-2" data-line-number="2"><span class="kw">vif</span>(ols1)</a></code></pre></div>
<pre><code>##       age education    income     ideol 
##  1.024094  1.098383  1.101733  1.009105</code></pre>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb386-1" data-line-number="1"><span class="dv">1</span><span class="op">/</span><span class="kw">vif</span>(ols1)</a></code></pre></div>
<pre><code>##       age education    income     ideol 
## 0.9764731 0.9104295 0.9076611 0.9909775</code></pre>
<p>Note that, for our example model, we are well within acceptable limits on both VIF and tolerance.</p>
<p>If multicollinearity is suspected, what can you do? One option is to drop one of the highly co-linear variables. However, this may result in model mis-specification. As with other modeling considerations, you must use theory as a guide. A second option would be to add new data, thereby lessening the threat posed by multicolinearity. A third option would be to obtain data from specialized samples that maximize independent variation in the collinear variables (e.g., elite samples may disentangle the effects of income, education, and other SES-related variables).</p>
<p>Yet another strategy involves reconsidering why your data are so highly correlated. It may be that your measures are in fact different “indicators” of the same underlying theoretical concept. This can happen, for example, when you measure sets of attitudes that are all influenced by a more general attitude or belief system. In such a case, data scaling is a promising option. This can be accomplished by building an additive scale, or using various scaling options in <span class="math inline">\(R\)</span>. Another approach would be to use techniques such as factor analysis to tease out the underlying (or ``latent&quot;) variables represented by your indicator variables. Indeed, the combination of factor analysis and regression modeling is an important and widely used approach, referred to as structural equation modeling (SEM). But that is a topic for another book and another course.</p>
</div>
</div>
<div id="summary-9" class="section level2">
<h2><span class="header-section-number">15.3</span> Summary</h2>
<p>In this chapter we have described how you can approach the diagnostic stage for OLS multiple regression analysis. We described the key threats to the necessary assumptions of OLS, and listed them and their effects in Table <a href="the-art-of-regression-diagnostics.html#fig:olsassum">15.1</a>. But we also noted that diagnostics are more of an art than a simple recipe. In this business you will learn as you go, both in the analysis of a particular model (or set of models) and in the development of your own approach and procedures. We wish you well, Grasshopper!</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="24">
<li id="fn24"><p>See the <code>lmtest</code> package documentation for more options and information.<a href="the-art-of-regression-diagnostics.html#fnref24" class="footnote-back">↩</a></p></li>
<li id="fn25"><p>Note that we jitter the points to make them easier to see.<a href="the-art-of-regression-diagnostics.html#fnref25" class="footnote-back">↩</a></p></li>
<li id="fn26"><p>H White, 1980. “A Heteroscedasticity-consistent covariance matrix estimator and a direct test for heteroscedasticity.” <em>Econometrica</em> 48: 817-838.<a href="the-art-of-regression-diagnostics.html#fnref26" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="topics-in-multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logit-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
